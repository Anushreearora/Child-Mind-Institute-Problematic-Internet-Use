{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.base import clone\n",
    "from sklearn.linear_model import LassoCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_squared_error, accuracy_score, cohen_kappa_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "SEED = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the datasets\n",
    "train_file_path = \"train.csv\"\n",
    "train_df = pd.read_csv(train_file_path)\n",
    "test_file_path = \"test.csv\"\n",
    "test_df = pd.read_csv(test_file_path)\n",
    "df = train_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATA EXPLORATION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ft = train_df.copy()\n",
    "test_ft = test_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_new_max_min_col(df):\n",
    "    df['GS_max'] = df[['FGC-FGC_GSND', 'FGC-FGC_GSD']].max(axis=1)\n",
    "    df['GS_min'] = df[['FGC-FGC_GSND', 'FGC-FGC_GSD']].min(axis=1)\n",
    "\n",
    "    df[\"SR_min\"] = df[['FGC-FGC_SRL', 'FGC-FGC_SRR']].min(axis=1)\n",
    "    df[\"SR_max\"] = df[['FGC-FGC_SRL', 'FGC-FGC_SRR']].max(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_new_max_min_col(train_ft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_new_max_min_col(test_ft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "cu_map = {}\n",
    "pu_map = {}\n",
    "tl_map = {}\n",
    "gs_max_map = {}\n",
    "gs_min_map = {}\n",
    "bmr_map = {}\n",
    "dee_map = {}\n",
    "sr_min_map = {}\n",
    "sr_max_map = {}\n",
    "ffmi_map = {}\n",
    "\n",
    "\n",
    "prev = 0\n",
    "for i in range(len(thresholds)):\n",
    "    curr = thresholds[i]\n",
    "    mean_cu = train_ft[(train_ft['Basic_Demos-Age'] > prev) & (train_ft['Basic_Demos-Age'] <= curr)]['FGC-FGC_CU'].mean()\n",
    "    mean_pu = train_ft[(train_ft['Basic_Demos-Age'] > prev) & (train_ft['Basic_Demos-Age'] <= curr)]['FGC-FGC_PU'].mean()\n",
    "    mean_tl = train_ft[(train_ft['Basic_Demos-Age'] > prev) & (train_ft['Basic_Demos-Age'] <= curr)]['FGC-FGC_TL'].mean()\n",
    "    mean_gs_max = train_ft[(train_ft['Basic_Demos-Age'] > prev) & (train_ft['Basic_Demos-Age'] <= curr)]['GS_max'].mean()\n",
    "    mean_gs_min = train_ft[(train_ft['Basic_Demos-Age'] > prev) & (train_ft['Basic_Demos-Age'] <= curr)]['GS_min'].mean()\n",
    "    mean_bmr = train_ft[(train_ft['Basic_Demos-Age'] > prev) & (train_ft['Basic_Demos-Age'] <= curr)]['BIA-BIA_BMR'].mean()\n",
    "    mean_dee = train_ft[(train_ft['Basic_Demos-Age'] > prev) & (train_ft['Basic_Demos-Age'] <= curr)]['BIA-BIA_DEE'].mean()\n",
    "    mean_sr_min = train_ft[(train_ft['Basic_Demos-Age'] > prev) & (train_ft['Basic_Demos-Age'] <= curr)]['SR_min'].mean()\n",
    "    mean_sr_max = train_ft[(train_ft['Basic_Demos-Age'] > prev) & (train_ft['Basic_Demos-Age'] <= curr)]['SR_max'].mean()\n",
    "    mean_ffmi = train_ft[(train_ft['Basic_Demos-Age'] > prev) & (train_ft['Basic_Demos-Age'] <= curr)]['BIA-BIA_FFMI'].mean()\n",
    "    cu_map[i] = mean_cu\n",
    "    pu_map[i] = mean_pu\n",
    "    tl_map[i] = mean_tl\n",
    "    gs_max_map[i] = mean_gs_max\n",
    "    gs_min_map[i] = mean_gs_min\n",
    "    bmr_map[i] = mean_bmr\n",
    "    dee_map[i] = mean_dee\n",
    "    sr_min_map[i] = mean_sr_min\n",
    "    sr_max_map[i] = mean_sr_max\n",
    "    ffmi_map[i] = mean_ffmi\n",
    "    \n",
    "    prev = curr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cu_map:  {0: 1.4891304347826086, 1: 7.3164362519201225, 2: 17.046195652173914, 3: 20.4, 4: 23.142857142857142}\n",
      "pu_map:  {0: 0.7065217391304348, 1: 4.097842835130971, 2: 7.517808219178082, 3: 10.437158469945356, 4: 15.285714285714286}\n",
      "tl_map:  {0: 7.9021739130434785, 1: 8.714543361473522, 2: 10.135054347826086, 3: 10.193548387096774, 4: 9.428571428571429}\n",
      "gs_max_map {0: nan, 1: 16.049668874172184, 2: 23.792010943912448, 3: 32.56924731182796, 4: 40.642857142857146}\n",
      "gs_min_map {0: nan, 1: 14.086026490066224, 2: 21.14311901504788, 3: 28.671505376344083, 4: 34.800000000000004}\n",
      "bmr_map {0: 934.2936000000002, 1: 1044.4710360036004, 2: 1351.50534318555, 3: 1562.4434319526626, 4: 1615.6395}\n",
      "dee_map {0: 1471.1115, 1: 1733.4984878487849, 2: 2303.2957471264367, 3: 2621.1362721893493, 4: 2722.585}\n",
      "sr_min_map {0: 9.256043956043955, 1: 8.503477588871716, 2: 7.748975409836065, 3: 8.251612903225807, 4: 8.25}\n",
      "sr_max_map {0: 10.187912087912089, 1: 9.44308346213292, 2: 8.690437158469946, 3: 9.350537634408601, 4: 10.0}\n",
      "ffmi_map {0: 14.450953333333333, 1: 14.120668929856116, 2: 15.969326393442623, 3: 17.181842603550297, 4: 17.7892925}\n"
     ]
    }
   ],
   "source": [
    "print(\"cu_map: \", cu_map)\n",
    "print(\"pu_map: \", pu_map)\n",
    "print(\"tl_map: \", tl_map)\n",
    "print(\"gs_max_map\", gs_max_map)\n",
    "print(\"gs_min_map\", gs_min_map)\n",
    "print(\"bmr_map\", bmr_map)\n",
    "print(\"dee_map\", dee_map)\n",
    "print(\"sr_min_map\", sr_min_map)\n",
    "print(\"sr_max_map\", sr_max_map)\n",
    "print(\"ffmi_map\", ffmi_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bin_data(train, test, columns, n_bins=10):\n",
    "    # Combine train and test for consistent bin edges\n",
    "    combined = pd.concat([train, test], axis=0)\n",
    "\n",
    "    bin_edges = {}\n",
    "    for col in columns:\n",
    "        # Compute quantile bin edges correctly\n",
    "        edges = pd.qcut(combined[col], n_bins, retbins=True, labels=False, duplicates=\"drop\")[1]\n",
    "        bin_edges[col] = edges\n",
    "\n",
    "    # Apply the same bin edges to both train and test\n",
    "    for col, edges in bin_edges.items():\n",
    "        num_bins = len(edges) - 1  # Ensure the correct number of labels\n",
    "        labels = range(num_bins)   # Matching labels with bins\n",
    "\n",
    "        train[col] = pd.cut(train[col], bins=edges, labels=labels, include_lowest=True).astype(float)\n",
    "        test[col] = pd.cut(test[col], bins=edges, labels=labels, include_lowest=True).astype(float)\n",
    "\n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_engineering(df):\n",
    "\n",
    "    df[\"CU_norm\"] = df['FGC-FGC_CU'] / df['age_group'].map(cu_map)\n",
    "    df[\"PU_norm\"] = df['FGC-FGC_PU'] / df['age_group'].map(pu_map)\n",
    "    df[\"TL_norm\"] = df['FGC-FGC_TL'] / df['age_group'].map(tl_map)\n",
    "\n",
    "    df['GS_max_norm'] = df['GS_max'] / df[\"age_group\"].map(gs_max_map)\n",
    "    df['GS_min_norm'] = df['GS_min'] / df[\"age_group\"].map(gs_min_map)\n",
    "\n",
    "    df['SR_max_norm'] = df['SR_max'] / df[\"age_group\"].map(gs_max_map)\n",
    "    df['SR_min_norm'] = df['SR_min'] / df[\"age_group\"].map(gs_min_map)\n",
    "\n",
    "    df[\"BMR_norm\"] = df[\"BIA-BIA_BMR\"] / df[\"age_group\"].map(bmr_map)\n",
    "    df[\"DEE_norm\"] = df[\"BIA-BIA_DEE\"] / df[\"age_group\"].map(dee_map)\n",
    "\n",
    "    df[\"FFMI_norm\"] = df[\"BIA-BIA_FFMI\"] / df[\"age_group\"].map(ffmi_map)\n",
    "\n",
    "    df[\"ECW_ICW_ratio\"] = df[\"BIA-BIA_ECW\"] / df[\"BIA-BIA_ICW\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_bin = [\n",
    "    \"CU_norm\", \"PU_norm\", \"TL_norm\", \"GS_min_norm\", \"GS_max_norm\", \n",
    "    \"SR_min_norm\", \"SR_max_norm\", \"BMR_norm\", \"DEE_norm\", \"FFMI_norm\", \"Physical-HeartRate\", \"Physical-Waist_Circumference\", \"Physical-Height\" ,\"Physical-Weight\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'BIA-BIA_BMI' already removed, so no need to add here\n",
    "columns_to_remove = ['FGC-FGC_CU', 'FGC-FGC_CU_Zone', 'FGC-FGC_GSND', 'FGC-FGC_GSND_Zone', 'FGC-FGC_GSD', 'FGC-FGC_GSD_Zone', 'FGC-FGC_PU', 'FGC-FGC_PU_Zone', 'FGC-FGC_SRL', 'FGC-FGC_SRL_Zone', 'FGC-FGC_SRR_Zone', 'FGC-FGC_SRR', 'FGC-FGC_TL', 'FGC-FGC_TL_Zone', \n",
    "                    'BIA-BIA_FFM', 'BIA-BIA_FMI','BIA-BIA_Frame_num', 'BIA-BIA_LDM']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_engineering(train_ft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_engineering(test_ft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ft, test_ft = bin_data(train_ft, test_ft, columns_to_bin, n_bins=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ft = train_ft.drop(columns_to_remove, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ft = test_ft.drop(columns_to_remove, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ACTIGRAPHY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_features(df):\n",
    "    # Convert time_of_day to hours\n",
    "    df[\"hours\"] = df[\"time_of_day\"] // (3_600 * 1_000_000_000)\n",
    "    # Basic features \n",
    "    features = [\n",
    "        df[\"non-wear_flag\"].mean(),\n",
    "        df[\"enmo\"][df[\"enmo\"] >= 0.05].sum(), # Filters out low level noise\n",
    "    ]\n",
    "    \n",
    "    # Define conditions for night, day, and no mask (full data)\n",
    "    night = ((df[\"hours\"] >= 21) | (df[\"hours\"] <= 5))\n",
    "    day = ((df[\"hours\"] <= 20) & (df[\"hours\"] >= 6))\n",
    "    no_mask = np.ones(len(df), dtype=bool)\n",
    "    \n",
    "    # List of columns of interest and masks\n",
    "    keys = [\"enmo\", \"anglez\", \"light\", \"battery_voltage\"]\n",
    "    masks = [no_mask, night, day]\n",
    "    \n",
    "    # Helper function for feature extraction\n",
    "    def extract_stats(data):\n",
    "        return [\n",
    "            data.mean(), \n",
    "            data.std(), \n",
    "            data.max(), \n",
    "            data.min(), \n",
    "            data.diff().mean(), \n",
    "            data.diff().std()\n",
    "        ]\n",
    "    \n",
    "    # Iterate over keys and masks to generate the statistics\n",
    "    for key in keys:\n",
    "        for mask in masks:\n",
    "            filtered_data = df.loc[mask, key]\n",
    "            features.extend(extract_stats(filtered_data))\n",
    "\n",
    "    return features\n",
    "\n",
    "def process_file(filename, dirname):\n",
    "    # Process file and extract time features\n",
    "    df = pd.read_parquet(os.path.join(dirname, filename, 'part-0.parquet'))\n",
    "    df.drop('step', axis=1, inplace=True)\n",
    "    return time_features(df), filename.split('=')[1]\n",
    "\n",
    "def load_time_series(dirname) -> pd.DataFrame:\n",
    "    # Load time series from directory in parallel\n",
    "    ids = os.listdir(dirname)\n",
    "    \n",
    "    with ThreadPoolExecutor() as executor:\n",
    "        results = list(tqdm(executor.map(lambda fname: process_file(fname, dirname), ids), total=len(ids)))\n",
    "    \n",
    "    stats, indexes = zip(*results)\n",
    "    \n",
    "    df = pd.DataFrame(stats, columns=[f\"stat_{i}\" for i in range(len(stats[0]))])\n",
    "    df['id'] = indexes\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 996/996 [00:45<00:00, 22.07it/s]\n"
     ]
    }
   ],
   "source": [
    "train_ts = load_time_series(\"series_train.parquet\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 14.38it/s]\n"
     ]
    }
   ],
   "source": [
    "test_ts = load_time_series(\"series_test.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Principal Component Analysis\n",
    "def perform_pca(train, test, n_components=None, random_state=42):\n",
    "    \n",
    "    pca = PCA(n_components=n_components, random_state=random_state)\n",
    "    train_pca = pca.fit_transform(train)\n",
    "    test_pca = pca.transform(test)\n",
    "    \n",
    "    explained_variance_ratio = pca.explained_variance_ratio_\n",
    "    print(f\"Explained variance ratio of the components:\\n {explained_variance_ratio}\")\n",
    "    print(np.sum(explained_variance_ratio))\n",
    "    \n",
    "    train_pca_df = pd.DataFrame(train_pca, columns=[f'PC_{i+1}' for i in range(train_pca.shape[1])])\n",
    "    test_pca_df = pd.DataFrame(test_pca, columns=[f'PC_{i+1}' for i in range(test_pca.shape[1])])\n",
    "    \n",
    "    return train_pca_df, test_pca_df, pca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(996, 74)\n",
      "Explained variance ratio of the components:\n",
      " [0.2297803  0.11595334 0.07236788 0.06201337 0.05346817 0.04800129\n",
      " 0.0408818  0.03782128 0.03380817 0.02967952 0.02575016 0.0241788\n",
      " 0.02183349 0.02151956 0.01905743]\n",
      "0.8361145330748827\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(3960, 81)"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Processing the time series data and merging with the main dataset\n",
    "df_train = train_ts.drop('id', axis=1)\n",
    "df_test = test_ts.drop('id', axis=1)\n",
    "\n",
    "scaler = StandardScaler() \n",
    "df_train = pd.DataFrame(scaler.fit_transform(df_train), columns=df_train.columns)\n",
    "df_test = pd.DataFrame(scaler.transform(df_test), columns=df_test.columns)\n",
    "\n",
    "for c in df_train.columns:\n",
    "    m = np.mean(df_train[c])\n",
    "    df_train[c].fillna(m, inplace=True)\n",
    "    df_test[c].fillna(m, inplace=True)\n",
    "\n",
    "print(df_train.shape)\n",
    "\n",
    "df_train_pca, df_test_pca, pca = perform_pca(df_train, df_test, n_components=15, random_state=SEED)\n",
    "\n",
    "df_train_pca['id'] = train_ts['id']\n",
    "df_test_pca['id'] = test_ts['id']\n",
    "\n",
    "train = pd.merge(train_ft, df_train_pca, how=\"left\", on='id')\n",
    "test = pd.merge(test_ft, df_test_pca, how=\"left\", on='id')\n",
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.drop(columns=['age_group_label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m train\u001b[38;5;241m.\u001b[39mto_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain_values_merge.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m      2\u001b[0m test\u001b[38;5;241m.\u001b[39mto_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_values_merge.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train' is not defined"
     ]
    }
   ],
   "source": [
    "train.to_csv(\"train_values_merge.csv\", index=False)\n",
    "test.to_csv(\"test_values_merge.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
