{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.base import clone\n",
    "from sklearn.metrics import cohen_kappa_score, make_scorer, confusion_matrix\n",
    "from sklearn.model_selection import StratifiedKFold, KFold, train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from scipy.optimize import minimize\n",
    "from scipy import stats\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "from sklearn.linear_model import ElasticNetCV, LassoCV, Lasso, LinearRegression\n",
    "from sklearn.ensemble import ExtraTreesRegressor, RandomForestRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "from xgboost import XGBRegressor\n",
    "#from catboost import CatBoostRegressor\n",
    "import optuna\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import random\n",
    "import pickle\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from fastdtw import fastdtw\n",
    "from scipy.spatial.distance import euclidean\n",
    "\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Basic_Demos-Enroll_Season</th>\n",
       "      <th>Basic_Demos-Age</th>\n",
       "      <th>Basic_Demos-Sex</th>\n",
       "      <th>CGAS-Season</th>\n",
       "      <th>CGAS-CGAS_Score</th>\n",
       "      <th>Physical-Season</th>\n",
       "      <th>Physical-BMI</th>\n",
       "      <th>Physical-Height</th>\n",
       "      <th>Physical-Weight</th>\n",
       "      <th>Physical-Waist_Circumference</th>\n",
       "      <th>...</th>\n",
       "      <th>PCIAT-PCIAT_18</th>\n",
       "      <th>PCIAT-PCIAT_19</th>\n",
       "      <th>PCIAT-PCIAT_20</th>\n",
       "      <th>PCIAT-PCIAT_Total</th>\n",
       "      <th>SDS-Season</th>\n",
       "      <th>SDS-SDS_Total_Raw</th>\n",
       "      <th>SDS-SDS_Total_T</th>\n",
       "      <th>PreInt_EduHx-Season</th>\n",
       "      <th>PreInt_EduHx-computerinternet_hoursday</th>\n",
       "      <th>sii</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>00008ff9</th>\n",
       "      <td>Fall</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>Winter</td>\n",
       "      <td>51.0</td>\n",
       "      <td>Fall</td>\n",
       "      <td>16.877316</td>\n",
       "      <td>46.0</td>\n",
       "      <td>50.8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>55.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Fall</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>000fd460</th>\n",
       "      <td>Summer</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Fall</td>\n",
       "      <td>14.035590</td>\n",
       "      <td>48.0</td>\n",
       "      <td>46.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Fall</td>\n",
       "      <td>46.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>Summer</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>00105258</th>\n",
       "      <td>Summer</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>Fall</td>\n",
       "      <td>71.0</td>\n",
       "      <td>Fall</td>\n",
       "      <td>16.648696</td>\n",
       "      <td>56.5</td>\n",
       "      <td>75.6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>Fall</td>\n",
       "      <td>38.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>Summer</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>00115b9f</th>\n",
       "      <td>Winter</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>Fall</td>\n",
       "      <td>71.0</td>\n",
       "      <td>Summer</td>\n",
       "      <td>18.292347</td>\n",
       "      <td>56.0</td>\n",
       "      <td>81.6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>Summer</td>\n",
       "      <td>31.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>Winter</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0016bb22</th>\n",
       "      <td>Spring</td>\n",
       "      <td>18</td>\n",
       "      <td>1</td>\n",
       "      <td>Summer</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 81 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         Basic_Demos-Enroll_Season  Basic_Demos-Age  Basic_Demos-Sex  \\\n",
       "id                                                                     \n",
       "00008ff9                      Fall                5                0   \n",
       "000fd460                    Summer                9                0   \n",
       "00105258                    Summer               10                1   \n",
       "00115b9f                    Winter                9                0   \n",
       "0016bb22                    Spring               18                1   \n",
       "\n",
       "         CGAS-Season  CGAS-CGAS_Score Physical-Season  Physical-BMI  \\\n",
       "id                                                                    \n",
       "00008ff9      Winter             51.0            Fall     16.877316   \n",
       "000fd460         NaN              NaN            Fall     14.035590   \n",
       "00105258        Fall             71.0            Fall     16.648696   \n",
       "00115b9f        Fall             71.0          Summer     18.292347   \n",
       "0016bb22      Summer              NaN             NaN           NaN   \n",
       "\n",
       "          Physical-Height  Physical-Weight  Physical-Waist_Circumference  ...  \\\n",
       "id                                                                        ...   \n",
       "00008ff9             46.0             50.8                           NaN  ...   \n",
       "000fd460             48.0             46.0                          22.0  ...   \n",
       "00105258             56.5             75.6                           NaN  ...   \n",
       "00115b9f             56.0             81.6                           NaN  ...   \n",
       "0016bb22              NaN              NaN                           NaN  ...   \n",
       "\n",
       "          PCIAT-PCIAT_18  PCIAT-PCIAT_19  PCIAT-PCIAT_20 PCIAT-PCIAT_Total  \\\n",
       "id                                                                           \n",
       "00008ff9             4.0             2.0             4.0              55.0   \n",
       "000fd460             0.0             0.0             0.0               0.0   \n",
       "00105258             2.0             1.0             1.0              28.0   \n",
       "00115b9f             3.0             4.0             1.0              44.0   \n",
       "0016bb22             NaN             NaN             NaN               NaN   \n",
       "\n",
       "          SDS-Season  SDS-SDS_Total_Raw  SDS-SDS_Total_T PreInt_EduHx-Season  \\\n",
       "id                                                                             \n",
       "00008ff9         NaN                NaN              NaN                Fall   \n",
       "000fd460        Fall               46.0             64.0              Summer   \n",
       "00105258        Fall               38.0             54.0              Summer   \n",
       "00115b9f      Summer               31.0             45.0              Winter   \n",
       "0016bb22         NaN                NaN              NaN                 NaN   \n",
       "\n",
       "          PreInt_EduHx-computerinternet_hoursday  sii  \n",
       "id                                                     \n",
       "00008ff9                                     3.0  2.0  \n",
       "000fd460                                     0.0  0.0  \n",
       "00105258                                     2.0  0.0  \n",
       "00115b9f                                     0.0  1.0  \n",
       "0016bb22                                     NaN  NaN  \n",
       "\n",
       "[5 rows x 81 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.read_csv('train.csv').set_index('id')\n",
    "test = pd.read_csv('test.csv').set_index('id')\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA\n",
    "experimentations\n",
    "|function name|description|\n",
    "|---|---|\n",
    "|EDA_1|View range of sii score|\n",
    "|EDA_2|View Covariance Matrix|\n",
    "|EDA_3|View Box plot of distributions|\n",
    "|EDA_4|View Statistics after Groupby age|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def EDA_1(df):\n",
    "  ## Distribution of sii\n",
    "  display(train.groupby('sii').min()['PCIAT-PCIAT_Total'])\n",
    "  display(train.groupby('sii').max()['PCIAT-PCIAT_Total'])\n",
    "  #sii_bucket = [0,31,50,80,100]\n",
    "\n",
    "def EDA_2(df):\n",
    "  ## Correlation\n",
    "  covariance_matrix = train.select_dtypes(include=['float64','int64']).corr()\n",
    "  display(covariance_matrix)\n",
    "\n",
    "def EDA_3(df):\n",
    "  ## Box Plot showing distribution of columns grouped by age\n",
    "  vis_col = ['Physical-BMI','Physical-Height','Physical-Weight','Physical-Waist_Circumference','Physical-Diastolic_BP','Physical-HeartRate','Physical-Systolic_BP']\n",
    "  for col in vis_col:\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    sns.boxplot(x='Basic_Demos-Age', y=col, data=df)\n",
    "    plt.title(f'{col} by sii')\n",
    "    plt.show()\n",
    "\n",
    "def EDA_4(df):\n",
    "  ## group train by age and find the mean and std\n",
    "  df_grouped = df.select_dtypes(include=['float64','int64']).groupby('Basic_Demos-Age').agg(['mean', 'std'])\n",
    "  return df_grouped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"2\" halign=\"left\">Basic_Demos-Sex</th>\n",
       "      <th colspan=\"2\" halign=\"left\">CGAS-CGAS_Score</th>\n",
       "      <th colspan=\"2\" halign=\"left\">Physical-BMI</th>\n",
       "      <th colspan=\"2\" halign=\"left\">Physical-Height</th>\n",
       "      <th colspan=\"2\" halign=\"left\">Physical-Weight</th>\n",
       "      <th>...</th>\n",
       "      <th colspan=\"2\" halign=\"left\">PCIAT-PCIAT_Total</th>\n",
       "      <th colspan=\"2\" halign=\"left\">SDS-SDS_Total_Raw</th>\n",
       "      <th colspan=\"2\" halign=\"left\">SDS-SDS_Total_T</th>\n",
       "      <th colspan=\"2\" halign=\"left\">PreInt_EduHx-computerinternet_hoursday</th>\n",
       "      <th colspan=\"2\" halign=\"left\">sii</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>...</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Basic_Demos-Age</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.321429</td>\n",
       "      <td>0.469124</td>\n",
       "      <td>61.732394</td>\n",
       "      <td>11.552313</td>\n",
       "      <td>16.751048</td>\n",
       "      <td>2.788413</td>\n",
       "      <td>44.813462</td>\n",
       "      <td>1.926404</td>\n",
       "      <td>48.200962</td>\n",
       "      <td>10.569038</td>\n",
       "      <td>...</td>\n",
       "      <td>13.325000</td>\n",
       "      <td>14.552198</td>\n",
       "      <td>41.433333</td>\n",
       "      <td>9.933167</td>\n",
       "      <td>58.333333</td>\n",
       "      <td>12.909054</td>\n",
       "      <td>0.731481</td>\n",
       "      <td>0.972582</td>\n",
       "      <td>0.162500</td>\n",
       "      <td>0.434108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.472045</td>\n",
       "      <td>64.695652</td>\n",
       "      <td>11.052825</td>\n",
       "      <td>16.468210</td>\n",
       "      <td>2.894492</td>\n",
       "      <td>46.533345</td>\n",
       "      <td>2.732059</td>\n",
       "      <td>49.592617</td>\n",
       "      <td>14.131335</td>\n",
       "      <td>...</td>\n",
       "      <td>15.667897</td>\n",
       "      <td>16.048089</td>\n",
       "      <td>41.655172</td>\n",
       "      <td>11.279298</td>\n",
       "      <td>58.413793</td>\n",
       "      <td>13.950170</td>\n",
       "      <td>0.558730</td>\n",
       "      <td>0.919816</td>\n",
       "      <td>0.210332</td>\n",
       "      <td>0.490695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.346330</td>\n",
       "      <td>0.476347</td>\n",
       "      <td>64.870968</td>\n",
       "      <td>11.729774</td>\n",
       "      <td>16.813720</td>\n",
       "      <td>3.253863</td>\n",
       "      <td>49.138916</td>\n",
       "      <td>3.110556</td>\n",
       "      <td>56.586186</td>\n",
       "      <td>19.125495</td>\n",
       "      <td>...</td>\n",
       "      <td>19.045455</td>\n",
       "      <td>17.486092</td>\n",
       "      <td>40.175549</td>\n",
       "      <td>10.410168</td>\n",
       "      <td>56.514107</td>\n",
       "      <td>12.955619</td>\n",
       "      <td>0.527624</td>\n",
       "      <td>0.871563</td>\n",
       "      <td>0.298701</td>\n",
       "      <td>0.572211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.359184</td>\n",
       "      <td>0.480251</td>\n",
       "      <td>66.087948</td>\n",
       "      <td>10.962000</td>\n",
       "      <td>17.705490</td>\n",
       "      <td>3.994144</td>\n",
       "      <td>51.434319</td>\n",
       "      <td>2.917345</td>\n",
       "      <td>65.706427</td>\n",
       "      <td>18.675367</td>\n",
       "      <td>...</td>\n",
       "      <td>20.891496</td>\n",
       "      <td>16.025622</td>\n",
       "      <td>41.375358</td>\n",
       "      <td>10.511601</td>\n",
       "      <td>58.137536</td>\n",
       "      <td>13.241718</td>\n",
       "      <td>0.670616</td>\n",
       "      <td>0.926280</td>\n",
       "      <td>0.313783</td>\n",
       "      <td>0.577584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.383298</td>\n",
       "      <td>0.486711</td>\n",
       "      <td>65.724252</td>\n",
       "      <td>11.993069</td>\n",
       "      <td>17.874013</td>\n",
       "      <td>3.486458</td>\n",
       "      <td>53.457479</td>\n",
       "      <td>2.752978</td>\n",
       "      <td>71.715190</td>\n",
       "      <td>19.889110</td>\n",
       "      <td>...</td>\n",
       "      <td>24.371345</td>\n",
       "      <td>17.462976</td>\n",
       "      <td>39.890173</td>\n",
       "      <td>9.532831</td>\n",
       "      <td>56.284058</td>\n",
       "      <td>12.367926</td>\n",
       "      <td>0.745455</td>\n",
       "      <td>0.961572</td>\n",
       "      <td>0.432749</td>\n",
       "      <td>0.658696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.354762</td>\n",
       "      <td>0.479012</td>\n",
       "      <td>65.511450</td>\n",
       "      <td>11.833368</td>\n",
       "      <td>18.827199</td>\n",
       "      <td>4.455868</td>\n",
       "      <td>55.586396</td>\n",
       "      <td>2.990931</td>\n",
       "      <td>82.535373</td>\n",
       "      <td>25.689304</td>\n",
       "      <td>...</td>\n",
       "      <td>28.468852</td>\n",
       "      <td>18.411055</td>\n",
       "      <td>41.490323</td>\n",
       "      <td>10.738955</td>\n",
       "      <td>58.329032</td>\n",
       "      <td>13.703334</td>\n",
       "      <td>0.940341</td>\n",
       "      <td>1.012384</td>\n",
       "      <td>0.583607</td>\n",
       "      <td>0.716621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.374251</td>\n",
       "      <td>0.484655</td>\n",
       "      <td>70.420513</td>\n",
       "      <td>67.857321</td>\n",
       "      <td>19.827135</td>\n",
       "      <td>4.609669</td>\n",
       "      <td>58.204781</td>\n",
       "      <td>3.343351</td>\n",
       "      <td>94.420085</td>\n",
       "      <td>32.956189</td>\n",
       "      <td>...</td>\n",
       "      <td>32.135514</td>\n",
       "      <td>19.054046</td>\n",
       "      <td>40.586364</td>\n",
       "      <td>10.254158</td>\n",
       "      <td>57.196347</td>\n",
       "      <td>13.342104</td>\n",
       "      <td>1.175573</td>\n",
       "      <td>1.042991</td>\n",
       "      <td>0.677570</td>\n",
       "      <td>0.752936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.472217</td>\n",
       "      <td>66.201117</td>\n",
       "      <td>11.987531</td>\n",
       "      <td>20.672142</td>\n",
       "      <td>4.833600</td>\n",
       "      <td>60.515087</td>\n",
       "      <td>3.365120</td>\n",
       "      <td>107.494421</td>\n",
       "      <td>34.407861</td>\n",
       "      <td>...</td>\n",
       "      <td>36.894737</td>\n",
       "      <td>19.407533</td>\n",
       "      <td>41.700461</td>\n",
       "      <td>10.313496</td>\n",
       "      <td>58.520737</td>\n",
       "      <td>12.964724</td>\n",
       "      <td>1.338583</td>\n",
       "      <td>1.068562</td>\n",
       "      <td>0.870813</td>\n",
       "      <td>0.836407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.394068</td>\n",
       "      <td>0.489688</td>\n",
       "      <td>64.720588</td>\n",
       "      <td>12.518840</td>\n",
       "      <td>20.949558</td>\n",
       "      <td>5.335717</td>\n",
       "      <td>63.203571</td>\n",
       "      <td>3.563755</td>\n",
       "      <td>119.145902</td>\n",
       "      <td>35.643626</td>\n",
       "      <td>...</td>\n",
       "      <td>39.200000</td>\n",
       "      <td>20.009219</td>\n",
       "      <td>40.595092</td>\n",
       "      <td>9.423269</td>\n",
       "      <td>57.196319</td>\n",
       "      <td>12.178639</td>\n",
       "      <td>1.515957</td>\n",
       "      <td>1.051996</td>\n",
       "      <td>0.993548</td>\n",
       "      <td>0.871607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.420000</td>\n",
       "      <td>0.494797</td>\n",
       "      <td>65.426230</td>\n",
       "      <td>11.795025</td>\n",
       "      <td>22.244483</td>\n",
       "      <td>5.603994</td>\n",
       "      <td>64.921220</td>\n",
       "      <td>3.327111</td>\n",
       "      <td>133.353374</td>\n",
       "      <td>38.111005</td>\n",
       "      <td>...</td>\n",
       "      <td>39.380282</td>\n",
       "      <td>19.420955</td>\n",
       "      <td>41.302013</td>\n",
       "      <td>9.697941</td>\n",
       "      <td>57.932432</td>\n",
       "      <td>12.396874</td>\n",
       "      <td>1.670520</td>\n",
       "      <td>0.994811</td>\n",
       "      <td>1.007042</td>\n",
       "      <td>0.854662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.421348</td>\n",
       "      <td>0.495168</td>\n",
       "      <td>63.184466</td>\n",
       "      <td>12.315101</td>\n",
       "      <td>22.895784</td>\n",
       "      <td>6.089276</td>\n",
       "      <td>65.725984</td>\n",
       "      <td>3.560181</td>\n",
       "      <td>136.369920</td>\n",
       "      <td>47.368613</td>\n",
       "      <td>...</td>\n",
       "      <td>39.693694</td>\n",
       "      <td>20.536350</td>\n",
       "      <td>41.221239</td>\n",
       "      <td>9.729130</td>\n",
       "      <td>57.991150</td>\n",
       "      <td>12.498925</td>\n",
       "      <td>1.820000</td>\n",
       "      <td>1.049576</td>\n",
       "      <td>0.927928</td>\n",
       "      <td>0.860356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.456954</td>\n",
       "      <td>0.499801</td>\n",
       "      <td>63.378378</td>\n",
       "      <td>11.413406</td>\n",
       "      <td>24.355125</td>\n",
       "      <td>6.670572</td>\n",
       "      <td>66.139298</td>\n",
       "      <td>3.731320</td>\n",
       "      <td>150.989565</td>\n",
       "      <td>48.428646</td>\n",
       "      <td>...</td>\n",
       "      <td>40.990000</td>\n",
       "      <td>21.943681</td>\n",
       "      <td>43.285714</td>\n",
       "      <td>12.212067</td>\n",
       "      <td>60.828571</td>\n",
       "      <td>15.496583</td>\n",
       "      <td>1.778689</td>\n",
       "      <td>1.087208</td>\n",
       "      <td>1.010000</td>\n",
       "      <td>0.937383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.421053</td>\n",
       "      <td>0.495908</td>\n",
       "      <td>65.903226</td>\n",
       "      <td>13.507387</td>\n",
       "      <td>23.388164</td>\n",
       "      <td>4.777030</td>\n",
       "      <td>66.839759</td>\n",
       "      <td>3.965897</td>\n",
       "      <td>149.092771</td>\n",
       "      <td>34.977808</td>\n",
       "      <td>...</td>\n",
       "      <td>42.067568</td>\n",
       "      <td>21.181752</td>\n",
       "      <td>43.951220</td>\n",
       "      <td>12.835403</td>\n",
       "      <td>61.243902</td>\n",
       "      <td>15.624949</td>\n",
       "      <td>2.043956</td>\n",
       "      <td>0.868103</td>\n",
       "      <td>1.175676</td>\n",
       "      <td>0.911992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.364865</td>\n",
       "      <td>0.484678</td>\n",
       "      <td>63.410256</td>\n",
       "      <td>11.970837</td>\n",
       "      <td>24.447013</td>\n",
       "      <td>5.651076</td>\n",
       "      <td>67.317358</td>\n",
       "      <td>4.156446</td>\n",
       "      <td>158.260377</td>\n",
       "      <td>41.229495</td>\n",
       "      <td>...</td>\n",
       "      <td>39.583333</td>\n",
       "      <td>21.684226</td>\n",
       "      <td>41.642857</td>\n",
       "      <td>10.774504</td>\n",
       "      <td>58.464286</td>\n",
       "      <td>13.782657</td>\n",
       "      <td>2.142857</td>\n",
       "      <td>0.942503</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.875190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.370370</td>\n",
       "      <td>0.492103</td>\n",
       "      <td>61.250000</td>\n",
       "      <td>14.790199</td>\n",
       "      <td>25.908522</td>\n",
       "      <td>5.872248</td>\n",
       "      <td>66.710000</td>\n",
       "      <td>4.034568</td>\n",
       "      <td>156.873684</td>\n",
       "      <td>59.323949</td>\n",
       "      <td>...</td>\n",
       "      <td>42.083333</td>\n",
       "      <td>24.160292</td>\n",
       "      <td>41.500000</td>\n",
       "      <td>8.386497</td>\n",
       "      <td>58.500000</td>\n",
       "      <td>11.090537</td>\n",
       "      <td>2.100000</td>\n",
       "      <td>1.119210</td>\n",
       "      <td>1.083333</td>\n",
       "      <td>0.996205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.510754</td>\n",
       "      <td>59.363636</td>\n",
       "      <td>12.635448</td>\n",
       "      <td>27.655478</td>\n",
       "      <td>6.636444</td>\n",
       "      <td>67.312500</td>\n",
       "      <td>4.260771</td>\n",
       "      <td>181.087500</td>\n",
       "      <td>57.839962</td>\n",
       "      <td>...</td>\n",
       "      <td>26.909091</td>\n",
       "      <td>18.997129</td>\n",
       "      <td>47.000000</td>\n",
       "      <td>20.904545</td>\n",
       "      <td>62.777778</td>\n",
       "      <td>20.668683</td>\n",
       "      <td>2.266667</td>\n",
       "      <td>0.883715</td>\n",
       "      <td>0.636364</td>\n",
       "      <td>0.674200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.517241</td>\n",
       "      <td>0.508548</td>\n",
       "      <td>55.846154</td>\n",
       "      <td>10.807761</td>\n",
       "      <td>25.980781</td>\n",
       "      <td>6.768879</td>\n",
       "      <td>66.177222</td>\n",
       "      <td>4.327068</td>\n",
       "      <td>152.652632</td>\n",
       "      <td>53.921789</td>\n",
       "      <td>...</td>\n",
       "      <td>32.181818</td>\n",
       "      <td>29.171281</td>\n",
       "      <td>40.142857</td>\n",
       "      <td>9.476789</td>\n",
       "      <td>56.428571</td>\n",
       "      <td>12.340410</td>\n",
       "      <td>2.285714</td>\n",
       "      <td>0.845154</td>\n",
       "      <td>0.727273</td>\n",
       "      <td>1.103713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.875000</td>\n",
       "      <td>0.353553</td>\n",
       "      <td>71.500000</td>\n",
       "      <td>4.949747</td>\n",
       "      <td>28.777482</td>\n",
       "      <td>7.290927</td>\n",
       "      <td>64.166667</td>\n",
       "      <td>1.125463</td>\n",
       "      <td>169.333333</td>\n",
       "      <td>46.888236</td>\n",
       "      <td>...</td>\n",
       "      <td>37.500000</td>\n",
       "      <td>9.192388</td>\n",
       "      <td>49.500000</td>\n",
       "      <td>0.707107</td>\n",
       "      <td>68.500000</td>\n",
       "      <td>0.707107</td>\n",
       "      <td>2.400000</td>\n",
       "      <td>0.547723</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>18 rows × 138 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                Basic_Demos-Sex           CGAS-CGAS_Score             \\\n",
       "                           mean       std            mean        std   \n",
       "Basic_Demos-Age                                                        \n",
       "5                      0.321429  0.469124       61.732394  11.552313   \n",
       "6                      0.333333  0.472045       64.695652  11.052825   \n",
       "7                      0.346330  0.476347       64.870968  11.729774   \n",
       "8                      0.359184  0.480251       66.087948  10.962000   \n",
       "9                      0.383298  0.486711       65.724252  11.993069   \n",
       "10                     0.354762  0.479012       65.511450  11.833368   \n",
       "11                     0.374251  0.484655       70.420513  67.857321   \n",
       "12                     0.333333  0.472217       66.201117  11.987531   \n",
       "13                     0.394068  0.489688       64.720588  12.518840   \n",
       "14                     0.420000  0.494797       65.426230  11.795025   \n",
       "15                     0.421348  0.495168       63.184466  12.315101   \n",
       "16                     0.456954  0.499801       63.378378  11.413406   \n",
       "17                     0.421053  0.495908       65.903226  13.507387   \n",
       "18                     0.364865  0.484678       63.410256  11.970837   \n",
       "19                     0.370370  0.492103       61.250000  14.790199   \n",
       "20                     0.500000  0.510754       59.363636  12.635448   \n",
       "21                     0.517241  0.508548       55.846154  10.807761   \n",
       "22                     0.875000  0.353553       71.500000   4.949747   \n",
       "\n",
       "                Physical-BMI           Physical-Height            \\\n",
       "                        mean       std            mean       std   \n",
       "Basic_Demos-Age                                                    \n",
       "5                  16.751048  2.788413       44.813462  1.926404   \n",
       "6                  16.468210  2.894492       46.533345  2.732059   \n",
       "7                  16.813720  3.253863       49.138916  3.110556   \n",
       "8                  17.705490  3.994144       51.434319  2.917345   \n",
       "9                  17.874013  3.486458       53.457479  2.752978   \n",
       "10                 18.827199  4.455868       55.586396  2.990931   \n",
       "11                 19.827135  4.609669       58.204781  3.343351   \n",
       "12                 20.672142  4.833600       60.515087  3.365120   \n",
       "13                 20.949558  5.335717       63.203571  3.563755   \n",
       "14                 22.244483  5.603994       64.921220  3.327111   \n",
       "15                 22.895784  6.089276       65.725984  3.560181   \n",
       "16                 24.355125  6.670572       66.139298  3.731320   \n",
       "17                 23.388164  4.777030       66.839759  3.965897   \n",
       "18                 24.447013  5.651076       67.317358  4.156446   \n",
       "19                 25.908522  5.872248       66.710000  4.034568   \n",
       "20                 27.655478  6.636444       67.312500  4.260771   \n",
       "21                 25.980781  6.768879       66.177222  4.327068   \n",
       "22                 28.777482  7.290927       64.166667  1.125463   \n",
       "\n",
       "                Physical-Weight             ... PCIAT-PCIAT_Total             \\\n",
       "                           mean        std  ...              mean        std   \n",
       "Basic_Demos-Age                             ...                                \n",
       "5                     48.200962  10.569038  ...         13.325000  14.552198   \n",
       "6                     49.592617  14.131335  ...         15.667897  16.048089   \n",
       "7                     56.586186  19.125495  ...         19.045455  17.486092   \n",
       "8                     65.706427  18.675367  ...         20.891496  16.025622   \n",
       "9                     71.715190  19.889110  ...         24.371345  17.462976   \n",
       "10                    82.535373  25.689304  ...         28.468852  18.411055   \n",
       "11                    94.420085  32.956189  ...         32.135514  19.054046   \n",
       "12                   107.494421  34.407861  ...         36.894737  19.407533   \n",
       "13                   119.145902  35.643626  ...         39.200000  20.009219   \n",
       "14                   133.353374  38.111005  ...         39.380282  19.420955   \n",
       "15                   136.369920  47.368613  ...         39.693694  20.536350   \n",
       "16                   150.989565  48.428646  ...         40.990000  21.943681   \n",
       "17                   149.092771  34.977808  ...         42.067568  21.181752   \n",
       "18                   158.260377  41.229495  ...         39.583333  21.684226   \n",
       "19                   156.873684  59.323949  ...         42.083333  24.160292   \n",
       "20                   181.087500  57.839962  ...         26.909091  18.997129   \n",
       "21                   152.652632  53.921789  ...         32.181818  29.171281   \n",
       "22                   169.333333  46.888236  ...         37.500000   9.192388   \n",
       "\n",
       "                SDS-SDS_Total_Raw            SDS-SDS_Total_T             \\\n",
       "                             mean        std            mean        std   \n",
       "Basic_Demos-Age                                                           \n",
       "5                       41.433333   9.933167       58.333333  12.909054   \n",
       "6                       41.655172  11.279298       58.413793  13.950170   \n",
       "7                       40.175549  10.410168       56.514107  12.955619   \n",
       "8                       41.375358  10.511601       58.137536  13.241718   \n",
       "9                       39.890173   9.532831       56.284058  12.367926   \n",
       "10                      41.490323  10.738955       58.329032  13.703334   \n",
       "11                      40.586364  10.254158       57.196347  13.342104   \n",
       "12                      41.700461  10.313496       58.520737  12.964724   \n",
       "13                      40.595092   9.423269       57.196319  12.178639   \n",
       "14                      41.302013   9.697941       57.932432  12.396874   \n",
       "15                      41.221239   9.729130       57.991150  12.498925   \n",
       "16                      43.285714  12.212067       60.828571  15.496583   \n",
       "17                      43.951220  12.835403       61.243902  15.624949   \n",
       "18                      41.642857  10.774504       58.464286  13.782657   \n",
       "19                      41.500000   8.386497       58.500000  11.090537   \n",
       "20                      47.000000  20.904545       62.777778  20.668683   \n",
       "21                      40.142857   9.476789       56.428571  12.340410   \n",
       "22                      49.500000   0.707107       68.500000   0.707107   \n",
       "\n",
       "                PreInt_EduHx-computerinternet_hoursday                 sii  \\\n",
       "                                                  mean       std      mean   \n",
       "Basic_Demos-Age                                                              \n",
       "5                                             0.731481  0.972582  0.162500   \n",
       "6                                             0.558730  0.919816  0.210332   \n",
       "7                                             0.527624  0.871563  0.298701   \n",
       "8                                             0.670616  0.926280  0.313783   \n",
       "9                                             0.745455  0.961572  0.432749   \n",
       "10                                            0.940341  1.012384  0.583607   \n",
       "11                                            1.175573  1.042991  0.677570   \n",
       "12                                            1.338583  1.068562  0.870813   \n",
       "13                                            1.515957  1.051996  0.993548   \n",
       "14                                            1.670520  0.994811  1.007042   \n",
       "15                                            1.820000  1.049576  0.927928   \n",
       "16                                            1.778689  1.087208  1.010000   \n",
       "17                                            2.043956  0.868103  1.175676   \n",
       "18                                            2.142857  0.942503  1.000000   \n",
       "19                                            2.100000  1.119210  1.083333   \n",
       "20                                            2.266667  0.883715  0.636364   \n",
       "21                                            2.285714  0.845154  0.727273   \n",
       "22                                            2.400000  0.547723  1.000000   \n",
       "\n",
       "                           \n",
       "                      std  \n",
       "Basic_Demos-Age            \n",
       "5                0.434108  \n",
       "6                0.490695  \n",
       "7                0.572211  \n",
       "8                0.577584  \n",
       "9                0.658696  \n",
       "10               0.716621  \n",
       "11               0.752936  \n",
       "12               0.836407  \n",
       "13               0.871607  \n",
       "14               0.854662  \n",
       "15               0.860356  \n",
       "16               0.937383  \n",
       "17               0.911992  \n",
       "18               0.875190  \n",
       "19               0.996205  \n",
       "20               0.674200  \n",
       "21               1.103713  \n",
       "22               0.000000  \n",
       "\n",
       "[18 rows x 138 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EDA_4(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering\n",
    "\n",
    "\n",
    "|input|output|\n",
    "|---|---|\n",
    "|data_df|data_df with new features|\n",
    "\n",
    "experimentations\n",
    "|function name|description|\n",
    "|---|---|\n",
    "|featureEngineering_1|Dropping all season and predictive columns|\n",
    "|featureEngineering_2|Applying StandardScaler|\n",
    "|featureEngineering_3||\n",
    "|featureEngineering_4||"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictive Columns do not exist\n"
     ]
    }
   ],
   "source": [
    "def featureEngineering_1(df):\n",
    "  season_cols = [col for col in df.columns if 'Season' in col]\n",
    "  df = df.drop(season_cols, axis=1)\n",
    "  pred_cols = ['PCIAT-PCIAT_01','PCIAT-PCIAT_02','PCIAT-PCIAT_03','PCIAT-PCIAT_04','PCIAT-PCIAT_05','PCIAT-PCIAT_06','PCIAT-PCIAT_07','PCIAT-PCIAT_08',\n",
    "               'PCIAT-PCIAT_09','PCIAT-PCIAT_10','PCIAT-PCIAT_11','PCIAT-PCIAT_12','PCIAT-PCIAT_13','PCIAT-PCIAT_14','PCIAT-PCIAT_15','PCIAT-PCIAT_16',\n",
    "               'PCIAT-PCIAT_17','PCIAT-PCIAT_18','PCIAT-PCIAT_19','PCIAT-PCIAT_20','SDS-SDS_Total_Raw']\n",
    "  try:\n",
    "    df = df.drop(pred_cols, axis=1)\n",
    "  except:\n",
    "    print(\"Predictive Columns do not exist\")\n",
    "  return df\n",
    "\n",
    "def featureEngineering_2(train,test):\n",
    "  scaler = StandardScaler()\n",
    "  train = pd.DataFrame(scaler.fit_transform(train), columns=train.columns)\n",
    "  test = pd.DataFrame(scaler.fit_transform(test), columns=test.columns)\n",
    "  return train,test\n",
    "\n",
    "train = featureEngineering_1(train)\n",
    "test = featureEngineering_1(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'SDS-SDS_Total_Raw'}\n",
      "{'PCIAT-PCIAT_Total', 'sii'}\n"
     ]
    }
   ],
   "source": [
    "print(set(test.columns) - set(train.columns))\n",
    "print(set(train.columns) - set(test.columns))\n",
    "test = test.drop('SDS-SDS_Total_Raw', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time series\n",
    "\n",
    "\n",
    "|input|output|\n",
    "|---|---|\n",
    "|parquet files|ts_df indexed by user id|\n",
    "\n",
    "experimentations\n",
    "|function name|description|\n",
    "|---|---|\n",
    "|tsEngineering_1|Feature Engineering to extract ts.describe() stats|\n",
    "|tsEngineering_2|Perform Autoencoding on Featured Engineered statistics|\n",
    "|tsEngineering_3|Clustering of ts_data based on DTW distances|\n",
    "|tsEngineering_4|Mapping of ts_data to external data based on DTW distances|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_file(filename, dirname):\n",
    "    df = pd.read_parquet(os.path.join(dirname, filename, 'part-0.parquet'))\n",
    "    df.drop('step', axis=1, inplace=True)\n",
    "    return df.describe().values.reshape(-1), filename.split('=')[1]\n",
    "\n",
    "def read_file(filename, dirname):\n",
    "    df = pd.read_parquet(os.path.join(dirname, filename, 'part-0.parquet'))\n",
    "    df.drop('step', axis=1, inplace=True)\n",
    "    return df.describe().values.reshape(-1), filename.split('=')[1]\n",
    "\n",
    "def load_time_series(dirname) -> pd.DataFrame:\n",
    "    ids = os.listdir(dirname)\n",
    "    \n",
    "    with ThreadPoolExecutor() as executor:\n",
    "        results = list(tqdm(executor.map(lambda fname: process_file(fname, dirname), ids), total=len(ids)))\n",
    "    \n",
    "    stats, indexes = zip(*results)\n",
    "    \n",
    "    df = pd.DataFrame(stats, columns=[f\"stat_{i}\" for i in range(len(stats[0]))])\n",
    "    df['id'] = indexes\n",
    "    return df\n",
    "\n",
    "\n",
    "class AutoEncoder(nn.Module):\n",
    "    def __init__(self, input_dim, encoding_dim):\n",
    "        super(AutoEncoder, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, encoding_dim*3),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(encoding_dim*3, encoding_dim*2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(encoding_dim*2, encoding_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(encoding_dim, input_dim*2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(input_dim*2, input_dim*3),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(input_dim*3, input_dim),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return decoded\n",
    "\n",
    "\n",
    "def perform_autoencoder(df, encoding_dim=50, epochs=50, batch_size=32):\n",
    "    scaler = StandardScaler()\n",
    "    df_scaled = scaler.fit_transform(df)\n",
    "    \n",
    "    data_tensor = torch.FloatTensor(df_scaled)\n",
    "    \n",
    "    input_dim = data_tensor.shape[1]\n",
    "    autoencoder = AutoEncoder(input_dim, encoding_dim)\n",
    "    \n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(autoencoder.parameters())\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        for i in range(0, len(data_tensor), batch_size):\n",
    "            batch = data_tensor[i : i + batch_size]\n",
    "            optimizer.zero_grad()\n",
    "            reconstructed = autoencoder(batch)\n",
    "            loss = criterion(reconstructed, batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f'Epoch [{epoch + 1}/{epochs}], Loss: {loss.item():.4f}]')\n",
    "                 \n",
    "    with torch.no_grad():\n",
    "        encoded_data = autoencoder.encoder(data_tensor).numpy()\n",
    "        \n",
    "    df_encoded = pd.DataFrame(encoded_data, columns=[f'Enc_{i + 1}' for i in range(encoded_data.shape[1])])\n",
    "    \n",
    "    return df_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tsEngineering_1():\n",
    "    #load_time_series(\"series_train.parquet\").to_csv('train_ts.csv', index=False)\n",
    "    #load_time_series(\"series_test.parquet\").to_csv('test_ts.csv', index=False)\n",
    "    df_train = pd.read_csv('train_ts.csv')\n",
    "    df_test = pd.read_csv('test_ts.csv')\n",
    "\n",
    "    return df_train, df_test\n",
    "\n",
    "def tsEngineering_2(df_train, df_test):\n",
    "    train_ts_encoded = perform_autoencoder(df_train.drop('id', axis=1), encoding_dim=60, epochs=100, batch_size=32)\n",
    "    test_ts_encoded = perform_autoencoder(df_test.drop('id', axis=1), encoding_dim=60, epochs=100, batch_size=32)\n",
    "\n",
    "    time_series_cols = train_ts_encoded.columns.tolist()\n",
    "    train_ts_encoded[\"id\"]=df_train[\"id\"]\n",
    "    test_ts_encoded['id']=df_test[\"id\"]\n",
    "\n",
    "    return train_ts_encoded, test_ts_encoded\n",
    "\n",
    "def tsEngineering_3(num_clusters = 5, dirname = \"series_train.parquet\"):\n",
    "    # agglomerative clustering using DTW-python\n",
    "    \n",
    "    def read_file(filename, dirname):\n",
    "        accel =  pd.read_parquet(os.path.join(dirname, filename, 'part-0.parquet')).iloc[:5000,1:4].values\n",
    "        return accel\n",
    "\n",
    "    def compare_time_series(center, distances, dirname) -> pd.DataFrame:\n",
    "        ids = os.listdir(dirname)\n",
    "        a = read_file(ids[center], dirname)\n",
    "\n",
    "        for fname in ids:\n",
    "            try:\n",
    "                b = read_file(fname, dirname)\n",
    "                distance, path = fastdtw(a, b, dist=euclidean)\n",
    "                distances[-1].append(distance)\n",
    "            except:\n",
    "                distances[-1].append(0)\n",
    "\n",
    "        return ids, distances\n",
    "\n",
    "    # Finding the reference clustering center (O(5n) = 1hr15mins)\n",
    "    centers = [0]\n",
    "    center = 0\n",
    "    distances = []\n",
    "    for i in range(num_clusters-1):\n",
    "        distances.append([])\n",
    "        ids, distances = compare_time_series(center, distances, dirname)\n",
    "        distance_np = np.array(distances)\n",
    "        distance_np *= (distance_np > distance_np.mean(axis=1)[:,None])\n",
    "        for j in centers:\n",
    "            distance_np[:,j] = 0\n",
    "            center = np.argmax(distance_np.sum(axis=0))\n",
    "        center = np.argmax(np.array(distances).sum(axis=0))\n",
    "        centers.append(center)\n",
    "        print(center)\n",
    "\n",
    "    # Finding the clustering center (O(n))\n",
    "    clusters = np.argmin(distances,axis=0)\n",
    "    centers = list(map(int, centers))\n",
    "    indexes = [x.split('=')[1] for x in ids]\n",
    "    classify_train_ts = pd.DataFrame({'id':indexes, 'cluster':clusters})\n",
    "\n",
    "    # Applying All Images to test data\n",
    "    test_dirname = 'series_test.parquet'\n",
    "    test_ids = os.listdir(test_dirname)\n",
    "    data_list = []\n",
    "    for center in centers:\n",
    "        data_list.append(read_file(ids[center], dirname))\n",
    "    \n",
    "    clusters = []\n",
    "    for fname in test_ids:\n",
    "        distances = []\n",
    "        for data in data_list:\n",
    "            try:\n",
    "                b = read_file(fname, test_dirname)\n",
    "                distance, path = fastdtw(data[:-1], b, dist=euclidean)\n",
    "                distances += [distance]\n",
    "            except:\n",
    "                distances += [1e99]\n",
    "        clusters.append(np.argmin(distances))\n",
    "\n",
    "    indexes = [x.split('=')[1] for x in test_ids]\n",
    "    classify_test_ts = pd.DataFrame({'id':indexes, 'cluster':clusters})\n",
    "    return classify_train_ts, classify_test_ts\n",
    "\n",
    "def tsEngineering_4(dirname = \"series_train.parquet\"):\n",
    "    \n",
    "    def read_file(filename, dirname):\n",
    "        accel =  pd.read_parquet(os.path.join(dirname, filename, 'part-0.parquet')).iloc[:5000,1:4].values\n",
    "        return accel\n",
    "    \n",
    "    def read_reference() -> list:\n",
    "        # create reference images\n",
    "        data_list = []\n",
    "        for file in ['1468360517106-acc-watch_tv.txt','1468367368314-acc-eat_chips.txt','1469751427337-acc-sweep.txt',\n",
    "                        '1468379023854-acc-brush_teeth.txt','1471982025220-acc-wash_hands.txt',\n",
    "                        '1468683416878-acc-type_on_keyboard.txt','1471982629473-acc-mop_floor.txt']:\n",
    "            with open(f'user/{file}','r+') as f:\n",
    "                data = f.read()\n",
    "                data_list.append([list(map(float, x.split(',')[1:])) for x in data.split('\\n')])\n",
    "        return data_list\n",
    "    \n",
    "    ids = os.listdir(dirname)\n",
    "    data_list = read_reference()\n",
    "\n",
    "    clusters = []\n",
    "    for fname in ids:\n",
    "        distances = []\n",
    "        for data in data_list:\n",
    "            try:\n",
    "                b = read_file(fname, dirname)\n",
    "                distance, path = fastdtw(data[:-1], b, dist=euclidean)\n",
    "                distances += [distance]\n",
    "            except:\n",
    "                distances += [1e99]\n",
    "        clusters.append(np.argmin(distances))\n",
    "\n",
    "    indexes = [x.split('=')[1] for x in ids]\n",
    "    classify_ts_df = pd.DataFrame({'id':indexes, 'cluster':clusters})\n",
    "    return classify_ts_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "classify_train_ts, classify_test_ts = tsEngineering_3(num_clusters=5, dirname=\"series_train.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classify_train_ts = tsEngineering_4('series_train.parquet')\n",
    "classify_test_ts = tsEngineering_4('series_test.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.merge(train, classify_train_ts, on='id', how='left')\n",
    "test = pd.merge(test, classify_test_ts, on='id', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1468360517106-acc-watch_tv.txt\n",
      "1468367368314-acc-eat_chips.txt\n",
      "1469751427337-acc-sweep.txt\n",
      "1468379023854-acc-brush_teeth.txt\n",
      "1471983192578-acc-brush_teeth.txt\n",
      "1471982025220-acc-wash_hands.txt\n",
      "1468683416878-acc-type_on_keyboard.txt\n",
      "1471982629473-acc-mop_floor.txt\n",
      "1469751995281-acc-type_on_keyboard.txt\n"
     ]
    }
   ],
   "source": [
    "# preparing external data for tsEngineering_4()\n",
    "def read_file(file):\n",
    "    with open(f'user/{file}','r+') as f:\n",
    "        data = f.read()\n",
    "        return [list(map(float, x.split(',')[1:])) for x in data.split('\\n')]\n",
    "\n",
    "def compare_time_series(center, distances) -> pd.DataFrame:\n",
    "    ids = os.listdir('user')\n",
    "    a = read_file(ids[center])[:-1]\n",
    "\n",
    "    for fname in ids:\n",
    "        try:\n",
    "            b = read_file(fname)[:-1]\n",
    "            distance, path = fastdtw(a, b, dist=euclidean)\n",
    "            distances[-1].append(distance)\n",
    "        except:\n",
    "            distances[-1].append(0)\n",
    "\n",
    "    return ids, distances\n",
    "'''\n",
    "# Finding the clustering center (O(5n) = 1hr15mins)\n",
    "num_clusters = 7\n",
    "centers = [0]\n",
    "center = 0\n",
    "distances = []\n",
    "for i in range(num_clusters):\n",
    "    distances.append([])\n",
    "    ids, distances = compare_time_series(center, distances)\n",
    "    distance_np = np.array(distances)\n",
    "    distance_np *= (distance_np > distance_np.mean(axis=1)[:,None])\n",
    "    for j in centers:\n",
    "        distance_np[:,j] = 0\n",
    "    center = np.argmax(distance_np.sum(axis=0))\n",
    "    centers.append(center)\n",
    "    print(center)\n",
    "\n",
    "# Finding the clustering center (O(n))\n",
    "distances = np.array(distances)\n",
    "clusters = distances.argmin(axis=0)\n",
    "for i in list(map(int,centers)):\n",
    "    print(ids[i])'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/100], Loss: 0.6958]\n",
      "Epoch [20/100], Loss: 0.6336]\n",
      "Epoch [30/100], Loss: 0.6242]\n",
      "Epoch [40/100], Loss: 0.6203]\n",
      "Epoch [50/100], Loss: 0.6110]\n",
      "Epoch [60/100], Loss: 0.6083]\n",
      "Epoch [70/100], Loss: 0.6013]\n",
      "Epoch [80/100], Loss: 0.6013]\n",
      "Epoch [90/100], Loss: 0.6007]\n",
      "Epoch [100/100], Loss: 0.5969]\n",
      "Epoch [10/100], Loss: 1.0135]\n",
      "Epoch [20/100], Loss: 0.6371]\n",
      "Epoch [30/100], Loss: 0.4271]\n",
      "Epoch [40/100], Loss: 0.4271]\n",
      "Epoch [50/100], Loss: 0.4271]\n",
      "Epoch [60/100], Loss: 0.4271]\n",
      "Epoch [70/100], Loss: 0.4271]\n",
      "Epoch [80/100], Loss: 0.4271]\n",
      "Epoch [90/100], Loss: 0.4271]\n",
      "Epoch [100/100], Loss: 0.4271]\n"
     ]
    }
   ],
   "source": [
    "df_train, df_test = tsEngineering_1()\n",
    "train_ts_encoded, test_ts_encoded = tsEngineering_2(df_train, df_test)\n",
    "train_w_ts = pd.merge(train.reset_index(), train_ts_encoded, how=\"left\", on='id')\n",
    "test_w_ts = pd.merge(test.reset_index(), test_ts_encoded, how=\"left\", on='id')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imputation of Missing values\n",
    "\n",
    "\n",
    "|input|output|\n",
    "|---|---|\n",
    "|data_df|data_df without missing values|\n",
    "\n",
    "experimentations\n",
    "|function name|description|\n",
    "|---|---|\n",
    "|imputation_1|MICE Imputer|\n",
    "|imputation_2|SimpleImputer|\n",
    "|imputation_3|Statistical Imputation|\n",
    "|imputation_4|KNNImputer|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imputation_1(train,test):\n",
    "  # Multivariate Imputation with Chain-Equation\n",
    "  from sklearn.experimental import enable_iterative_imputer\n",
    "  from sklearn.impute import IterativeImputer\n",
    "  \n",
    "  imputer = IterativeImputer(max_iter=10, random_state=0)\n",
    "  mice_train = train.select_dtypes(include=['float64', 'int64'])\n",
    "  mice_test = test.select_dtypes(include=['float64', 'int64'])\n",
    "\n",
    "  mice_train = imputer.fit_transform(mice_train)\n",
    "  mice_test = imputer.fit_transform(mice_test)\n",
    "  mice_train_df = pd.DataFrame(mice_train, columns=train.columns)\n",
    "  mice_test_df = pd.DataFrame(mice_test, columns=test.columns)\n",
    "\n",
    "  return mice_train_df,mice_test_df\n",
    "\n",
    "def imputation_2(train,test):\n",
    "  from sklearn.impute import SimpleImputer\n",
    "\n",
    "  # Initialize the SimpleImputer\n",
    "  simple_imputer = SimpleImputer(strategy='mean')  # You can change 'mean' to 'median', 'most_frequent', or 'constant'\n",
    "\n",
    "  # Perform SimpleImputer imputation\n",
    "  simple_train = simple_imputer.fit_transform(train.select_dtypes(include=['float64', 'int64']))\n",
    "  simple_test = simple_imputer.fit_transform(test.select_dtypes(include=['float64', 'int64']))\n",
    "  simple_train_df = pd.DataFrame(simple_train, columns=train.columns)\n",
    "  simple_test_df = pd.DataFrame(simple_test, columns=test.columns)\n",
    "  \n",
    "  return simple_train_df, simple_test_df\n",
    "\n",
    "def imputation_3(train,test):\n",
    "  train_stats = EDA_4(train)\n",
    "\n",
    "  def statsImpute(row):\n",
    "    # Find column that has missing value\n",
    "    missing = row.isnull()\n",
    "    for missing_col in row[missing].index:\n",
    "      age = row['Basic_Demos-Age']\n",
    "      mean = train_stats[missing_col]['mean'].loc[age]\n",
    "      std = train_stats[missing_col]['std'].loc[age]\n",
    "      # Fill in value with random value from normal distribution\n",
    "      row[missing_col] = random.gauss(mean,std)\n",
    "    return row\n",
    "\n",
    "  stats_train_df = train.apply(statsImpute, axis=1)\n",
    "  stats_train_df = stats_train_df.fillna(-1)\n",
    "  stats_test_df = test.apply(statsImpute, axis=1)\n",
    "  stats_test_df = stats_test_df.fillna(-1)\n",
    "  return stats_train_df, stats_test_df\n",
    "\n",
    "\n",
    "def imputation_4(train,test):\n",
    "  from sklearn.impute import KNNImputer\n",
    "\n",
    "  # Initialize the KNNImputer\n",
    "  knn_imputer = KNNImputer(n_neighbors=5)\n",
    "\n",
    "  # Perform KNN imputation\n",
    "  knn_train = knn_imputer.fit_transform(train.select_dtypes(include=['float64', 'int64']))\n",
    "  knn_test = knn_imputer.fit_transform(test.select_dtypes(include=['float64', 'int64']))\n",
    "  knn_train_df = pd.DataFrame(knn_train, columns=train.columns)\n",
    "  knn_test_df = pd.DataFrame(knn_test, columns=test.columns)\n",
    "  return knn_train_df, knn_test_df\n",
    "\n",
    "train,test = imputation_3(train,test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "81\n"
     ]
    }
   ],
   "source": [
    "# Code for finding optimal thresholds copied from: Michael Semenoff\n",
    "# https://www.kaggle.com/code/michaelsemenoff/cmi-actigraphy-feature-engineering-selection\n",
    "\n",
    "SEED = 9365\n",
    "n_splits = 10\n",
    "optimize_params = False\n",
    "n_trials = 25 # n_trials for optuna \n",
    "voting = True\n",
    "base_thresholds = [30, 50, 80]\n",
    "\n",
    "def round_with_thresholds(raw_preds, thresholds):\n",
    "    return np.where(raw_preds < thresholds[0], int(0),\n",
    "                    np.where(raw_preds < thresholds[1], int(1),\n",
    "                             np.where(raw_preds < thresholds[2], int(2), int(3))))\n",
    "\n",
    "def optimize_thresholds(y_true, raw_preds, start_vals=[0.5, 1.5, 2.5]):\n",
    "    def fun(thresholds, y_true, raw_preds):\n",
    "        rounded_preds = round_with_thresholds(raw_preds, thresholds)\n",
    "        return -cohen_kappa_score(y_true, rounded_preds, weights='quadratic')\n",
    "\n",
    "    res = minimize(fun, x0=start_vals, args=(y_true, raw_preds), method='Powell')\n",
    "    assert res.success\n",
    "    return res.x\n",
    "def calculate_weights(series):\n",
    "    # Create bins for the target variable and assign weights based on frequency\n",
    "    bins = pd.cut(series, bins=10, labels=False)\n",
    "    weights = bins.value_counts().reset_index()\n",
    "    weights.columns = ['target_bins', 'count']\n",
    "    weights['count'] = 1 / weights['count']\n",
    "    weight_map = weights.set_index('target_bins')['count'].to_dict()\n",
    "    weights = bins.map(weight_map)\n",
    "    return weights / weights.mean() \n",
    "def cross_validate(model_, data, features, score_col, index_col, cv, sample_weights=False, verbose=False):\n",
    "    \"\"\"\n",
    "    Perform cross-validation with a given model and compute the out-of-fold \n",
    "    predictions and Cohen's Kappa score for each fold.\n",
    "\n",
    "    Returns:\n",
    "    float: Mean Kappa score across all folds.\n",
    "    array: Out-of-fold score predictions for the entire dataset.\n",
    "    \"\"\"\n",
    "    kappa_scores = [] \n",
    "    oof_score_predictions = np.zeros(len(data))  \n",
    "\n",
    "    score_to_index_thresholds = base_thresholds  \n",
    "    thresholds = []\n",
    "    for fold_idx, (train_idx, val_idx) in enumerate(cv.split(data, data[index_col])):\n",
    "        X_train, X_val = data[features].iloc[train_idx], data[features].iloc[val_idx]\n",
    "        y_train_score = data[score_col].iloc[train_idx] \n",
    "        y_train_index = data[index_col].iloc[train_idx]\n",
    "        y_val_score = data[score_col].iloc[val_idx]      \n",
    "        y_val_index = data[index_col].iloc[val_idx]     \n",
    "        \n",
    "        # Train model with sample weights if provided\n",
    "        if sample_weights:\n",
    "            weights = calculate_weights(y_train_score)\n",
    "            model_.fit(X_train, y_train_score, sample_weight=weights)\n",
    "        else:\n",
    "            model_.fit(X_train, y_train_score)\n",
    "\n",
    "        y_pred_train_score = model_.predict(X_train)\n",
    "        y_pred_val_score = model_.predict(X_val)\n",
    "        \n",
    "        oof_score_predictions[val_idx] = y_pred_val_score \n",
    "\n",
    "        # Find optimal threshold in sample \n",
    "        t_1 = optimize_thresholds(y_train_index, y_pred_train_score, start_vals=base_thresholds)\n",
    "        thresholds.append(t_1)\n",
    "\n",
    "        y_pred_val_index = round_with_thresholds(y_pred_val_score, t_1)\n",
    "\n",
    "        kappa_score = cohen_kappa_score(y_val_index, y_pred_val_index, weights='quadratic')\n",
    "        kappa_scores.append(kappa_score)\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"Fold {fold_idx}: Optimized Kappa Score = {kappa_score}\")\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"## Mean CV Kappa Score: {np.mean(kappa_scores)} ##\")\n",
    "        print(f\"## Std CV: {np.std(kappa_scores)}\")\n",
    "    \n",
    "    return np.mean(kappa_scores), oof_score_predictions, thresholds\n",
    "\n",
    "def n_cross_validate(model_, data, features, score_col, index_col, cv, seeds, sample_weights=False, verbose=False):\n",
    "    scores = []\n",
    "    for seed in seeds:\n",
    "        cv.random_state=seed\n",
    "        score, oof, _ = cross_validate(model_, data, features, score_col, index_col, cv, sample_weights=True, verbose=False)\n",
    "        scores.append(score)\n",
    "    return score, oof\n",
    "def objective(trial, model_type, X, features, score_col, index_col, cv, sample_weights=False):\n",
    "    # Parameter space to explore if model is xgboost\n",
    "    if model_type == 'xgboost':\n",
    "        params = {\n",
    "            'objective': trial.suggest_categorical('objective', ['reg:tweedie', 'reg:pseudohubererror']),\n",
    "            'random_state': SEED,\n",
    "            'num_parallel_tree': trial.suggest_int('num_parallel_tree', 2, 30),\n",
    "            'n_estimators': trial.suggest_int('n_estimators', 100, 300),\n",
    "            'max_depth': trial.suggest_int('max_depth', 2, 4),\n",
    "            'learning_rate': trial.suggest_loguniform('learning_rate', 0.02, 0.05),\n",
    "            'subsample': trial.suggest_float('subsample', 0.5, 0.8),\n",
    "            'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 0.8),\n",
    "            'reg_alpha': trial.suggest_loguniform('reg_alpha', 1e-5, 1e-1),\n",
    "            'reg_lambda': trial.suggest_loguniform('reg_lambda', 1e-5, 1e-1),\n",
    "        }\n",
    "        if params['objective'] == 'reg:tweedie':\n",
    "            params['tweedie_variance_power'] = trial.suggest_float('tweedie_variance_power', 1, 2)\n",
    "        model = XGBRegressor(**params, use_label_encoder=False)\n",
    "    \n",
    "    # Parameter space to explore if model is lightgbm\n",
    "    elif model_type == 'lightgbm':\n",
    "        params = {\n",
    "            'objective': trial.suggest_categorical('objective', ['poisson', 'tweedie', 'regression']),\n",
    "            'random_state': SEED,\n",
    "            'verbosity': -1,\n",
    "            'n_estimators': trial.suggest_int('n_estimators', 100, 300),\n",
    "            'max_depth': trial.suggest_int('max_depth', 2, 4),\n",
    "            'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.05),\n",
    "            'subsample': trial.suggest_float('subsample', 0.5, 0.8),\n",
    "            'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 0.8),\n",
    "            'min_data_in_leaf': trial.suggest_int('min_data_in_leaf', 20, 100)\n",
    "        }\n",
    "        if params['objective'] == 'tweedie':\n",
    "            params['tweedie_variance_power'] = trial.suggest_float('tweedie_variance_power', 1, 2)\n",
    "        model = LGBMRegressor(**params)\n",
    "    \n",
    "    # Parameter space to explore if model is catboost\n",
    "    '''elif model_type == 'catboost':\n",
    "        params = {\n",
    "            'loss_function': trial.suggest_categorical('objective', ['Tweedie:variance_power=1.5', \n",
    "                                                                     'Poisson', 'RMSE']),\n",
    "            'random_state': SEED,\n",
    "            'iterations': trial.suggest_int('iterations', 100, 300),\n",
    "            'depth': trial.suggest_int('depth', 2, 4),\n",
    "            'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.05),\n",
    "            'l2_leaf_reg': trial.suggest_loguniform('l2_leaf_reg', 1e-3, 1e-1),\n",
    "            'subsample': trial.suggest_float('subsample', 0.5, 0.7),\n",
    "            'bagging_temperature': trial.suggest_float('bagging_temperature', 0.0, 1.0),\n",
    "            'random_strength': trial.suggest_float('random_strength', 1e-3, 10.0),\n",
    "            'min_data_in_leaf': trial.suggest_int('min_data_in_leaf', 20, 60),\n",
    "        }\n",
    "        model = CatBoostRegressor(**params, verbose=0)\n",
    "    \n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported model_type: {model_type}\")'''\n",
    "        \n",
    "    seeds = [random.randint(1, 10000) for _ in range(20)]\n",
    "\n",
    "    score, _ = n_cross_validate(model, X, features, score_col, index_col, cv, seeds, sample_weights=True, verbose=True)\n",
    "\n",
    "    return score\n",
    "\n",
    "def run_optimization(X, features, score_col, index_col, model_type, n_trials=30, cv=None, sample_weights=False):\n",
    "    study = optuna.create_study(direction=\"maximize\")\n",
    "    study.optimize(lambda trial: objective(trial, model_type, X, features, score_col, index_col, cv, sample_weights), \n",
    "                   n_trials=n_trials)\n",
    "    \n",
    "    print(f\"Best params for {model_type}: {study.best_params}\")\n",
    "    print(f\"Best score: {study.best_value}\")\n",
    "    return study.best_params\n",
    "# Replace if subsets for features have been selected\n",
    "exclude = [\"PC_9\", \"PC_12\", \"Fitness_Endurance-Max_Stage\", \"Basic_Demos-Sex\", 'BMI_mean_norm', \"PC_11\", \"PC_8\", \"FGC_Zones_min\", 'Physical-Systolic_BP',\n",
    "           \"PC_4\", \"BIA-BIA_FMI\", \"BIA-BIA_LST\", \"Physical-Diastolic_BP\", 'BIA-BIA_ECW', 'Fitness_Endurance-Time_Mins', 'PAQ_C-PAQ_C_Total', 'PC_10',\n",
    "           'BIA-BIA_Fat', 'FFM_norm', 'PC_14', 'PC_7']\n",
    "exclude = []\n",
    "\n",
    "features = train.columns.tolist()\n",
    "reduced_features = [f for f in features if f not in exclude]\n",
    "\n",
    "lgb_features = reduced_features\n",
    "xgb_features = reduced_features\n",
    "cat_features = reduced_features\n",
    "print(len(reduced_features))\n",
    "\n",
    "# Parameters for LGBM, XGB and CatBoost\n",
    "lgb_params = {\n",
    "    'objective': 'poisson', \n",
    "    'n_estimators': 295, \n",
    "    'max_depth': 4, \n",
    "    'learning_rate': 0.04505693066482616, \n",
    "    'subsample': 0.6042489155604022, \n",
    "    'colsample_bytree': 0.5021876720502726, \n",
    "    'min_data_in_leaf': 100\n",
    "}\n",
    "\n",
    "xgb_params = {'objective': 'reg:tweedie', 'num_parallel_tree': 12, 'n_estimators': 236, 'max_depth': 3, 'learning_rate': 0.04223740904479563, 'subsample': 0.7157264603586825, 'colsample_bytree': 0.7897918901977528, 'reg_alpha': 0.005335705058190553, 'reg_lambda': 0.0001897435318347022, 'tweedie_variance_power': 1.1393958601390142}\n",
    "\n",
    "xgb_params_2 = {\n",
    "    'objective': 'reg:tweedie', \n",
    "    'num_parallel_tree': 18, \n",
    "    'n_estimators': 175, \n",
    "    'max_depth': 3, \n",
    "    'learning_rate': 0.032620453423049305, \n",
    "    'subsample': 0.6155579670568023, \n",
    "    'colsample_bytree': 0.5988773292417443, \n",
    "    'reg_alpha': 0.0028895066837627205, \n",
    "    'reg_lambda': 0.002232531512636924, \n",
    "    'tweedie_variance_power': 1.1708678482038286\n",
    "}\n",
    "\n",
    "cat_params = {\n",
    "    'objective': 'RMSE', \n",
    "    'iterations': 238, \n",
    "    'depth': 4, \n",
    "    'learning_rate': 0.044523361750173816, \n",
    "    'l2_leaf_reg': 0.09301285673435761, \n",
    "    'subsample': 0.6902492783438681, \n",
    "    'bagging_temperature': 0.3007304771330199, \n",
    "    'random_strength': 3.562201626987314, \n",
    "    'min_data_in_leaf': 60\n",
    "}\n",
    "\n",
    "xtrees_params = {\n",
    "    'n_estimators': 500, \n",
    "    'max_depth': 15, \n",
    "    'min_samples_leaf': 20, \n",
    "    'bootstrap': False\n",
    "}\n",
    "kf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=SEED)\n",
    "if optimize_params:\n",
    "    # LightGBM Optimization\n",
    "    lgb_params = run_optimization(train, lgb_features, 'PCIAT-PCIAT_Total', 'sii', 'lightgbm', n_trials=n_trials, cv=kf, sample_weights=True)\n",
    "\n",
    "    # XGBoost Optimization\n",
    "    xgb_params = run_optimization(train, xgb_features, 'PCIAT-PCIAT_Total', 'sii', 'xgboost', n_trials=n_trials, cv=kf, sample_weights=True)\n",
    "\n",
    "    # CatBoost Optimization\n",
    "    #cat_params = run_optimization(train, cat_features, 'PCIAT-PCIAT_Total', 'sii', 'catboost', n_trials=n_trials, cv=kf, sample_weights=True)\n",
    "# Define models\n",
    "lgb_model = LGBMRegressor(**lgb_params, random_state=SEED, verbosity=-1)\n",
    "xgb_model = XGBRegressor(**xgb_params, random_state=SEED, verbosity=0)\n",
    "xgb_model_2 = XGBRegressor(**xgb_params_2, random_state=SEED, verbosity=0)\n",
    "#cat_model = CatBoostRegressor(**cat_params, random_state=SEED, verbose=0)\n",
    "xtrees_model = ExtraTreesRegressor(**xtrees_params, random_state=SEED)\n",
    "\n",
    "\n",
    "def runModel(train,test):\n",
    "    # Drop non-float64 and non-int64 columns\n",
    "    train = train.select_dtypes(include=['float64', 'int64'])\n",
    "\n",
    "    # Continue with your existing code\n",
    "    weights = calculate_weights(train['PCIAT-PCIAT_Total'])\n",
    "\n",
    "    # Cross-validate LGBM model\n",
    "    print('LGBM model')\n",
    "    score_lgb, oof_lgb, lgb_thresholds = cross_validate(\n",
    "        lgb_model, train, lgb_features, 'PCIAT-PCIAT_Total', 'sii', kf, verbose=True, sample_weights=True\n",
    "    )\n",
    "    lgb_model.fit(train[lgb_features], train['PCIAT-PCIAT_Total'], sample_weight=weights)\n",
    "    test_lgb = lgb_model.predict(test[lgb_features])\n",
    "\n",
    "    # Cross-validate XGBoost model\n",
    "    print('XGBoost model')\n",
    "    score_xgb, oof_xgb, xgb_thresholds = cross_validate(\n",
    "        xgb_model, train, xgb_features, 'PCIAT-PCIAT_Total', 'sii', kf, verbose=True, sample_weights=True\n",
    "    )\n",
    "    xgb_model.fit(train[xgb_features], train['PCIAT-PCIAT_Total'], sample_weight=weights)\n",
    "    test_xgb = xgb_model.predict(test[xgb_features])\n",
    "\n",
    "    # Cross-validate XGBoost model 2\n",
    "    print('XGBoost model2')\n",
    "    score_xgb_2, oof_xgb_2, xgb_2_thresholds = cross_validate(\n",
    "        xgb_model_2, train, xgb_features, 'PCIAT-PCIAT_Total', 'sii', kf, verbose=True, sample_weights=True\n",
    "    )\n",
    "    xgb_model_2.fit(train[xgb_features], train['PCIAT-PCIAT_Total'], sample_weight=weights)\n",
    "    test_xgb_2 = xgb_model_2.predict(test[xgb_features])\n",
    "\n",
    "    # Cross-validate ExtraTreesRegressor model\n",
    "    print('ExtraTreesRegressor model')\n",
    "    score_xtrees, oof_xtrees, xtrees_thresholds = cross_validate(\n",
    "        xtrees_model, train, reduced_features, 'PCIAT-PCIAT_Total', 'sii', kf, verbose=True, sample_weights=True\n",
    "    )\n",
    "    xtrees_model.fit(train[reduced_features], train['PCIAT-PCIAT_Total'], sample_weight=weights)\n",
    "    test_xtrees = xtrees_model.predict(test[reduced_features])\n",
    "\n",
    "    # Print overall mean Kappa score for all models\n",
    "    print(f'Overall Mean Kappa: {np.mean([score_lgb, score_xgb, score_xtrees])}') # Ensemble score likely higher\n",
    "\n",
    "    lgb_thresholds_ens = np.mean(np.array(lgb_thresholds), axis=0)\n",
    "    xgb_thresholds_ens = np.mean(np.array(xgb_thresholds), axis=0)\n",
    "    xgb_2_thresholds_ens = np.mean(np.array(xgb_2_thresholds), axis=0)\n",
    "    xtrees_thresholds_ens = np.mean(np.array(xtrees_thresholds), axis=0)\n",
    "    test_lgb = round_with_thresholds(test_lgb, lgb_thresholds_ens)\n",
    "    test_xgb = round_with_thresholds(test_xgb, xgb_thresholds_ens)\n",
    "    test_xgb_2 = round_with_thresholds(test_xgb_2, xgb_2_thresholds_ens)\n",
    "    test_xtrees = round_with_thresholds(test_xtrees, xtrees_thresholds_ens)\n",
    "\n",
    "    if voting:\n",
    "        test_preds = np.array([test_lgb, test_xgb, test_xgb_2, test_xtrees])\n",
    "        voted_test = stats.mode(test_preds, axis=0).mode.flatten().astype(int)\n",
    "        final_test = np.round(voted_test).astype(int)\n",
    "    else:\n",
    "        test_preds = np.array([test_lgb, test_xgb, test_xgb_2, test_xtrees])\n",
    "        weighted_test = np.average(test_preds, axis=0, weights=weights)\n",
    "        final_test = np.round(weighted_test).astype(int)\n",
    "        \n",
    "    return final_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = runModel(train,test)\n",
    "\n",
    "## Submission\n",
    "submission = pd.read_csv(\"/kaggle/input/child-mind-institute-problematic-internet-use/sample_submission.csv\")\n",
    "submission['sii'] = test\n",
    "submission.to_csv(\"submission.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
