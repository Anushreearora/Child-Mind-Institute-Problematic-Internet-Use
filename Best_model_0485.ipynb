{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.11"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":81933,"databundleVersionId":9643020,"sourceType":"competition"}],"dockerImageVersionId":31011,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true},"papermill":{"default_parameters":{},"duration":609.130015,"end_time":"2025-04-17T15:38:38.677060","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2025-04-17T15:28:29.547045","version":"2.6.0"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler\nfrom sklearn.decomposition import PCA\nfrom concurrent.futures import ThreadPoolExecutor\nimport os\nfrom tqdm import tqdm\nfrom sklearn.impute import KNNImputer\nfrom sklearn.base import clone\nfrom sklearn.linear_model import LassoCV\nfrom sklearn.linear_model import LogisticRegression\n\nfrom xgboost import XGBRegressor\nfrom lightgbm import LGBMRegressor\nfrom scipy.optimize import minimize\nfrom sklearn.model_selection import KFold, StratifiedKFold\nfrom sklearn.metrics import mean_squared_error, accuracy_score, cohen_kappa_score\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.experimental import enable_iterative_imputer\nfrom sklearn.impute import IterativeImputer\nfrom sklearn.linear_model import BayesianRidge\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom catboost import CatBoostRegressor\n\nimport optuna\nfrom scipy import stats","metadata":{"execution":{"iopub.execute_input":"2025-04-17T15:28:36.309193Z","iopub.status.busy":"2025-04-17T15:28:36.308354Z","iopub.status.idle":"2025-04-17T15:28:50.469692Z","shell.execute_reply":"2025-04-17T15:28:50.468657Z"},"papermill":{"duration":14.182449,"end_time":"2025-04-17T15:28:50.471837","exception":false,"start_time":"2025-04-17T15:28:36.289388","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Constants\nSEED = 42","metadata":{"execution":{"iopub.execute_input":"2025-04-17T15:28:50.512288Z","iopub.status.busy":"2025-04-17T15:28:50.511659Z","iopub.status.idle":"2025-04-17T15:28:50.516062Z","shell.execute_reply":"2025-04-17T15:28:50.515194Z"},"papermill":{"duration":0.023347,"end_time":"2025-04-17T15:28:50.517529","exception":false,"start_time":"2025-04-17T15:28:50.494182","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Loading the datasets\ntrain_file_path = \"/kaggle/input/child-mind-institute-problematic-internet-use/train.csv\"\ntrain_df = pd.read_csv(train_file_path)\ntest_file_path = \"/kaggle/input/child-mind-institute-problematic-internet-use/test.csv\"\ntest_df = pd.read_csv(test_file_path)\ndf = train_df","metadata":{"execution":{"iopub.execute_input":"2025-04-17T15:28:50.550523Z","iopub.status.busy":"2025-04-17T15:28:50.549871Z","iopub.status.idle":"2025-04-17T15:28:50.658756Z","shell.execute_reply":"2025-04-17T15:28:50.657711Z"},"papermill":{"duration":0.127186,"end_time":"2025-04-17T15:28:50.660614","exception":false,"start_time":"2025-04-17T15:28:50.533428","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# DATA EXPLORATION","metadata":{"papermill":{"duration":0.015367,"end_time":"2025-04-17T15:28:50.692448","exception":false,"start_time":"2025-04-17T15:28:50.677081","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# Basic Info about Dataset\nprint(\"\\nðŸ”¹ Dataset Info:\")\nprint(df.info())","metadata":{"execution":{"iopub.execute_input":"2025-04-17T15:28:50.725076Z","iopub.status.busy":"2025-04-17T15:28:50.724287Z","iopub.status.idle":"2025-04-17T15:28:50.768679Z","shell.execute_reply":"2025-04-17T15:28:50.767777Z"},"papermill":{"duration":0.06299,"end_time":"2025-04-17T15:28:50.771023","exception":false,"start_time":"2025-04-17T15:28:50.708033","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Checking for Missing Values\nprint(\"\\nðŸ” Missing Values Count:\")\nprint(df.isnull().sum())","metadata":{"execution":{"iopub.execute_input":"2025-04-17T15:28:50.804860Z","iopub.status.busy":"2025-04-17T15:28:50.804191Z","iopub.status.idle":"2025-04-17T15:28:50.814846Z","shell.execute_reply":"2025-04-17T15:28:50.813911Z"},"papermill":{"duration":0.029039,"end_time":"2025-04-17T15:28:50.816372","exception":false,"start_time":"2025-04-17T15:28:50.787333","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Summary Statistics\nprint(\"\\nðŸ“Š Summary Statistics (Numerical Features):\")\nprint(df.describe())","metadata":{"execution":{"iopub.execute_input":"2025-04-17T15:28:50.853444Z","iopub.status.busy":"2025-04-17T15:28:50.852889Z","iopub.status.idle":"2025-04-17T15:28:50.991876Z","shell.execute_reply":"2025-04-17T15:28:50.990908Z"},"papermill":{"duration":0.159422,"end_time":"2025-04-17T15:28:50.993607","exception":false,"start_time":"2025-04-17T15:28:50.834185","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Identify Categorical & Numerical Columns\ncategorical_cols = df.select_dtypes(include=['object']).columns\nnumerical_cols = df.select_dtypes(include=['int64', 'float64']).columns\n\nprint(f\"\\nðŸ“ Categorical Columns: {list(categorical_cols)}\")\nprint(f\"\\nðŸ”¢ Numerical Columns: {list(numerical_cols)}\")","metadata":{"execution":{"iopub.execute_input":"2025-04-17T15:28:51.027427Z","iopub.status.busy":"2025-04-17T15:28:51.026864Z","iopub.status.idle":"2025-04-17T15:28:51.034526Z","shell.execute_reply":"2025-04-17T15:28:51.033530Z"},"papermill":{"duration":0.026023,"end_time":"2025-04-17T15:28:51.036069","exception":false,"start_time":"2025-04-17T15:28:51.010046","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"code","source":"len(numerical_cols)","metadata":{"execution":{"iopub.execute_input":"2025-04-17T15:28:51.140246Z","iopub.status.busy":"2025-04-17T15:28:51.139303Z","iopub.status.idle":"2025-04-17T15:28:51.145807Z","shell.execute_reply":"2025-04-17T15:28:51.145099Z"},"papermill":{"duration":0.026983,"end_time":"2025-04-17T15:28:51.147165","exception":false,"start_time":"2025-04-17T15:28:51.120182","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"code","source":"## Correlation\ncovariance_matrix = df.select_dtypes(include=['float64','int64']).corr()\ndisplay(covariance_matrix)\ncovariance_matrix.to_csv(\"covariance_matrix.csv\")","metadata":{"execution":{"iopub.execute_input":"2025-04-17T15:28:51.181575Z","iopub.status.busy":"2025-04-17T15:28:51.180984Z","iopub.status.idle":"2025-04-17T15:28:51.288622Z","shell.execute_reply":"2025-04-17T15:28:51.287941Z"},"papermill":{"duration":0.12643,"end_time":"2025-04-17T15:28:51.290135","exception":false,"start_time":"2025-04-17T15:28:51.163705","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"code","source":"numeric_df = train_df.select_dtypes(include='number')\n\nmin_max_df = pd.DataFrame({\n    'Min': numeric_df.min(),\n    'Max': numeric_df.max()\n}).sort_values(by='Min', ascending=True)  # or sort by 'Max' if preferred\n\nprint(min_max_df)","metadata":{"execution":{"iopub.execute_input":"2025-04-17T15:28:51.328007Z","iopub.status.busy":"2025-04-17T15:28:51.327237Z","iopub.status.idle":"2025-04-17T15:28:51.339305Z","shell.execute_reply":"2025-04-17T15:28:51.338258Z"},"papermill":{"duration":0.033032,"end_time":"2025-04-17T15:28:51.340729","exception":false,"start_time":"2025-04-17T15:28:51.307697","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# BASIC DATA CLEANING","metadata":{"papermill":{"duration":0.016822,"end_time":"2025-04-17T15:28:51.374600","exception":false,"start_time":"2025-04-17T15:28:51.357778","status":"completed"},"tags":[]}},{"cell_type":"code","source":"def clean_data(df):\n    \n    season_colums = [col for col in df.columns if 'Season' in col]\n    df = df.drop(season_colums, axis=1) # Removes columns with 'Season' in the name\n    \n    df = df.drop(columns=['Fitness_Endurance-Max_Stage', 'Fitness_Endurance-Time_Mins', 'Fitness_Endurance-Time_Sec'])\n    \n    df[\"BIA-BIA_Fat\"] = df[\"BIA-BIA_Fat\"].abs() # Replaces negative values with positive values\n\n    df[\"BIA-BIA_FMI\"] = df[\"BIA-BIA_FMI\"].abs() # Replaces negative values with positive values\n\n    df.loc[df[\"BIA-BIA_Fat\"] > 65, \"BIA-BIA_Fat\"] = np.nan # Replaces values greater than 65 with NaN - more than 65% body fat is not possible\n\n    df.loc[df[\"BIA-BIA_Fat\"] < 5, \"BIA-BIA_Fat\"] = np.nan # Replaces values less than 5 with NaN - less 5% body fat is not possible\n\n    df.loc[df[\"BIA-BIA_FMI\"] > 40, \"BIA-BIA_FMI\"] = np.nan # Replaces values greater than 40 with NaN\n\n    df[['FGC-FGC_GSND', 'FGC-FGC_GSD']] = df[['FGC-FGC_GSND', 'FGC-FGC_GSD']].clip(lower=0, upper=100) # Replaces values less than 0 with 0 and values greater than 100 with 100\n\n    df.loc[df[\"BIA-BIA_BMR\"] > 4000, \"BIA-BIA_BMR\"] = np.nan # Replaces values greater than 4000 with NaN\n\n    df.loc[df[\"BIA-BIA_DEE\"] > 8000, \"BIA-BIA_DEE\"] = np.nan # Replaces values greater than 8000 with NaN\n\n    df[\"BIA-BIA_BMC\"] = df[\"BIA-BIA_BMC\"].abs() # Replaces negative values with positive values\n\n    df[['BIA-BIA_BMC']] = df[['BIA-BIA_BMC']].clip(lower=0, upper=10) # Replaces values less than 0 with 0 and values greater than 10 with 10\n\n    df.loc[df[\"BIA-BIA_BMC\"] == 0, \"BIA-BIA_BMC\"] = np.nan # Replaces values equal to 0 with NaN\n\n    df.loc[df[\"BIA-BIA_FFM\"] > 400, \"BIA-BIA_FFM\"] = np.nan # Replaces values greater than 400 with NaN\n\n    df.loc[df[\"BIA-BIA_ECW\"] > 100, \"BIA-BIA_ECW\"] = np.nan # Replaces values greater than 100 with NaN\n\n    df.loc[df[\"BIA-BIA_ICW\"] > 100, \"BIA-BIA_ICW\"] = np.nan # Replaces values greater than 100 with NaN\n\n    df.loc[df[\"BIA-BIA_LDM\"] > 100, \"BIA-BIA_LDM\"] = np.nan # Replaces values greater than 100 with NaN\n\n    df.loc[df[\"BIA-BIA_LST\"] > 300, \"BIA-BIA_LST\"] = np.nan # Replaces values greater than 300 with NaN\n\n    df.loc[df[\"BIA-BIA_TBW\"] > 300, \"BIA-BIA_TBW\"] = np.nan # Replaces values greater than 300 with NaN\n\n    df.loc[df[\"BIA-BIA_SMM\"] > 300, \"BIA-BIA_SMM\"] = np.nan # Replaces values greater than 300 with NaN\n\n    df.loc[df[\"Physical-Height\"] == 0, \"Physical-Height\"] = np.nan # Replaces values equal to 0 with NaN\n\n    df.loc[df[\"Physical-Weight\"] == 0, \"Physical-Weight\"] = np.nan # Replaces values equal to 0 with NaN\n    \n    df.loc[df['CGAS-CGAS_Score'] == 999, 'CGAS-CGAS_Score'] = np.nan #Replace the single outlier CGAS Score with NAN\n\n    df['PAQ_Total'] = df['PAQ_C-PAQ_C_Total'].fillna(df['PAQ_A-PAQ_A_Total']) # Fills missing values in PAQ_Total with values from PAQ_A-PAQ_A_Total\n\n    df = df.drop('PAQ_C-PAQ_C_Total', axis=1) \n    df = df.drop('PAQ_A-PAQ_A_Total', axis=1)\n    \n    # Removes columns with 'Zone' in the name because they are not properly assigned - there is a lot of noise in the data.\n    df = df.drop(columns=['FGC-FGC_CU_Zone', 'FGC-FGC_GSND_Zone', 'FGC-FGC_GSD_Zone', 'FGC-FGC_PU_Zone', 'FGC-FGC_SRL_Zone', 'FGC-FGC_SRR_Zone', 'FGC-FGC_TL_Zone'])\n    \n    return df\n\ntrain_df = clean_data(train_df)\ntest_df = clean_data(test_df) ","metadata":{"execution":{"iopub.execute_input":"2025-04-17T15:28:51.410183Z","iopub.status.busy":"2025-04-17T15:28:51.409882Z","iopub.status.idle":"2025-04-17T15:28:51.463043Z","shell.execute_reply":"2025-04-17T15:28:51.462007Z"},"papermill":{"duration":0.073541,"end_time":"2025-04-17T15:28:51.464921","exception":false,"start_time":"2025-04-17T15:28:51.391380","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Quick function to display the statistics of a column\ndef get_statistic(df_col):\n    print(df_col.describe())\n\ndef assign_groups(age):\n    thresholds = [5, 10, 15, 18, 22] # Define the age thresholds for grouping.\n    for i, j in enumerate(thresholds):\n        if age <= j:\n            return i\n    return np.nan\n\ndef group_statistics(df, column):\n    mean_value = df.groupby('age_group')[column].mean() # Calculate the mean value of the column for each age group.\n    print(mean_value)\n\ntrain_df['age_group'] = train_df['Basic_Demos-Age'].apply(assign_groups) # Apply the function to the 'Demographics-Age' column.\ntest_df['age_group'] = test_df['Basic_Demos-Age'].apply(assign_groups) # Apply the function to the 'Demographics-Age' column.\n\n","metadata":{"execution":{"iopub.execute_input":"2025-04-17T15:28:51.502217Z","iopub.status.busy":"2025-04-17T15:28:51.501350Z","iopub.status.idle":"2025-04-17T15:28:51.510812Z","shell.execute_reply":"2025-04-17T15:28:51.510160Z"},"papermill":{"duration":0.02932,"end_time":"2025-04-17T15:28:51.512164","exception":false,"start_time":"2025-04-17T15:28:51.482844","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# DATA IMPUTATION","metadata":{"papermill":{"duration":0.016909,"end_time":"2025-04-17T15:28:51.546451","exception":false,"start_time":"2025-04-17T15:28:51.529542","status":"completed"},"tags":[]}},{"cell_type":"code","source":"pd.reset_option('display.max_rows')","metadata":{"execution":{"iopub.execute_input":"2025-04-17T15:28:51.582424Z","iopub.status.busy":"2025-04-17T15:28:51.581960Z","iopub.status.idle":"2025-04-17T15:28:51.586182Z","shell.execute_reply":"2025-04-17T15:28:51.585261Z"},"papermill":{"duration":0.024082,"end_time":"2025-04-17T15:28:51.587708","exception":false,"start_time":"2025-04-17T15:28:51.563626","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_df","metadata":{"execution":{"iopub.execute_input":"2025-04-17T15:28:51.623915Z","iopub.status.busy":"2025-04-17T15:28:51.623621Z","iopub.status.idle":"2025-04-17T15:28:51.656238Z","shell.execute_reply":"2025-04-17T15:28:51.655333Z"},"papermill":{"duration":0.052484,"end_time":"2025-04-17T15:28:51.657715","exception":false,"start_time":"2025-04-17T15:28:51.605231","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"code","source":"pd.set_option('display.max_rows', None)\n\nmissing_cols = train_df.isna().sum()\nmissing_cols = missing_cols[missing_cols > 0]\nprint(missing_cols)","metadata":{"execution":{"iopub.execute_input":"2025-04-17T15:28:51.695039Z","iopub.status.busy":"2025-04-17T15:28:51.694706Z","iopub.status.idle":"2025-04-17T15:28:51.703920Z","shell.execute_reply":"2025-04-17T15:28:51.702916Z"},"papermill":{"duration":0.029484,"end_time":"2025-04-17T15:28:51.705425","exception":false,"start_time":"2025-04-17T15:28:51.675941","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"code","source":"corr = pd.DataFrame(train_df.select_dtypes(include='number').corr()['PCIAT-PCIAT_Total'].sort_values(ascending = False))\ncorr","metadata":{"execution":{"iopub.execute_input":"2025-04-17T15:28:51.743448Z","iopub.status.busy":"2025-04-17T15:28:51.742777Z","iopub.status.idle":"2025-04-17T15:28:51.803064Z","shell.execute_reply":"2025-04-17T15:28:51.802197Z"},"papermill":{"duration":0.081049,"end_time":"2025-04-17T15:28:51.804774","exception":false,"start_time":"2025-04-17T15:28:51.723725","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Height and Weight using KNN Imputation","metadata":{"papermill":{"duration":0.019064,"end_time":"2025-04-17T15:28:51.842728","exception":false,"start_time":"2025-04-17T15:28:51.823664","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# 1. Getting all columns with missing values\nmissing_cols = train_df.columns[train_df.isna().any()]\n\n# 2. Getting the correlation matrix (numeric columns only)\ncorr_matrix = train_df.corr(numeric_only=True)\n\n# 3. Defining our target columns\ntarget_cols = ['Physical-Height', 'Physical-Weight']\n\n# 4. Finding non-missing columns\nnon_missing_cols = [col for col in train_df.columns if col not in missing_cols]\n\n# 5. For each target, getting sorted correlations with non-missing columns\nfor target in target_cols:\n    print(f\"\\nTop correlated non-missing columns with {target}:\\n\")\n    if target not in corr_matrix:\n        print(\"No correlation available (likely too many NaNs).\")\n        continue\n    correlations = corr_matrix[target].dropna()\n    correlations = correlations[correlations.index.isin(non_missing_cols) & (correlations.index != target)]\n    print(f\"Found {len(correlations)} valid correlations\")\n    print(correlations.sort_values(ascending=False).head(10))","metadata":{"execution":{"iopub.execute_input":"2025-04-17T15:28:51.882636Z","iopub.status.busy":"2025-04-17T15:28:51.881974Z","iopub.status.idle":"2025-04-17T15:28:51.938549Z","shell.execute_reply":"2025-04-17T15:28:51.937608Z"},"papermill":{"duration":0.078191,"end_time":"2025-04-17T15:28:51.940223","exception":false,"start_time":"2025-04-17T15:28:51.862032","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plt.figure(figsize=(8,6))\nsns.scatterplot(\n    data=train_df,\n    x='Basic_Demos-Age',\n    y='Physical-Weight'\n)\nplt.title('Age Vs Weight')\nplt.xlabel('Age')\nplt.ylabel('Weight')\nplt.grid(True)\nplt.show()","metadata":{"execution":{"iopub.execute_input":"2025-04-17T15:28:51.984445Z","iopub.status.busy":"2025-04-17T15:28:51.983801Z","iopub.status.idle":"2025-04-17T15:28:52.384660Z","shell.execute_reply":"2025-04-17T15:28:52.383720Z"},"papermill":{"duration":0.422165,"end_time":"2025-04-17T15:28:52.386159","exception":false,"start_time":"2025-04-17T15:28:51.963994","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plt.figure(figsize=(8,6))\nsns.scatterplot(\n    data=train_df,\n    x='Basic_Demos-Age',\n    y='Physical-Height'\n)\nplt.title('Age Vs Height')\nplt.xlabel('Age')\nplt.ylabel('Height')\nplt.grid(True)\nplt.show()","metadata":{"execution":{"iopub.execute_input":"2025-04-17T15:28:52.429958Z","iopub.status.busy":"2025-04-17T15:28:52.429634Z","iopub.status.idle":"2025-04-17T15:28:52.645139Z","shell.execute_reply":"2025-04-17T15:28:52.644218Z"},"papermill":{"duration":0.238904,"end_time":"2025-04-17T15:28:52.646754","exception":false,"start_time":"2025-04-17T15:28:52.407850","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_df['Physical-Height'].min()","metadata":{"execution":{"iopub.execute_input":"2025-04-17T15:28:52.695078Z","iopub.status.busy":"2025-04-17T15:28:52.694448Z","iopub.status.idle":"2025-04-17T15:28:52.700194Z","shell.execute_reply":"2025-04-17T15:28:52.699350Z"},"papermill":{"duration":0.030557,"end_time":"2025-04-17T15:28:52.701580","exception":false,"start_time":"2025-04-17T15:28:52.671023","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"code","source":"columns = ['Physical-Height', 'Physical-Weight', 'Basic_Demos-Age', 'Basic_Demos-Sex']\n\n# Selecting only these columns for imputation\ntrain_subset = train_df[columns]\ntest_subset = test_df[columns]\n\n# 2. Creating and fittinf the imputer ONLY on training data\nimputer = KNNImputer(n_neighbors=3)\nimputer.fit(train_subset)   # <-- fit only on train!\n\n# 3. Transforming both train and test\ntrain_imputed = pd.DataFrame(imputer.transform(train_subset), columns=columns)\ntest_imputed = pd.DataFrame(imputer.transform(test_subset), columns=columns)\n\n# 4. Replacing the imputed columns in original dataframes\ncolumn_replace = ['Physical-Height', 'Physical-Weight']\nfor col in column_replace:\n    train_df[col] = train_imputed[col]\n    test_df[col] = test_imputed[col]","metadata":{"execution":{"iopub.execute_input":"2025-04-17T15:28:52.748377Z","iopub.status.busy":"2025-04-17T15:28:52.747612Z","iopub.status.idle":"2025-04-17T15:28:53.089826Z","shell.execute_reply":"2025-04-17T15:28:53.088799Z"},"papermill":{"duration":0.36723,"end_time":"2025-04-17T15:28:53.091603","exception":false,"start_time":"2025-04-17T15:28:52.724373","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plt.figure(figsize=(8,6))\nsns.scatterplot(\n    data=train_df,\n    x='Basic_Demos-Age',\n    y='Physical-Weight'\n)\nplt.title('Age Vs Weight')\nplt.xlabel('Age')\nplt.ylabel('Weight')\nplt.grid(True)\nplt.show()","metadata":{"execution":{"iopub.execute_input":"2025-04-17T15:28:53.139815Z","iopub.status.busy":"2025-04-17T15:28:53.138980Z","iopub.status.idle":"2025-04-17T15:28:53.368047Z","shell.execute_reply":"2025-04-17T15:28:53.367265Z"},"papermill":{"duration":0.254045,"end_time":"2025-04-17T15:28:53.370282","exception":false,"start_time":"2025-04-17T15:28:53.116237","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plt.figure(figsize=(8,6))\nsns.scatterplot(\n    data=train_df,\n    x='Basic_Demos-Age',\n    y='Physical-Height'\n)\nplt.title('Age Vs Height')\nplt.xlabel('Age')\nplt.ylabel('Height')\nplt.grid(True)\nplt.show()","metadata":{"execution":{"iopub.execute_input":"2025-04-17T15:28:53.419859Z","iopub.status.busy":"2025-04-17T15:28:53.419165Z","iopub.status.idle":"2025-04-17T15:28:53.637740Z","shell.execute_reply":"2025-04-17T15:28:53.636889Z"},"papermill":{"duration":0.245952,"end_time":"2025-04-17T15:28:53.640120","exception":false,"start_time":"2025-04-17T15:28:53.394168","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Imputing BMI using Height and Weight","metadata":{"papermill":{"duration":0.024887,"end_time":"2025-04-17T15:28:53.692726","exception":false,"start_time":"2025-04-17T15:28:53.667839","status":"completed"},"tags":[]}},{"cell_type":"code","source":"train_df['Physical-Weight'] = train_df['Physical-Weight'] * 0.453592\ntrain_df['Physical-Height'] = train_df['Physical-Height'] * 0.0254\n\ntest_df['Physical-Weight'] = test_df['Physical-Weight'] * 0.453592\ntest_df['Physical-Height'] = test_df['Physical-Height'] * 0.0254","metadata":{"execution":{"iopub.execute_input":"2025-04-17T15:28:53.744659Z","iopub.status.busy":"2025-04-17T15:28:53.744242Z","iopub.status.idle":"2025-04-17T15:28:53.750341Z","shell.execute_reply":"2025-04-17T15:28:53.749523Z"},"papermill":{"duration":0.033673,"end_time":"2025-04-17T15:28:53.751801","exception":false,"start_time":"2025-04-17T15:28:53.718128","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def recompute_bmi(df):\n    mask = df['Physical-Weight'].notna() & df['Physical-Height'].notna()\n    df['Physical-BMI'] = np.nan\n    df.loc[mask, 'Physical-BMI'] = df.loc[mask, 'Physical-Weight'] / (df.loc[mask, 'Physical-Height'] ** 2)\n    return df\n\n# Apply to both\ntrain_df = recompute_bmi(train_df)\ntest_df = recompute_bmi(test_df)","metadata":{"execution":{"iopub.execute_input":"2025-04-17T15:28:53.804765Z","iopub.status.busy":"2025-04-17T15:28:53.804072Z","iopub.status.idle":"2025-04-17T15:28:53.813913Z","shell.execute_reply":"2025-04-17T15:28:53.813198Z"},"papermill":{"duration":0.038075,"end_time":"2025-04-17T15:28:53.815379","exception":false,"start_time":"2025-04-17T15:28:53.777304","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plt.figure(figsize=(8,6))\nsns.scatterplot(\n    data=train_df,\n    x='Physical-Weight',\n    y='Physical-BMI'\n)\nplt.title('Weight Vs BMI')\nplt.xlabel('Weight')\nplt.ylabel('BMI')\nplt.grid(True)\nplt.show()","metadata":{"execution":{"iopub.execute_input":"2025-04-17T15:28:53.873824Z","iopub.status.busy":"2025-04-17T15:28:53.873029Z","iopub.status.idle":"2025-04-17T15:28:54.096684Z","shell.execute_reply":"2025-04-17T15:28:54.095692Z"},"papermill":{"duration":0.252547,"end_time":"2025-04-17T15:28:54.098910","exception":false,"start_time":"2025-04-17T15:28:53.846363","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_df = train_df.drop(columns=['BIA-BIA_BMI'])","metadata":{"execution":{"iopub.execute_input":"2025-04-17T15:28:54.154041Z","iopub.status.busy":"2025-04-17T15:28:54.153726Z","iopub.status.idle":"2025-04-17T15:28:54.159889Z","shell.execute_reply":"2025-04-17T15:28:54.159202Z"},"papermill":{"duration":0.034938,"end_time":"2025-04-17T15:28:54.161154","exception":false,"start_time":"2025-04-17T15:28:54.126216","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### Impute waist circumference using linear regression","metadata":{"papermill":{"duration":0.026249,"end_time":"2025-04-17T15:28:54.214175","exception":false,"start_time":"2025-04-17T15:28:54.187926","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# 1. Getting all columns with missing values\nmissing_cols = train_df.columns[train_df.isna().any()]\n\n# 2. Gettinf the correlation matrix (numeric columns only)\ncorr_matrix = train_df.corr(numeric_only=True)\n\n# 3. Defining our target columns\ntarget_cols = ['Physical-Waist_Circumference']\n\n# 4. Finding non-missing columns\nnon_missing_cols = [col for col in train_df.columns if col not in missing_cols]\n\n# 5. For each target, gettinf sorted correlations with non-missing columns\nfor target in target_cols:\n    print(f\"\\nTop correlated non-missing columns with {target}:\\n\")\n    if target not in corr_matrix:\n        print(\"No correlation available (likely too many NaNs).\")\n        continue\n    correlations = corr_matrix[target].dropna()\n    correlations = correlations[correlations.index.isin(non_missing_cols) & (correlations.index != target)]\n    print(f\"Found {len(correlations)} valid correlations\")\n    print(correlations.sort_values(ascending=False).head(10))","metadata":{"execution":{"iopub.execute_input":"2025-04-17T15:28:54.268626Z","iopub.status.busy":"2025-04-17T15:28:54.268288Z","iopub.status.idle":"2025-04-17T15:28:54.326302Z","shell.execute_reply":"2025-04-17T15:28:54.325227Z"},"papermill":{"duration":0.087118,"end_time":"2025-04-17T15:28:54.327817","exception":false,"start_time":"2025-04-17T15:28:54.240699","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"code","source":"columns = ['Physical-Weight', 'Physical-BMI', 'Physical-Waist_Circumference']\ncolumns_to_impute = ['Physical-Waist_Circumference']\n\ndef iterative_impute(train_df, test_df, columns):\n    subset_train = train_df[columns]\n    subset_test = test_df[columns]\n    imputer = IterativeImputer(estimator=BayesianRidge())\n    imputer.fit(subset_train)\n    train_imputed = pd.DataFrame(imputer.transform(subset_train), columns=columns)\n    test_imputed = pd.DataFrame(imputer.transform(subset_test), columns=columns)\n    for col in columns_to_impute:\n        train_df[col] = train_imputed[col]\n        test_df[col] = test_imputed[col]\n    return train_df, test_df\n\n# Usage:\ntrain_df, test_df = iterative_impute(train_df, test_df, columns)","metadata":{"execution":{"iopub.execute_input":"2025-04-17T15:28:54.391990Z","iopub.status.busy":"2025-04-17T15:28:54.391273Z","iopub.status.idle":"2025-04-17T15:28:54.468123Z","shell.execute_reply":"2025-04-17T15:28:54.467304Z"},"papermill":{"duration":0.109641,"end_time":"2025-04-17T15:28:54.469693","exception":false,"start_time":"2025-04-17T15:28:54.360052","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plt.figure(figsize=(8,6))\nsns.scatterplot(\n    data=train_df,\n    x='Physical-Weight',\n    y='Physical-Waist_Circumference'\n)\nplt.title('Weight vs Waist Circumference')\nplt.xlabel('Weight')\nplt.ylabel('Waist Circumference')\nplt.grid(True)\nplt.show()","metadata":{"execution":{"iopub.execute_input":"2025-04-17T15:28:54.526008Z","iopub.status.busy":"2025-04-17T15:28:54.525713Z","iopub.status.idle":"2025-04-17T15:28:54.760630Z","shell.execute_reply":"2025-04-17T15:28:54.759718Z"},"papermill":{"duration":0.265673,"end_time":"2025-04-17T15:28:54.762982","exception":false,"start_time":"2025-04-17T15:28:54.497309","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_df['Physical-Waist_Circumference'].isna().sum()","metadata":{"execution":{"iopub.execute_input":"2025-04-17T15:28:54.823084Z","iopub.status.busy":"2025-04-17T15:28:54.822762Z","iopub.status.idle":"2025-04-17T15:28:54.828969Z","shell.execute_reply":"2025-04-17T15:28:54.828114Z"},"papermill":{"duration":0.0378,"end_time":"2025-04-17T15:28:54.830358","exception":false,"start_time":"2025-04-17T15:28:54.792558","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### Imputing SDS-Total-Raw using KNN Imputation","metadata":{"papermill":{"duration":0.029569,"end_time":"2025-04-17T15:28:54.888908","exception":false,"start_time":"2025-04-17T15:28:54.859339","status":"completed"},"tags":[]}},{"cell_type":"code","source":"equal_mask = train_df['SDS-SDS_Total_T'] == train_df['SDS-SDS_Total_Raw']\nprint(f\"{equal_mask.sum()} records have equal values.\")","metadata":{"execution":{"iopub.execute_input":"2025-04-17T15:28:54.948306Z","iopub.status.busy":"2025-04-17T15:28:54.947981Z","iopub.status.idle":"2025-04-17T15:28:54.953303Z","shell.execute_reply":"2025-04-17T15:28:54.952482Z"},"papermill":{"duration":0.036886,"end_time":"2025-04-17T15:28:54.954749","exception":false,"start_time":"2025-04-17T15:28:54.917863","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_df['SDS_Diff'] = train_df['SDS-SDS_Total_T'] - train_df['SDS-SDS_Total_Raw']\ntrain_df['SDS_Diff']","metadata":{"execution":{"iopub.execute_input":"2025-04-17T15:28:55.014893Z","iopub.status.busy":"2025-04-17T15:28:55.014523Z","iopub.status.idle":"2025-04-17T15:28:55.114568Z","shell.execute_reply":"2025-04-17T15:28:55.113433Z"},"papermill":{"duration":0.133353,"end_time":"2025-04-17T15:28:55.117566","exception":false,"start_time":"2025-04-17T15:28:54.984213","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_df.describe()","metadata":{"execution":{"iopub.execute_input":"2025-04-17T15:28:55.190449Z","iopub.status.busy":"2025-04-17T15:28:55.189823Z","iopub.status.idle":"2025-04-17T15:28:55.311982Z","shell.execute_reply":"2025-04-17T15:28:55.311054Z"},"papermill":{"duration":0.159745,"end_time":"2025-04-17T15:28:55.313549","exception":false,"start_time":"2025-04-17T15:28:55.153804","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_df = train_df.drop(columns=['SDS_Diff'])","metadata":{"execution":{"iopub.execute_input":"2025-04-17T15:28:55.412880Z","iopub.status.busy":"2025-04-17T15:28:55.412410Z","iopub.status.idle":"2025-04-17T15:28:55.421304Z","shell.execute_reply":"2025-04-17T15:28:55.420134Z"},"papermill":{"duration":0.071369,"end_time":"2025-04-17T15:28:55.423414","exception":false,"start_time":"2025-04-17T15:28:55.352045","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"code","source":"sns.scatterplot(\n    x='SDS-SDS_Total_Raw',\n    y='SDS-SDS_Total_T',\n    data=train_df\n)\nplt.plot([train_df['SDS-SDS_Total_Raw'].min(), train_df['SDS-SDS_Total_Raw'].max()],\n         [train_df['SDS-SDS_Total_Raw'].min(), train_df['SDS-SDS_Total_Raw'].max()],\n         color='red', linestyle='--', label='y = x')\nplt.legend()\nplt.title('SDS Total Raw vs T')\nplt.xlabel('SDS Total Raw')\nplt.ylabel('SDS Total T')\nplt.grid(True)\nplt.show()","metadata":{"execution":{"iopub.execute_input":"2025-04-17T15:28:55.514563Z","iopub.status.busy":"2025-04-17T15:28:55.513846Z","iopub.status.idle":"2025-04-17T15:28:55.770261Z","shell.execute_reply":"2025-04-17T15:28:55.769284Z"},"papermill":{"duration":0.296188,"end_time":"2025-04-17T15:28:55.771888","exception":false,"start_time":"2025-04-17T15:28:55.475700","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"code","source":"sns.scatterplot(\n    x='Basic_Demos-Age',\n    y='SDS-SDS_Total_T',\n    data=train_df\n)\n\nplt.legend()\nplt.title('SDS Total T vs Age')\nplt.xlabel('SDS Total Age')\nplt.ylabel('SDS Total T')\nplt.grid(True)\nplt.show()","metadata":{"execution":{"iopub.execute_input":"2025-04-17T15:28:55.845900Z","iopub.status.busy":"2025-04-17T15:28:55.845059Z","iopub.status.idle":"2025-04-17T15:28:56.088357Z","shell.execute_reply":"2025-04-17T15:28:56.087408Z"},"papermill":{"duration":0.280237,"end_time":"2025-04-17T15:28:56.089992","exception":false,"start_time":"2025-04-17T15:28:55.809755","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"code","source":"sns.scatterplot(\n    x='Basic_Demos-Sex',\n    y='SDS-SDS_Total_Raw',\n    data=train_df\n)\n\nplt.legend()\nplt.title('SDS Total Raw vs Age')\nplt.xlabel('SDS Total Age')\nplt.ylabel('SDS Total Raw')\nplt.grid(True)\nplt.show()","metadata":{"execution":{"iopub.execute_input":"2025-04-17T15:28:56.162122Z","iopub.status.busy":"2025-04-17T15:28:56.161154Z","iopub.status.idle":"2025-04-17T15:28:56.394107Z","shell.execute_reply":"2025-04-17T15:28:56.393280Z"},"papermill":{"duration":0.270588,"end_time":"2025-04-17T15:28:56.395795","exception":false,"start_time":"2025-04-17T15:28:56.125207","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_df = train_df.drop(columns=['SDS-SDS_Total_T'])","metadata":{"execution":{"iopub.execute_input":"2025-04-17T15:28:56.477419Z","iopub.status.busy":"2025-04-17T15:28:56.477095Z","iopub.status.idle":"2025-04-17T15:28:56.483423Z","shell.execute_reply":"2025-04-17T15:28:56.482782Z"},"papermill":{"duration":0.049383,"end_time":"2025-04-17T15:28:56.484688","exception":false,"start_time":"2025-04-17T15:28:56.435305","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 1. Getting all columns with missing values\nmissing_cols = train_df.columns[train_df.isna().any()]\n\n# 2. Getting the correlation matrix (numeric columns only)\ncorr_matrix = train_df.corr(numeric_only=True)\n\n# 3. Defining your target columns\ntarget_cols = ['SDS-SDS_Total_Raw']\n\n# 4. Finding non-missing columns\nnon_missing_cols = [col for col in train_df.columns if col not in missing_cols]\n\n# 5. For each target, getting sorted correlations with non-missing columns\nfor target in target_cols:\n    print(f\"\\nTop correlated non-missing columns with {target}:\\n\")\n    if target not in corr_matrix:\n        print(\"No correlation available (likely too many NaNs).\")\n        continue\n    correlations = corr_matrix[target].dropna()\n    correlations = correlations[correlations.index.isin(non_missing_cols) & (correlations.index != target)]\n    print(f\"Found {len(correlations)} valid correlations\")\n    print(correlations.sort_values(ascending=False).head(10))","metadata":{"execution":{"iopub.execute_input":"2025-04-17T15:28:56.557536Z","iopub.status.busy":"2025-04-17T15:28:56.557192Z","iopub.status.idle":"2025-04-17T15:28:56.613066Z","shell.execute_reply":"2025-04-17T15:28:56.611951Z"},"papermill":{"duration":0.093664,"end_time":"2025-04-17T15:28:56.614530","exception":false,"start_time":"2025-04-17T15:28:56.520866","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Columns we want to impute\ncolumns = ['SDS-SDS_Total_Raw', 'Basic_Demos-Age', 'Basic_Demos-Sex']\ncolumns_to_impute = ['SDS-SDS_Total_Raw']\n# 1. Selecting the subset from train and test\ntrain_subset = train_df[columns]\ntest_subset = test_df[columns]\n\n# 2. Creating and fitting the imputer ONLY on training data\nimputer = KNNImputer(n_neighbors=3)\nimputer.fit(train_subset)   # <-- fitting only on train!\n\n# 3. Transforming both train and test\ntrain_imputed = pd.DataFrame(imputer.transform(train_subset), columns=columns)\ntest_imputed = pd.DataFrame(imputer.transform(test_subset), columns=columns)\n\n# 4. Replacing the imputed columns in original dataframes\nfor col in columns_to_impute:\n    train_df[col] = train_imputed[col]\n    test_df[col] = test_imputed[col]","metadata":{"execution":{"iopub.execute_input":"2025-04-17T15:28:56.688535Z","iopub.status.busy":"2025-04-17T15:28:56.688170Z","iopub.status.idle":"2025-04-17T15:28:56.976189Z","shell.execute_reply":"2025-04-17T15:28:56.975396Z"},"papermill":{"duration":0.3274,"end_time":"2025-04-17T15:28:56.978103","exception":false,"start_time":"2025-04-17T15:28:56.650703","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"code","source":"sns.scatterplot(\n    x='Basic_Demos-Age',\n    y='SDS-SDS_Total_Raw',\n    data=train_df\n)\n\nplt.legend()\nplt.title('SDS Total Raw vs Age')\nplt.xlabel('SDS Total Age')\nplt.ylabel('SDS Total Raw')\nplt.grid(True)\nplt.show()","metadata":{"execution":{"iopub.execute_input":"2025-04-17T15:28:57.070009Z","iopub.status.busy":"2025-04-17T15:28:57.069670Z","iopub.status.idle":"2025-04-17T15:28:57.326196Z","shell.execute_reply":"2025-04-17T15:28:57.325242Z"},"papermill":{"duration":0.302407,"end_time":"2025-04-17T15:28:57.327734","exception":false,"start_time":"2025-04-17T15:28:57.025327","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### Imputing Internet Hours","metadata":{"papermill":{"duration":0.036147,"end_time":"2025-04-17T15:28:57.402347","exception":false,"start_time":"2025-04-17T15:28:57.366200","status":"completed"},"tags":[]}},{"cell_type":"code","source":"#quick function for summary data for post and pre imputation\ndef summarize_data(df, selected_columns=None):\n    # Convert single column name to list\n    if isinstance(selected_columns, str):\n        selected_columns = [selected_columns]\n    \n    # Use all columns if none specified\n    if selected_columns is None:\n        selected_columns = df.columns.tolist()\n    \n    result_tables = []\n    \n    for column in selected_columns:\n        # Get the data for the current column\n        col_data = df[column]\n        \n        # Handle categorical or text data\n        if col_data.dtype in ['object', 'category']:\n            # Calculate frequencies and percentages\n            freq_table = pd.DataFrame()\n            values = col_data.value_counts(dropna=False)\n            percentages = col_data.value_counts(dropna=False, normalize=True) * 100\n            \n            # Combine counts and percentages\n            freq_table['count (%)'] = [\n                f\"{count} ({percentage:.2f}%)\" \n                for count, percentage in zip(values, percentages)\n            ]\n            \n            result_tables.append(freq_table)\n        \n        # Handle numerical data\n        else:\n            # Get descriptive statistics\n            num_stats = pd.DataFrame()\n            \n            # Add basic statistics as a row\n            descriptives = col_data.describe().to_dict()\n            num_stats = pd.DataFrame(descriptives, index=[0])\n            \n            # Add missing values count\n            num_stats['missing'] = col_data.isna().sum()\n            \n            # Set index name\n            num_stats.index = [column]\n            \n            result_tables.append(num_stats)\n    \n    # Combine all results\n    combined_stats = pd.concat(result_tables)\n    \n    return combined_stats","metadata":{"execution":{"iopub.execute_input":"2025-04-17T15:28:57.478124Z","iopub.status.busy":"2025-04-17T15:28:57.477807Z","iopub.status.idle":"2025-04-17T15:28:57.485159Z","shell.execute_reply":"2025-04-17T15:28:57.484314Z"},"papermill":{"duration":0.047562,"end_time":"2025-04-17T15:28:57.486592","exception":false,"start_time":"2025-04-17T15:28:57.439030","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"code","source":"summarize_data(train_df, ['PreInt_EduHx-computerinternet_hoursday'])","metadata":{"execution":{"iopub.execute_input":"2025-04-17T15:28:57.561771Z","iopub.status.busy":"2025-04-17T15:28:57.560981Z","iopub.status.idle":"2025-04-17T15:28:57.577090Z","shell.execute_reply":"2025-04-17T15:28:57.576280Z"},"papermill":{"duration":0.055056,"end_time":"2025-04-17T15:28:57.578406","exception":false,"start_time":"2025-04-17T15:28:57.523350","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"code","source":"internet_data_age = train_df[train_df['PreInt_EduHx-computerinternet_hoursday'].notna()]\nage_range = internet_data_age['Basic_Demos-Age']\nprint(\n    f\"Range of Age for those who have internet usage data calculated/already exisiting within our initial data:\"\n    f\" {age_range.min()} - {age_range.max()} years\"\n)","metadata":{"execution":{"iopub.execute_input":"2025-04-17T15:28:57.653873Z","iopub.status.busy":"2025-04-17T15:28:57.653543Z","iopub.status.idle":"2025-04-17T15:28:57.661681Z","shell.execute_reply":"2025-04-17T15:28:57.660876Z"},"papermill":{"duration":0.047338,"end_time":"2025-04-17T15:28:57.663060","exception":false,"start_time":"2025-04-17T15:28:57.615722","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"code","source":"fig, axes = plt.subplots(1, 4, figsize=(19, 5))\nax = axes.ravel()\n\n# Internet usage hours distribution\nsns.histplot(data=train_df, x='PreInt_EduHx-computerinternet_hoursday', ax=ax[0], bins=10, kde=True)\nax[0].set_title('Distribution of Internet Use (In Hours)')\nax[0].set_xlabel('Internet Usage (In Hours)')\nax[0].set_ylabel('Frequency')\ntrain_beforeimpute = train_df.copy()\ntrain_internet = train_df.copy()\n# Add count and percentage labels to the histogram\ntotal = len(train_internet['PreInt_EduHx-computerinternet_hoursday'].dropna())\nfor i, p in enumerate(ax[0].patches):\n    count = int(p.get_height())\n    percentage = '{:.1f}%'.format(100 * count / total)\n    ax[0].annotate(f'{count} ({percentage})', (p.get_x() + p.get_width() / 2., p.get_height()), \n                 ha='center', va='baseline', fontsize=10, color='black', xytext=(0, 5), \n                 textcoords='offset points')\n\n# Internet usage by Age (boxplot)\nsns.boxplot(data=train_df, x='PreInt_EduHx-computerinternet_hoursday', y='Basic_Demos-Age', ax=ax[1])\nax[1].set_title('Internet Usage by Age')\nax[1].set_ylabel('Age')\nax[1].set_xlabel('Internet Usage (In Hours)')\n\n# Internet usage by Age (violin plot)\nsns.violinplot(data=train_df, x='PreInt_EduHx-computerinternet_hoursday', y='Basic_Demos-Age', ax=ax[2])\nax[2].set_title('Internet Usage by Age')\nax[2].set_ylabel('Age')\nax[2].set_xlabel('Internet Usage (In Hours)')\n\n# Create age group labels based on the thresholds from your assign_groups function\nthresholds = [5, 10, 15, 18, 22]\nage_group_labels = [f\"â‰¤{thresholds[0]}\", \n                    f\"{thresholds[0]+1}-{thresholds[1]}\", \n                    f\"{thresholds[1]+1}-{thresholds[2]}\", \n                    f\"{thresholds[2]+1}-{thresholds[3]}\", \n                    f\"{thresholds[3]+1}-{thresholds[4]}\", \n                    f\">{thresholds[4]}\"]\n\n# Map numeric age_group to labels\ntrain_df['age_group_label'] = train_df['age_group'].map(\n    {i: label for i, label in enumerate(age_group_labels)}\n)\n\n# Internet usage by Age Group (boxplot)\nsns.boxplot(data=train_df, x='age_group_label', y='PreInt_EduHx-computerinternet_hoursday', ax=ax[3], \n            order=age_group_labels)\nax[3].set_title('Internet Usage by Different Age Group')\nax[3].set_ylabel('Internet Usage Per Day (In Hours)')\nax[3].set_xlabel('Age Group')\nax[3].set_xticklabels(ax[3].get_xticklabels(), rotation=45)\n\nplt.tight_layout()\nplt.show()","metadata":{"execution":{"iopub.execute_input":"2025-04-17T15:28:57.739912Z","iopub.status.busy":"2025-04-17T15:28:57.739281Z","iopub.status.idle":"2025-04-17T15:28:58.839702Z","shell.execute_reply":"2025-04-17T15:28:58.838824Z"},"papermill":{"duration":1.141913,"end_time":"2025-04-17T15:28:58.842656","exception":false,"start_time":"2025-04-17T15:28:57.700743","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Recreate proportion stats for plotting\nstats_raw = train_df.groupby(['Basic_Demos-Sex', 'PreInt_EduHx-computerinternet_hoursday']).size().unstack(fill_value=0)\nstats_prop = stats_raw.div(stats_raw.sum(axis=1), axis=0) * 100\n\n# Plot: Stacked Bar Chart\nplt.figure(figsize=(10, 6))\nstats_prop.plot(kind='bar', stacked=True, colormap='viridis', edgecolor='black')\n\nplt.title(\"Internet Usage per Day by Gender (Proportional)\")\nplt.xlabel(\"Gender\")\nplt.ylabel(\"Percentage (%)\")\nplt.legend(title=\"Hours per Day\", bbox_to_anchor=(1.05, 1), loc='upper left')\nplt.tight_layout()\nplt.show()\n","metadata":{"execution":{"iopub.execute_input":"2025-04-17T15:28:58.925519Z","iopub.status.busy":"2025-04-17T15:28:58.925167Z","iopub.status.idle":"2025-04-17T15:28:59.229378Z","shell.execute_reply":"2025-04-17T15:28:59.228554Z"},"papermill":{"duration":0.348289,"end_time":"2025-04-17T15:28:59.231033","exception":false,"start_time":"2025-04-17T15:28:58.882744","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# SDS-SDS_Total_Raw'\n# Define predictor columns (excluding 'Season') and the target variable\ninternet_hours_impute_features = ['Basic_Demos-Age', 'Basic_Demos-Sex', 'SDS-SDS_Total_Raw']\ntarget_column = 'PreInt_EduHx-computerinternet_hoursday'\n\n# Check for missing values in the features\nmissing_summary = train_df[internet_hours_impute_features].isna().sum()\n\n# Filter and print only the columns with missing values\nmissing_columns = missing_summary[missing_summary > 0]\nprint(\"Columns with NaN values in training features:\")\nprint(missing_columns)\n\nmissing_count = train_df[target_column].isna().sum()\nprint(f\"Missing values in target column '{target_column}': {missing_count}\")\n\n# Select rows with known target values for training\ntraining_subset = train_df[train_df[target_column].notna()]\nX_train_data = training_subset[internet_hours_impute_features]\ny_train_data = training_subset[target_column]\n\n# Define a transformer pipeline for preprocessing\ntransform_pipeline = ColumnTransformer(\n    transformers=[\n        ('numeric', 'passthrough', internet_hours_impute_features)\n    ]\n)\n\n# Combine preprocessing with a logistic regression model\nprediction_model = Pipeline(steps=[\n    ('transform', transform_pipeline),\n    ('logistic_model', LogisticRegression(max_iter=1000, multi_class='ovr'))\n])\n\n# Train the model using the available data\nprediction_model.fit(X_train_data, y_train_data)\n\n\ntrain_missing = train_df[train_df[target_column].isna()][internet_hours_impute_features]\nprint(train_missing.head())\npredicted_values = prediction_model.predict(train_missing)\n\ntrain_df.loc[train_df[target_column].isna(), target_column] = predicted_values\n\n\n# Predict and update missing values in the test dataset\ntest_missing = test_df[test_df[target_column].isna()]\ntest_df.loc[test_df[target_column].isna(), target_column] = prediction_model.predict(test_missing[internet_hours_impute_features])","metadata":{"execution":{"iopub.execute_input":"2025-04-17T15:28:59.322586Z","iopub.status.busy":"2025-04-17T15:28:59.322226Z","iopub.status.idle":"2025-04-17T15:28:59.895663Z","shell.execute_reply":"2025-04-17T15:28:59.894800Z"},"papermill":{"duration":0.621114,"end_time":"2025-04-17T15:28:59.897511","exception":false,"start_time":"2025-04-17T15:28:59.276397","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Call function to evaluate the updated training data\nsummarize_data(train_df, [target_column])","metadata":{"execution":{"iopub.execute_input":"2025-04-17T15:29:00.024310Z","iopub.status.busy":"2025-04-17T15:29:00.023627Z","iopub.status.idle":"2025-04-17T15:29:00.038387Z","shell.execute_reply":"2025-04-17T15:29:00.037553Z"},"papermill":{"duration":0.067083,"end_time":"2025-04-17T15:29:00.039938","exception":false,"start_time":"2025-04-17T15:28:59.972855","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Set the plot style\nsns.set(style=\"whitegrid\")\n\n# Plot the distribution of the target column after imputation\nplt.figure(figsize=(10, 6))\nsns.histplot(train_df[target_column], bins=20, kde=True)\n\nplt.title(f\"Distribution of '{target_column}' After Imputation\")\nplt.xlabel(\"Hours of Computer/Internet Use per Day\")\nplt.ylabel(\"Count\")\nplt.tight_layout()\nplt.show()","metadata":{"execution":{"iopub.execute_input":"2025-04-17T15:29:00.133234Z","iopub.status.busy":"2025-04-17T15:29:00.132706Z","iopub.status.idle":"2025-04-17T15:29:00.611421Z","shell.execute_reply":"2025-04-17T15:29:00.610628Z"},"papermill":{"duration":0.52696,"end_time":"2025-04-17T15:29:00.613315","exception":false,"start_time":"2025-04-17T15:29:00.086355","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Define the features to normalize\nfeatures_to_normalize = ['Basic_Demos-Age', 'PreInt_EduHx-computerinternet_hoursday']\n\n# Initialize the scaler\nscaler = MinMaxScaler()\n\n# 1. Fit ONLY on train\nscaler.fit(train_df[features_to_normalize])\n\n# 2. Transform train\nnormalized_features_train = scaler.transform(train_df[features_to_normalize])\ntrain_df['norm_age'] = normalized_features_train[:, 0]\ntrain_df['norm_internet_usage'] = normalized_features_train[:, 1]\ntrain_df['norm_age_internet_product'] = train_df['norm_age'] * train_df['norm_internet_usage']\n\n# 3. Transform test (IMPORTANT: only transform, NOT fit again!)\nnormalized_features_test = scaler.transform(test_df[features_to_normalize])\ntest_df['norm_age'] = normalized_features_test[:, 0]\ntest_df['norm_internet_usage'] = normalized_features_test[:, 1]\ntest_df['norm_age_internet_product'] = test_df['norm_age'] * test_df['norm_internet_usage']\n\n# 4. Drop unwanted columns (if they exist)\nfor col in ['norm_age', 'norm_internet_usage', 'age_group_label']:\n    if col in train_df.columns:\n        train_df = train_df.drop(columns=[col])\n\n    if col in test_df.columns:\n        test_df = test_df.drop(columns=[col])\n\n# 5. (Optional) Check stats\nprint(\"Train stats after normalization:\")\nprint(train_df[['norm_age_internet_product']].describe())\n\nprint(\"\\nTest stats after normalization:\")\nprint(test_df[['norm_age_internet_product']].describe())\n","metadata":{"execution":{"iopub.execute_input":"2025-04-17T15:29:00.721265Z","iopub.status.busy":"2025-04-17T15:29:00.720597Z","iopub.status.idle":"2025-04-17T15:29:00.766806Z","shell.execute_reply":"2025-04-17T15:29:00.765542Z"},"papermill":{"duration":0.10594,"end_time":"2025-04-17T15:29:00.768783","exception":false,"start_time":"2025-04-17T15:29:00.662843","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# ACTIGRAPHY","metadata":{"papermill":{"duration":0.055154,"end_time":"2025-04-17T15:29:00.872364","exception":false,"start_time":"2025-04-17T15:29:00.817210","status":"completed"},"tags":[]}},{"cell_type":"code","source":"def time_features(df):\n    # Convert time_of_day to hours\n    df[\"hours\"] = df[\"time_of_day\"] // (3_600 * 1_000_000_000)\n    # Basic features \n    features = [\n        df[\"non-wear_flag\"].mean(),\n        df[\"enmo\"][df[\"enmo\"] >= 0.05].sum(), # Filters out low level noise\n    ]\n    \n    # Define conditions for night, day, and no mask (full data)\n    night = ((df[\"hours\"] >= 21) | (df[\"hours\"] <= 5))\n    day = ((df[\"hours\"] <= 20) & (df[\"hours\"] >= 6))\n    no_mask = np.ones(len(df), dtype=bool)\n    \n    # List of columns of interest and masks\n    keys = [\"enmo\", \"anglez\", \"light\", \"battery_voltage\"]\n    masks = [no_mask, night, day]\n    \n    # Helper function for feature extraction\n    def extract_stats(data):\n        return [\n            data.mean(), \n            data.std(), \n            data.max(), \n            data.min(), \n            data.diff().mean(), \n            data.diff().std()\n        ]\n    \n    # Iterate over keys and masks to generate the statistics\n    for key in keys:\n        for mask in masks:\n            filtered_data = df.loc[mask, key]\n            features.extend(extract_stats(filtered_data))\n\n    return features\n\ndef process_file(filename, dirname):\n    # Process file and extract time features\n    df = pd.read_parquet(os.path.join(dirname, filename, 'part-0.parquet'))\n    df.drop('step', axis=1, inplace=True)\n    return time_features(df), filename.split('=')[1]\n\ndef load_time_series(dirname) -> pd.DataFrame:\n    # Load time series from directory in parallel\n    ids = os.listdir(dirname)\n    \n    with ThreadPoolExecutor() as executor:\n        results = list(tqdm(executor.map(lambda fname: process_file(fname, dirname), ids), total=len(ids)))\n    \n    stats, indexes = zip(*results)\n    \n    df = pd.DataFrame(stats, columns=[f\"stat_{i}\" for i in range(len(stats[0]))])\n    df['id'] = indexes\n    \n    return df","metadata":{"execution":{"iopub.execute_input":"2025-04-17T15:29:00.961526Z","iopub.status.busy":"2025-04-17T15:29:00.961183Z","iopub.status.idle":"2025-04-17T15:29:01.150188Z","shell.execute_reply":"2025-04-17T15:29:01.149512Z"},"papermill":{"duration":0.234434,"end_time":"2025-04-17T15:29:01.151700","exception":false,"start_time":"2025-04-17T15:29:00.917266","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_ts = load_time_series(\"/kaggle/input/child-mind-institute-problematic-internet-use/series_train.parquet\")\n","metadata":{"execution":{"iopub.execute_input":"2025-04-17T15:29:01.251639Z","iopub.status.busy":"2025-04-17T15:29:01.250924Z","iopub.status.idle":"2025-04-17T15:30:08.512961Z","shell.execute_reply":"2025-04-17T15:30:08.511831Z"},"papermill":{"duration":67.308349,"end_time":"2025-04-17T15:30:08.514630","exception":false,"start_time":"2025-04-17T15:29:01.206281","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test_ts = load_time_series(\"/kaggle/input/child-mind-institute-problematic-internet-use/series_test.parquet\")","metadata":{"execution":{"iopub.execute_input":"2025-04-17T15:30:08.621966Z","iopub.status.busy":"2025-04-17T15:30:08.621660Z","iopub.status.idle":"2025-04-17T15:30:08.851936Z","shell.execute_reply":"2025-04-17T15:30:08.850857Z"},"papermill":{"duration":0.28582,"end_time":"2025-04-17T15:30:08.853523","exception":false,"start_time":"2025-04-17T15:30:08.567703","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Principal Component Analysis\ndef perform_pca(train, test, n_components=None, random_state=42):\n    \n    pca = PCA(n_components=n_components, random_state=random_state)\n    train_pca = pca.fit_transform(train)\n    test_pca = pca.transform(test)\n    \n    explained_variance_ratio = pca.explained_variance_ratio_\n    print(f\"Explained variance ratio of the components:\\n {explained_variance_ratio}\")\n    print(np.sum(explained_variance_ratio))\n    \n    train_pca_df = pd.DataFrame(train_pca, columns=[f'PC_{i+1}' for i in range(train_pca.shape[1])])\n    test_pca_df = pd.DataFrame(test_pca, columns=[f'PC_{i+1}' for i in range(test_pca.shape[1])])\n    \n    return train_pca_df, test_pca_df, pca","metadata":{"execution":{"iopub.execute_input":"2025-04-17T15:30:08.966520Z","iopub.status.busy":"2025-04-17T15:30:08.966178Z","iopub.status.idle":"2025-04-17T15:30:08.972338Z","shell.execute_reply":"2025-04-17T15:30:08.971436Z"},"papermill":{"duration":0.062825,"end_time":"2025-04-17T15:30:08.973815","exception":false,"start_time":"2025-04-17T15:30:08.910990","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Processing the time series data and merging with the main dataset\ndf_train = train_ts.drop('id', axis=1)\ndf_test = test_ts.drop('id', axis=1)\n\nscaler = StandardScaler() \ndf_train = pd.DataFrame(scaler.fit_transform(df_train), columns=df_train.columns)\ndf_test = pd.DataFrame(scaler.transform(df_test), columns=df_test.columns)\n\nfor c in df_train.columns:\n    m = np.mean(df_train[c])\n    df_train[c].fillna(m, inplace=True)\n    df_test[c].fillna(m, inplace=True)\n\nprint(df_train.shape)\n\ndf_train_pca, df_test_pca, pca = perform_pca(df_train, df_test, n_components=15, random_state=SEED)\n\ndf_train_pca['id'] = train_ts['id']\ndf_test_pca['id'] = test_ts['id']\n\ntrain = pd.merge(train_df, df_train_pca, how=\"left\", on='id')\ntest = pd.merge(test_df, df_test_pca, how=\"left\", on='id')\ntrain.shape","metadata":{"execution":{"iopub.execute_input":"2025-04-17T15:30:09.084103Z","iopub.status.busy":"2025-04-17T15:30:09.083783Z","iopub.status.idle":"2025-04-17T15:30:09.200862Z","shell.execute_reply":"2025-04-17T15:30:09.197500Z"},"papermill":{"duration":0.175506,"end_time":"2025-04-17T15:30:09.204567","exception":false,"start_time":"2025-04-17T15:30:09.029061","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train.to_csv(\"train_values_merge.csv\", index=False)\ntest.to_csv(\"test_values_merge.csv\", index=False)","metadata":{"execution":{"iopub.execute_input":"2025-04-17T15:30:09.359649Z","iopub.status.busy":"2025-04-17T15:30:09.359068Z","iopub.status.idle":"2025-04-17T15:30:09.573382Z","shell.execute_reply":"2025-04-17T15:30:09.572439Z"},"papermill":{"duration":0.277287,"end_time":"2025-04-17T15:30:09.575063","exception":false,"start_time":"2025-04-17T15:30:09.297776","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Features to exclude, because they're not in test\nexclude = ['PCIAT-PCIAT_01', 'PCIAT-PCIAT_02', 'PCIAT-PCIAT_03',\n           'PCIAT-PCIAT_04', 'PCIAT-PCIAT_05', 'PCIAT-PCIAT_06', 'PCIAT-PCIAT_07',\n           'PCIAT-PCIAT_08', 'PCIAT-PCIAT_09', 'PCIAT-PCIAT_10', 'PCIAT-PCIAT_11',\n           'PCIAT-PCIAT_12', 'PCIAT-PCIAT_13', 'PCIAT-PCIAT_14', 'PCIAT-PCIAT_15',\n           'PCIAT-PCIAT_16', 'PCIAT-PCIAT_17', 'PCIAT-PCIAT_18', 'PCIAT-PCIAT_19',\n           'PCIAT-PCIAT_20', 'PCIAT-PCIAT_Total', 'sii', 'id']\n\ny_model = \"PCIAT-PCIAT_Total\" # Score, target for the model\ny_comp = \"sii\" # Index, target of the competition\ncategorical_features = ['Basic_Demos-Sex','BIA-BIA_Activity_Level_num', 'BIA-BIA_Frame_num', 'PreInt_EduHx-computerinternet_hoursday', \n                        'PCIAT-PCIAT_01', 'PCIAT-PCIAT_02', 'PCIAT-PCIAT_03', 'PCIAT-PCIAT_04', \n                        'PCIAT-PCIAT_05', 'PCIAT-PCIAT_06', 'PCIAT-PCIAT_07', 'PCIAT-PCIAT_08', \n                        'PCIAT-PCIAT_09', 'PCIAT-PCIAT_10', 'PCIAT-PCIAT_11', 'PCIAT-PCIAT_12', \n                        'PCIAT-PCIAT_13', 'PCIAT-PCIAT_14', 'PCIAT-PCIAT_15', 'PCIAT-PCIAT_16', \n                        'PCIAT-PCIAT_17', 'PCIAT-PCIAT_18', 'PCIAT-PCIAT_19', 'PCIAT-PCIAT_20', \n                        'PCIAT-PCIAT_Total', 'sii', 'id']\nfeatures = [f for f in train.columns if f not in exclude]\nnumerical_features = [f for f in train.columns if f not in categorical_features]\ncategorical_features_pred = ['Basic_Demos-Sex', 'BIA-BIA_Activity_Level_num', 'BIA-BIA_Frame_num', 'PreInt_EduHx-computerinternet_hoursday']","metadata":{"execution":{"iopub.execute_input":"2025-04-17T15:30:09.686868Z","iopub.status.busy":"2025-04-17T15:30:09.686003Z","iopub.status.idle":"2025-04-17T15:30:09.692783Z","shell.execute_reply":"2025-04-17T15:30:09.691882Z"},"papermill":{"duration":0.064793,"end_time":"2025-04-17T15:30:09.694241","exception":false,"start_time":"2025-04-17T15:30:09.629448","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train[categorical_features_pred] = train[categorical_features_pred].astype('category')\ntest[categorical_features_pred] = test[categorical_features_pred].astype('category')","metadata":{"execution":{"iopub.execute_input":"2025-04-17T15:30:09.805075Z","iopub.status.busy":"2025-04-17T15:30:09.804245Z","iopub.status.idle":"2025-04-17T15:30:09.816357Z","shell.execute_reply":"2025-04-17T15:30:09.815692Z"},"papermill":{"duration":0.068856,"end_time":"2025-04-17T15:30:09.817954","exception":false,"start_time":"2025-04-17T15:30:09.749098","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Identify Categorical & Numerical Columns\ncategorical_cols = train.select_dtypes(include=['category']).columns\nnumerical_cols = train.select_dtypes(include=['int64', 'float64']).columns\n\nprint(f\"\\nðŸ“ Categorical Columns: {list(categorical_cols)}\")\nprint(f\"\\nðŸ”¢ Numerical Columns: {list(numerical_cols)}\")","metadata":{"execution":{"iopub.execute_input":"2025-04-17T15:30:09.927992Z","iopub.status.busy":"2025-04-17T15:30:09.927171Z","iopub.status.idle":"2025-04-17T15:30:09.935393Z","shell.execute_reply":"2025-04-17T15:30:09.934438Z"},"papermill":{"duration":0.064808,"end_time":"2025-04-17T15:30:09.936998","exception":false,"start_time":"2025-04-17T15:30:09.872190","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"code","source":"## WE ARE USING LASSO ONLY FOR NUMERICAL FEATURES\nclass Impute_With_Model:\n    \n    def __init__(self, na_frac=0.5, min_samples=0):\n        self.model_dict = {}\n        self.mean_dict = {}\n        self.features = None\n        self.na_frac = na_frac\n        self.min_samples = min_samples\n        self.group_means_dict = {} #Stores mean for numerical features and mode for categorical features\n        self.group_col = \"age_group\" # Group column for mean imputation\n\n    '''def find_features(self, data, feature, tmp_features):\n        missing_rows = data[feature].isna()\n        na_fraction = data[missing_rows][tmp_features].isna().mean(axis=0)\n        valid_features = np.array(tmp_features)[na_fraction <= self.na_frac]\n        return valid_features'''\n    \n    def find_features(self, data, feature, tmp_features, min_corr=0.4):\n        # Step 1: Identify rows where the target feature is missing\n        missing_rows = data[feature].isna()\n\n        # Step 2: Exclude tmp_features that are too missing in those rows\n        na_fraction = data[missing_rows][tmp_features].isna().mean(axis=0)\n        valid_missingness = np.array(tmp_features)[na_fraction <= self.na_frac]\n\n        # Step 3: Split valid features into numeric and categorical\n        numeric_features = [f for f in valid_missingness if pd.api.types.is_numeric_dtype(data[f])]\n        categorical_features = [f for f in valid_missingness if not pd.api.types.is_numeric_dtype(data[f])]\n\n        # Step 4: Compute correlation for numeric features only\n        if pd.api.types.is_numeric_dtype(data[feature]):\n            non_missing_data = data[feature].notna()\n            corrs = data.loc[non_missing_data, numeric_features].corrwith(\n                data.loc[non_missing_data, feature], numeric_only=True\n            )\n            valid_numerics = corrs[abs(corrs) >= min_corr].index.tolist()\n        else:\n            valid_numerics =numeric_features\n        # Step 5: Keep numerics with high correlation + all categoricals\n        \n        valid_features = valid_numerics + categorical_features\n\n        return valid_features\n\n\n\n    def fit_models(self, model, data, features, numerical_features):\n        self.features = features\n        n_data = data.shape[0]\n        self.group_means_dict = {}\n        for feature in features:\n            if pd.api.types.is_float_dtype(data[feature]) or pd.api.types.is_integer_dtype(data[feature]):\n                self.group_means_dict[feature] = data.groupby(self.group_col)[feature].mean()\n            elif pd.api.types.is_categorical_dtype(data[feature]) or pd.api.types.is_object_dtype(data[feature]):\n                self.group_means_dict[feature] = data.groupby(self.group_col)[feature].agg(lambda x: x.mode().iloc[0] if not x.mode().empty else np.nan)\n\n        for feature in features:\n            if data[feature].dtype in ['float64', 'int64']:\n                self.mean_dict[feature] = np.mean(data[feature])\n            elif data[feature].dtype in ['object', 'category']:\n                self.mean_dict[feature] = data[feature].mode().iloc[0]\n            \n        for feature in tqdm(numerical_features):\n            if data[feature].isna().sum() > 0:\n                model_clone = clone(model)\n                X = data[data[feature].notna()].copy()\n                tmp_features = [f for f in features if f != feature]\n                tmp_features = self.find_features(data, feature, tmp_features)\n                if len(tmp_features) >= 1 and X.shape[0] > self.min_samples:\n                    for f in tmp_features:\n                        group_means = self.group_means_dict[f]\n                        X[f] = X.apply(\n                            lambda row: group_means[row[self.group_col]] if pd.isna(row[f]) else row[f],\n                            axis=1\n                        )\n                        #X[f] = X[f].fillna(self.mean_dict[f])\n                    model_clone.fit(X[tmp_features], X[feature])\n                    self.model_dict[feature] = (model_clone, tmp_features.copy())\n                else:\n                    self.model_dict[feature] = (\"mean\", self.mean_dict[feature])\n            \n    def impute(self, data):\n        imputed_data = data.copy()\n        for feature, model in self.model_dict.items():\n            missing_rows = imputed_data[feature].isna()\n            group_means = self.group_means_dict[feature]\n            if missing_rows.any():\n                if model[0] == \"mean\":\n                    imputed_data.loc[missing_rows, feature] = imputed_data.loc[missing_rows].apply(\n                        lambda row: group_means.get(row[self.group_col], self.mean_dict[feature]), axis=1\n                    )\n                else:\n                    tmp_features = [f for f in self.features if f != feature]\n                    X_missing = data.loc[missing_rows, tmp_features].copy()\n                    for f in tmp_features:\n                        X_missing[f] = X_missing[f].fillna(self.mean_dict[f])\n                    imputed_data.loc[missing_rows, feature] = model[0].predict(X_missing[model[1]])\n        return imputed_data\n    \n    def impute_test(self, data):\n        imputed_data = data.copy()\n        for feature, model in self.model_dict.items():\n            if feature in exclude:\n                continue\n            group_means = self.group_means_dict[feature]\n            missing_rows = imputed_data[feature].isna()\n            if missing_rows.any():\n                if model[0] == \"mean\":\n                    imputed_data.loc[missing_rows, feature] = imputed_data.loc[missing_rows].apply(\n                        lambda row: group_means.get(row[self.group_col], self.mean_dict[feature]), axis=1\n                    )\n                else:\n                    tmp_features = [f for f in self.features if f != feature]\n                    X_missing = data.loc[missing_rows, tmp_features].copy()\n                    for f in tmp_features:\n                        X_missing[f] = X_missing[f].fillna(self.mean_dict[f])\n                    imputed_data.loc[missing_rows, feature] = model[0].predict(X_missing[model[1]])\n        return imputed_data","metadata":{"execution":{"iopub.execute_input":"2025-04-17T15:30:10.051297Z","iopub.status.busy":"2025-04-17T15:30:10.050736Z","iopub.status.idle":"2025-04-17T15:30:10.070369Z","shell.execute_reply":"2025-04-17T15:30:10.069681Z"},"jupyter":{"source_hidden":true},"papermill":{"duration":0.076883,"end_time":"2025-04-17T15:30:10.071616","exception":false,"start_time":"2025-04-17T15:30:09.994733","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model = LassoCV(cv=5, random_state=SEED)\nimputer = Impute_With_Model(na_frac=0.4) \n# na_frac is the maximum fraction of missing values until which a feature is imputed with the model\n# if there are more missing values than for example 40% then we revert to mean imputation\nimputer.fit_models(model, train, features, numerical_features)\ntrain_impute = imputer.impute(train)\n","metadata":{"execution":{"iopub.execute_input":"2025-04-17T15:30:10.182420Z","iopub.status.busy":"2025-04-17T15:30:10.181748Z","iopub.status.idle":"2025-04-17T15:30:17.128660Z","shell.execute_reply":"2025-04-17T15:30:17.127998Z"},"papermill":{"duration":7.0035,"end_time":"2025-04-17T15:30:17.130219","exception":false,"start_time":"2025-04-17T15:30:10.126719","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"code","source":"## WE ARE USING LASSO ONLY FOR NUMERICAL FEATURES\nclass Impute_With_Model_lr:\n    \n    def __init__(self, na_frac=0.5, min_samples=0):\n        self.model_dict = {}\n        self.mean_dict = {}\n        self.features = None\n        self.na_frac = na_frac\n        self.min_samples = min_samples\n        self.group_means_dict = {} #Stores mean for numerical features and mode for categorical features\n        self.group_col = \"age_group\" # Group column for mean imputation\n\n    '''def find_features(self, data, feature, tmp_features):\n        missing_rows = data[feature].isna()\n        na_fraction = data[missing_rows][tmp_features].isna().mean(axis=0)\n        valid_features = np.array(tmp_features)[na_fraction <= self.na_frac]\n        return valid_features'''\n    \n    def find_features(self, data, feature, tmp_features, min_corr=0.4):\n        # Step 1: Identify rows where the target feature is missing\n        missing_rows = data[feature].isna()\n\n        # Step 2: Exclude tmp_features that are too missing in those rows\n        na_fraction = data[missing_rows][tmp_features].isna().mean(axis=0)\n        valid_missingness = np.array(tmp_features)[na_fraction <= self.na_frac]\n\n        # Step 3: Split valid features into numeric and categorical\n        numeric_features = [f for f in valid_missingness if pd.api.types.is_numeric_dtype(data[f])]\n        categorical_features = [f for f in valid_missingness if not pd.api.types.is_numeric_dtype(data[f])]\n\n        # Step 4: Compute correlation for numeric features only\n        if pd.api.types.is_numeric_dtype(data[feature]):\n            non_missing_data = data[feature].notna()\n            corrs = data.loc[non_missing_data, numeric_features].corrwith(\n                data.loc[non_missing_data, feature], numeric_only=True\n            )\n            valid_numerics = corrs[abs(corrs) >= min_corr].index.tolist()\n        else:\n            valid_numerics =numeric_features\n        # Step 5: Keep numerics with high correlation + all categoricals\n        \n        valid_features = valid_numerics + categorical_features\n\n        return valid_features\n\n\n\n    def fit_models(self, model, data, features, categorical_features):\n        self.features = features\n        n_data = data.shape[0]\n        self.group_means_dict = {}\n        for feature in features:\n            if pd.api.types.is_float_dtype(data[feature]) or pd.api.types.is_integer_dtype(data[feature]):\n                self.group_means_dict[feature] = data.groupby(self.group_col)[feature].mean()\n            elif pd.api.types.is_categorical_dtype(data[feature]) or pd.api.types.is_object_dtype(data[feature]):\n                self.group_means_dict[feature] = data.groupby(self.group_col)[feature].agg(lambda x: x.mode().iloc[0] if not x.mode().empty else np.nan)\n\n        for feature in features:\n            if data[feature].dtype in ['float64', 'int64']:\n                self.mean_dict[feature] = np.mean(data[feature])\n            elif data[feature].dtype in ['object', 'category']:\n                self.mean_dict[feature] = data[feature].mode().iloc[0]\n            \n        for feature in tqdm(categorical_features):\n            if data[feature].isna().sum() > 0:\n                model_clone = clone(model)\n                X = data[data[feature].notna()].copy()\n                tmp_features = [f for f in features if f != feature]\n                tmp_features = self.find_features(data, feature, tmp_features)\n                if len(tmp_features) >= 1 and X.shape[0] > self.min_samples:\n                    for f in tmp_features:\n                        group_means = self.group_means_dict[f]\n                        X[f] = X.apply(\n                            lambda row: group_means[row[self.group_col]] if pd.isna(row[f]) else row[f],\n                            axis=1\n                        )\n                        #X[f] = X[f].fillna(self.mean_dict[f])\n                    model_clone.fit(X[tmp_features], X[feature])\n                    self.model_dict[feature] = (model_clone, tmp_features.copy())\n                else:\n                    self.model_dict[feature] = (\"mean\", self.mean_dict[feature])\n            \n    def impute(self, data):\n        imputed_data = data.copy()\n        for feature, model in self.model_dict.items():\n            missing_rows = imputed_data[feature].isna()\n            group_means = self.group_means_dict[feature]\n            if missing_rows.any():\n                if model[0] == \"mean\":\n                    imputed_data.loc[missing_rows, feature] = imputed_data.loc[missing_rows].apply(\n                        lambda row: group_means.get(row[self.group_col], self.mean_dict[feature]), axis=1\n                    )\n                else:\n                    tmp_features = [f for f in self.features if f != feature]\n                    X_missing = data.loc[missing_rows, tmp_features].copy()\n                    for f in tmp_features:\n                        X_missing[f] = X_missing[f].fillna(self.mean_dict[f])\n                    predicted = model[0].predict(X_missing[model[1]])\n\n                    # If feature is categorical and has categories defined, convert to object before assignment\n                    if pd.api.types.is_categorical_dtype(imputed_data[feature]):\n                        imputed_data[feature] = imputed_data[feature].astype(object)\n\n                    imputed_data.loc[missing_rows, feature] = predicted\n        return imputed_data\n    \n    def impute_test(self, data):\n        imputed_data = data.copy()\n        for feature, model in self.model_dict.items():\n            if feature in exclude:\n                continue\n            group_means = self.group_means_dict[feature]\n            missing_rows = imputed_data[feature].isna()\n            if missing_rows.any():\n                if model[0] == \"mean\":\n                    imputed_data.loc[missing_rows, feature] = imputed_data.loc[missing_rows].apply(\n                        lambda row: group_means.get(row[self.group_col], self.mean_dict[feature]), axis=1\n                    )\n                else:\n                    tmp_features = [f for f in self.features if f != feature]\n                    X_missing = data.loc[missing_rows, tmp_features].copy()\n                    for f in tmp_features:\n                        X_missing[f] = X_missing[f].fillna(self.mean_dict[f])\n                    predicted = model[0].predict(X_missing[model[1]])\n\n                    # If feature is categorical and has categories defined, convert to object before assignment\n                    if pd.api.types.is_categorical_dtype(imputed_data[feature]):\n                        imputed_data[feature] = imputed_data[feature].astype(object)\n\n                    imputed_data.loc[missing_rows, feature] = predicted\n        return imputed_data","metadata":{"execution":{"iopub.execute_input":"2025-04-17T15:30:17.243487Z","iopub.status.busy":"2025-04-17T15:30:17.243103Z","iopub.status.idle":"2025-04-17T15:30:17.265849Z","shell.execute_reply":"2025-04-17T15:30:17.264896Z"},"papermill":{"duration":0.081222,"end_time":"2025-04-17T15:30:17.267547","exception":false,"start_time":"2025-04-17T15:30:17.186325","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model_lr = LogisticRegression(max_iter=200, solver='lbfgs', multi_class='auto')\nimputer_lr = Impute_With_Model_lr(na_frac=0.4) \n# na_frac is the maximum fraction of missing values until which a feature is imputed with the model\n# if there are more missing values than for example 40% then we revert to mean imputation\nimputer_lr.fit_models(model_lr, train_impute, features, categorical_features_pred)\ntrain_impute_lr = imputer_lr.impute(train_impute)\n","metadata":{"execution":{"iopub.execute_input":"2025-04-17T15:30:17.381651Z","iopub.status.busy":"2025-04-17T15:30:17.380879Z","iopub.status.idle":"2025-04-17T15:30:21.480874Z","shell.execute_reply":"2025-04-17T15:30:21.480018Z"},"papermill":{"duration":4.158577,"end_time":"2025-04-17T15:30:21.482573","exception":false,"start_time":"2025-04-17T15:30:17.323996","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test_impute = imputer.impute_test(test)\ntest_impute_lr = imputer_lr.impute_test(test_impute)","metadata":{"execution":{"iopub.execute_input":"2025-04-17T15:30:21.620824Z","iopub.status.busy":"2025-04-17T15:30:21.620510Z","iopub.status.idle":"2025-04-17T15:30:22.248236Z","shell.execute_reply":"2025-04-17T15:30:22.247264Z"},"papermill":{"duration":0.686149,"end_time":"2025-04-17T15:30:22.249705","exception":false,"start_time":"2025-04-17T15:30:21.563556","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_impute_lr.to_csv(\"train_values_impute.csv\", index=False)\ntest_impute_lr.to_csv(\"test_values_impute.csv\", index=False)","metadata":{"execution":{"iopub.execute_input":"2025-04-17T15:30:22.366310Z","iopub.status.busy":"2025-04-17T15:30:22.365517Z","iopub.status.idle":"2025-04-17T15:30:22.715338Z","shell.execute_reply":"2025-04-17T15:30:22.714413Z"},"papermill":{"duration":0.409826,"end_time":"2025-04-17T15:30:22.716957","exception":false,"start_time":"2025-04-17T15:30:22.307131","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Loading the datasets\ntrain_file_path = \"train_values_impute.csv\"\ntrain_df = pd.read_csv(train_file_path)\ntest_file_path = \"test_values_impute.csv\"\ntest_df = pd.read_csv(test_file_path)\ndf = train_df","metadata":{"execution":{"iopub.execute_input":"2025-04-17T15:30:22.831414Z","iopub.status.busy":"2025-04-17T15:30:22.831098Z","iopub.status.idle":"2025-04-17T15:30:22.889416Z","shell.execute_reply":"2025-04-17T15:30:22.888736Z"},"papermill":{"duration":0.117594,"end_time":"2025-04-17T15:30:22.891078","exception":false,"start_time":"2025-04-17T15:30:22.773484","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_ft = train_df.copy()\ntest_ft = test_df.copy()","metadata":{"execution":{"iopub.execute_input":"2025-04-17T15:30:23.006495Z","iopub.status.busy":"2025-04-17T15:30:23.005712Z","iopub.status.idle":"2025-04-17T15:30:23.010311Z","shell.execute_reply":"2025-04-17T15:30:23.009680Z"},"papermill":{"duration":0.064087,"end_time":"2025-04-17T15:30:23.011785","exception":false,"start_time":"2025-04-17T15:30:22.947698","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def create_new_max_min_col(df):\n    df['GS_max'] = df[['FGC-FGC_GSND', 'FGC-FGC_GSD']].max(axis=1)\n    df['GS_min'] = df[['FGC-FGC_GSND', 'FGC-FGC_GSD']].min(axis=1)\n\n    df[\"SR_min\"] = df[['FGC-FGC_SRL', 'FGC-FGC_SRR']].min(axis=1)\n    df[\"SR_max\"] = df[['FGC-FGC_SRL', 'FGC-FGC_SRR']].max(axis=1)","metadata":{"execution":{"iopub.execute_input":"2025-04-17T15:30:23.127040Z","iopub.status.busy":"2025-04-17T15:30:23.126739Z","iopub.status.idle":"2025-04-17T15:30:23.132136Z","shell.execute_reply":"2025-04-17T15:30:23.131237Z"},"papermill":{"duration":0.064627,"end_time":"2025-04-17T15:30:23.133625","exception":false,"start_time":"2025-04-17T15:30:23.068998","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"code","source":"create_new_max_min_col(train_ft)","metadata":{"execution":{"iopub.execute_input":"2025-04-17T15:30:23.247804Z","iopub.status.busy":"2025-04-17T15:30:23.247458Z","iopub.status.idle":"2025-04-17T15:30:23.259815Z","shell.execute_reply":"2025-04-17T15:30:23.259114Z"},"papermill":{"duration":0.07142,"end_time":"2025-04-17T15:30:23.261413","exception":false,"start_time":"2025-04-17T15:30:23.189993","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"code","source":"create_new_max_min_col(test_ft)","metadata":{"execution":{"iopub.execute_input":"2025-04-17T15:30:23.388599Z","iopub.status.busy":"2025-04-17T15:30:23.387916Z","iopub.status.idle":"2025-04-17T15:30:23.398487Z","shell.execute_reply":"2025-04-17T15:30:23.397455Z"},"papermill":{"duration":0.075703,"end_time":"2025-04-17T15:30:23.399912","exception":false,"start_time":"2025-04-17T15:30:23.324209","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"code","source":"thresholds = [5, 10, 15, 18, 22]\ncu_map = {}\npu_map = {}\ntl_map = {}\ngs_max_map = {}\ngs_min_map = {}\nbmr_map = {}\ndee_map = {}\nsr_min_map = {}\nsr_max_map = {}\nffmi_map = {}\n\n\nprev = 0\nfor i in range(len(thresholds)):\n    curr = thresholds[i]\n    mean_cu = train_ft[(train_ft['Basic_Demos-Age'] > prev) & (train_ft['Basic_Demos-Age'] <= curr)]['FGC-FGC_CU'].mean()\n    mean_pu = train_ft[(train_ft['Basic_Demos-Age'] > prev) & (train_ft['Basic_Demos-Age'] <= curr)]['FGC-FGC_PU'].mean()\n    mean_tl = train_ft[(train_ft['Basic_Demos-Age'] > prev) & (train_ft['Basic_Demos-Age'] <= curr)]['FGC-FGC_TL'].mean()\n    mean_gs_max = train_ft[(train_ft['Basic_Demos-Age'] > prev) & (train_ft['Basic_Demos-Age'] <= curr)]['GS_max'].mean()\n    mean_gs_min = train_ft[(train_ft['Basic_Demos-Age'] > prev) & (train_ft['Basic_Demos-Age'] <= curr)]['GS_min'].mean()\n    mean_bmr = train_ft[(train_ft['Basic_Demos-Age'] > prev) & (train_ft['Basic_Demos-Age'] <= curr)]['BIA-BIA_BMR'].mean()\n    mean_dee = train_ft[(train_ft['Basic_Demos-Age'] > prev) & (train_ft['Basic_Demos-Age'] <= curr)]['BIA-BIA_DEE'].mean()\n    mean_sr_min = train_ft[(train_ft['Basic_Demos-Age'] > prev) & (train_ft['Basic_Demos-Age'] <= curr)]['SR_min'].mean()\n    mean_sr_max = train_ft[(train_ft['Basic_Demos-Age'] > prev) & (train_ft['Basic_Demos-Age'] <= curr)]['SR_max'].mean()\n    mean_ffmi = train_ft[(train_ft['Basic_Demos-Age'] > prev) & (train_ft['Basic_Demos-Age'] <= curr)]['BIA-BIA_FFMI'].mean()\n    cu_map[i] = mean_cu\n    pu_map[i] = mean_pu\n    tl_map[i] = mean_tl\n    gs_max_map[i] = mean_gs_max\n    gs_min_map[i] = mean_gs_min\n    bmr_map[i] = mean_bmr\n    dee_map[i] = mean_dee\n    sr_min_map[i] = mean_sr_min\n    sr_max_map[i] = mean_sr_max\n    ffmi_map[i] = mean_ffmi\n    \n    prev = curr","metadata":{"execution":{"iopub.execute_input":"2025-04-17T15:30:23.513929Z","iopub.status.busy":"2025-04-17T15:30:23.513183Z","iopub.status.idle":"2025-04-17T15:30:23.564323Z","shell.execute_reply":"2025-04-17T15:30:23.563314Z"},"papermill":{"duration":0.109588,"end_time":"2025-04-17T15:30:23.566135","exception":false,"start_time":"2025-04-17T15:30:23.456547","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"cu_map: \", cu_map)\nprint(\"pu_map: \", pu_map)\nprint(\"tl_map: \", tl_map)\nprint(\"gs_max_map\", gs_max_map)\nprint(\"gs_min_map\", gs_min_map)\nprint(\"bmr_map\", bmr_map)\nprint(\"dee_map\", dee_map)\nprint(\"sr_min_map\", sr_min_map)\nprint(\"sr_max_map\", sr_max_map)\nprint(\"ffmi_map\", ffmi_map)","metadata":{"execution":{"iopub.execute_input":"2025-04-17T15:30:23.693567Z","iopub.status.busy":"2025-04-17T15:30:23.693214Z","iopub.status.idle":"2025-04-17T15:30:23.699526Z","shell.execute_reply":"2025-04-17T15:30:23.697748Z"},"papermill":{"duration":0.07177,"end_time":"2025-04-17T15:30:23.701049","exception":false,"start_time":"2025-04-17T15:30:23.629279","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def bin_data(train, test, columns, n_bins=10):\n    # Combine train and test for consistent bin edges\n    combined = pd.concat([train, test], axis=0)\n\n    bin_edges = {}\n    for col in columns:\n        # Compute quantile bin edges correctly\n        edges = pd.qcut(combined[col], n_bins, retbins=True, labels=False, duplicates=\"drop\")[1]\n        bin_edges[col] = edges\n\n    # Apply the same bin edges to both train and test\n    for col, edges in bin_edges.items():\n        num_bins = len(edges) - 1  # Ensure the correct number of labels\n        labels = range(num_bins)   # Matching labels with bins\n\n        train[col] = pd.cut(train[col], bins=edges, labels=labels, include_lowest=True).astype(float)\n        test[col] = pd.cut(test[col], bins=edges, labels=labels, include_lowest=True).astype(float)\n\n    return train, test","metadata":{"execution":{"iopub.execute_input":"2025-04-17T15:30:23.817165Z","iopub.status.busy":"2025-04-17T15:30:23.816837Z","iopub.status.idle":"2025-04-17T15:30:23.823349Z","shell.execute_reply":"2025-04-17T15:30:23.822660Z"},"papermill":{"duration":0.065519,"end_time":"2025-04-17T15:30:23.824675","exception":false,"start_time":"2025-04-17T15:30:23.759156","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def feature_engineering(df):\n\n    df[\"CU_norm\"] = df['FGC-FGC_CU'] / df['age_group'].map(cu_map)\n    df[\"PU_norm\"] = df['FGC-FGC_PU'] / df['age_group'].map(pu_map)\n    df[\"TL_norm\"] = df['FGC-FGC_TL'] / df['age_group'].map(tl_map)\n\n    df['GS_max_norm'] = df['GS_max'] / df[\"age_group\"].map(gs_max_map)\n    df['GS_min_norm'] = df['GS_min'] / df[\"age_group\"].map(gs_min_map)\n\n    df['SR_max_norm'] = df['SR_max'] / df[\"age_group\"].map(gs_max_map)\n    df['SR_min_norm'] = df['SR_min'] / df[\"age_group\"].map(gs_min_map)\n\n    df[\"BMR_norm\"] = df[\"BIA-BIA_BMR\"] / df[\"age_group\"].map(bmr_map)\n    df[\"DEE_norm\"] = df[\"BIA-BIA_DEE\"] / df[\"age_group\"].map(dee_map)\n\n    df[\"FFMI_norm\"] = df[\"BIA-BIA_FFMI\"] / df[\"age_group\"].map(ffmi_map)\n\n    df[\"ECW_ICW_ratio\"] = df[\"BIA-BIA_ECW\"] / df[\"BIA-BIA_ICW\"]","metadata":{"execution":{"iopub.execute_input":"2025-04-17T15:30:23.940355Z","iopub.status.busy":"2025-04-17T15:30:23.939582Z","iopub.status.idle":"2025-04-17T15:30:23.945856Z","shell.execute_reply":"2025-04-17T15:30:23.945160Z"},"papermill":{"duration":0.065559,"end_time":"2025-04-17T15:30:23.947342","exception":false,"start_time":"2025-04-17T15:30:23.881783","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"code","source":"columns_to_bin = [\n    \"CU_norm\", \"PU_norm\", \"TL_norm\", \"GS_min_norm\", \"GS_max_norm\", \n    \"SR_min_norm\", \"SR_max_norm\", \"BMR_norm\", \"DEE_norm\", \"FFMI_norm\", \"Physical-HeartRate\", \"Physical-Waist_Circumference\", \"Physical-Height\" ,\"Physical-Weight\"\n]","metadata":{"execution":{"iopub.execute_input":"2025-04-17T15:30:24.061926Z","iopub.status.busy":"2025-04-17T15:30:24.061335Z","iopub.status.idle":"2025-04-17T15:30:24.065937Z","shell.execute_reply":"2025-04-17T15:30:24.065037Z"},"papermill":{"duration":0.06329,"end_time":"2025-04-17T15:30:24.067399","exception":false,"start_time":"2025-04-17T15:30:24.004109","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 'BIA-BIA_BMI' already removed, so no need to add here\ncolumns_to_remove = ['FGC-FGC_CU', 'FGC-FGC_GSND', 'FGC-FGC_GSD', 'FGC-FGC_PU', 'FGC-FGC_SRL', 'FGC-FGC_SRR', 'FGC-FGC_TL', \n                    'BIA-BIA_FFM', 'BIA-BIA_FMI','BIA-BIA_Frame_num', 'BIA-BIA_LDM']","metadata":{"execution":{"iopub.execute_input":"2025-04-17T15:30:24.182986Z","iopub.status.busy":"2025-04-17T15:30:24.182279Z","iopub.status.idle":"2025-04-17T15:30:24.186431Z","shell.execute_reply":"2025-04-17T15:30:24.185748Z"},"papermill":{"duration":0.062949,"end_time":"2025-04-17T15:30:24.187760","exception":false,"start_time":"2025-04-17T15:30:24.124811","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"code","source":"feature_engineering(train_ft)","metadata":{"execution":{"iopub.execute_input":"2025-04-17T15:30:24.302219Z","iopub.status.busy":"2025-04-17T15:30:24.301931Z","iopub.status.idle":"2025-04-17T15:30:24.314071Z","shell.execute_reply":"2025-04-17T15:30:24.313409Z"},"papermill":{"duration":0.070985,"end_time":"2025-04-17T15:30:24.315448","exception":false,"start_time":"2025-04-17T15:30:24.244463","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"code","source":"feature_engineering(test_ft)","metadata":{"execution":{"iopub.execute_input":"2025-04-17T15:30:24.430255Z","iopub.status.busy":"2025-04-17T15:30:24.429454Z","iopub.status.idle":"2025-04-17T15:30:24.441208Z","shell.execute_reply":"2025-04-17T15:30:24.440521Z"},"papermill":{"duration":0.070551,"end_time":"2025-04-17T15:30:24.442566","exception":false,"start_time":"2025-04-17T15:30:24.372015","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_ft, test_ft = bin_data(train_ft, test_ft, columns_to_bin, n_bins=10)","metadata":{"execution":{"iopub.execute_input":"2025-04-17T15:30:24.556529Z","iopub.status.busy":"2025-04-17T15:30:24.555944Z","iopub.status.idle":"2025-04-17T15:30:24.616048Z","shell.execute_reply":"2025-04-17T15:30:24.615030Z"},"papermill":{"duration":0.119044,"end_time":"2025-04-17T15:30:24.617691","exception":false,"start_time":"2025-04-17T15:30:24.498647","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_ft = train_ft.drop(columns_to_remove, axis=1)","metadata":{"execution":{"iopub.execute_input":"2025-04-17T15:30:24.734063Z","iopub.status.busy":"2025-04-17T15:30:24.733693Z","iopub.status.idle":"2025-04-17T15:30:24.741496Z","shell.execute_reply":"2025-04-17T15:30:24.740460Z"},"papermill":{"duration":0.067954,"end_time":"2025-04-17T15:30:24.742989","exception":false,"start_time":"2025-04-17T15:30:24.675035","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test_ft = test_ft.drop(columns_to_remove, axis=1)","metadata":{"execution":{"iopub.execute_input":"2025-04-17T15:30:24.856860Z","iopub.status.busy":"2025-04-17T15:30:24.856236Z","iopub.status.idle":"2025-04-17T15:30:24.862235Z","shell.execute_reply":"2025-04-17T15:30:24.861528Z"},"papermill":{"duration":0.064892,"end_time":"2025-04-17T15:30:24.863694","exception":false,"start_time":"2025-04-17T15:30:24.798802","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_ft.shape","metadata":{"execution":{"iopub.execute_input":"2025-04-17T15:30:24.978713Z","iopub.status.busy":"2025-04-17T15:30:24.977918Z","iopub.status.idle":"2025-04-17T15:30:24.983429Z","shell.execute_reply":"2025-04-17T15:30:24.982590Z"},"papermill":{"duration":0.064754,"end_time":"2025-04-17T15:30:24.984834","exception":false,"start_time":"2025-04-17T15:30:24.920080","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_ft.to_csv(\"train_ft.csv\", index=False)\ntest_ft.to_csv(\"test_ft.csv\", index=False)","metadata":{"execution":{"iopub.execute_input":"2025-04-17T15:30:25.100553Z","iopub.status.busy":"2025-04-17T15:30:25.099902Z","iopub.status.idle":"2025-04-17T15:30:25.437411Z","shell.execute_reply":"2025-04-17T15:30:25.436530Z"},"papermill":{"duration":0.397861,"end_time":"2025-04-17T15:30:25.439081","exception":false,"start_time":"2025-04-17T15:30:25.041220","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Loading the datasets\ntrain_file_path = \"train_ft.csv\"\ntrain = pd.read_csv(train_file_path)\ntest_file_path = \"test_ft.csv\"\ntest = pd.read_csv(test_file_path)\ndf = train","metadata":{"execution":{"iopub.execute_input":"2025-04-17T15:30:25.554528Z","iopub.status.busy":"2025-04-17T15:30:25.553826Z","iopub.status.idle":"2025-04-17T15:30:25.608636Z","shell.execute_reply":"2025-04-17T15:30:25.607837Z"},"papermill":{"duration":0.114072,"end_time":"2025-04-17T15:30:25.610213","exception":false,"start_time":"2025-04-17T15:30:25.496141","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train.dropna(subset=['sii'], how='all', inplace=True)\n","metadata":{"execution":{"iopub.execute_input":"2025-04-17T15:30:25.728124Z","iopub.status.busy":"2025-04-17T15:30:25.727344Z","iopub.status.idle":"2025-04-17T15:30:25.733981Z","shell.execute_reply":"2025-04-17T15:30:25.733326Z"},"papermill":{"duration":0.068117,"end_time":"2025-04-17T15:30:25.735265","exception":false,"start_time":"2025-04-17T15:30:25.667148","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"code","source":"sii_thresholds = [30, 50, 80, 100] # Thresholds for rating\ndef get_rating(x): ## Thresholds for rating\n    if 0 <= x <= 29:\n        return 0\n    elif 30 <= x <= 49:\n        return 1\n    elif 50 <= x <= 79:\n        return 2\n    else:\n        return 3\n\ndef quadratic_weighted_kappa(y_true, y_pred): # The Quadric Kappa Evaluation Metric\n    return cohen_kappa_score(y_true, y_pred, weights='quadratic')\n\ndef evaluate_predictions(y_true, oof_non_rounded):\n    rounded_p = get_rating(oof_non_rounded)\n    return -quadratic_weighted_kappa(y_true, rounded_p)\n\n#train_impute_lr['new_sii'] = train_impute_lr['PCIAT_Total_Imputed'].apply(get_rating)","metadata":{"execution":{"iopub.execute_input":"2025-04-17T15:30:25.849597Z","iopub.status.busy":"2025-04-17T15:30:25.849266Z","iopub.status.idle":"2025-04-17T15:30:25.854794Z","shell.execute_reply":"2025-04-17T15:30:25.854045Z"},"papermill":{"duration":0.064034,"end_time":"2025-04-17T15:30:25.856263","exception":false,"start_time":"2025-04-17T15:30:25.792229","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"code","source":"for col in train.columns:\n    if train[col].dtype == 'object' or str(train[col].dtype) == 'category':\n        le = LabelEncoder()\n        train[col] = le.fit_transform(train[col].astype(str))\n\nfor col in test.columns:\n    if test[col].dtype == 'object' or str(test[col].dtype) == 'category':\n        le = LabelEncoder()\n        test[col] = le.fit_transform(test[col].astype(str))\n","metadata":{"execution":{"iopub.execute_input":"2025-04-17T15:30:25.970986Z","iopub.status.busy":"2025-04-17T15:30:25.970691Z","iopub.status.idle":"2025-04-17T15:30:25.985776Z","shell.execute_reply":"2025-04-17T15:30:25.985117Z"},"papermill":{"duration":0.073475,"end_time":"2025-04-17T15:30:25.987135","exception":false,"start_time":"2025-04-17T15:30:25.913660","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_model = train\nY_target = train['PCIAT-PCIAT_Total']\nX = train_model.drop(columns=['id', 'sii', 'PCIAT-PCIAT_01', 'PCIAT-PCIAT_02', 'PCIAT-PCIAT_03','PCIAT-PCIAT_04', 'PCIAT-PCIAT_05', 'PCIAT-PCIAT_06', 'PCIAT-PCIAT_07',\n           'PCIAT-PCIAT_08', 'PCIAT-PCIAT_09', 'PCIAT-PCIAT_10', 'PCIAT-PCIAT_11',\n           'PCIAT-PCIAT_12', 'PCIAT-PCIAT_13', 'PCIAT-PCIAT_14', 'PCIAT-PCIAT_15',\n           'PCIAT-PCIAT_16', 'PCIAT-PCIAT_17', 'PCIAT-PCIAT_18', 'PCIAT-PCIAT_19',\n           'PCIAT-PCIAT_20', 'PCIAT-PCIAT_Total','Physical-Diastolic_BP', 'Physical-Systolic_BP','SR_max_norm', 'FFMI_norm', 'Physical-Waist_Circumference'])\nY_class = train['sii']","metadata":{"execution":{"iopub.execute_input":"2025-04-17T15:30:26.100664Z","iopub.status.busy":"2025-04-17T15:30:26.100107Z","iopub.status.idle":"2025-04-17T15:30:26.106189Z","shell.execute_reply":"2025-04-17T15:30:26.105502Z"},"papermill":{"duration":0.064467,"end_time":"2025-04-17T15:30:26.107413","exception":false,"start_time":"2025-04-17T15:30:26.042946","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"code","source":"## BALANCING THE DATASET (UPSAMPLING AND DOWNSAMPLING IS NOT APPROPRIATE FOR THIS DATASET)\ndef sample_weights_optimized(series):\n    \"\"\"\n    Calculate sample weights for continuous PCIAT target variables using equal-width binning to solve class imbalance issue by finding frequency of each pciat total value: Converts frequency into weight:\n    Rare bins get higher weights (inverse of frequency is used to calculate weight )\n    Common bins get lower weights\n  \n    \n    Parameters:\n    series: pandas Series containing all target values (assumed continuous with no nulls)\n    \n    Returns:\n    pandas Series: Sample weights normalized to mean 1.0\n    \"\"\"\n    # Handle edge cases efficiently\n    if len(series) <= 1 or series.nunique() <= 1:\n        return pd.Series(1.0, index=series.index)\n    \n    # Create equal-width bins directly\n    bins = pd.cut(series, bins=10, labels=False)\n    \n    # Get bin counts and calculate inverse frequency in one step\n    bin_counts = bins.value_counts()\n    inverse_freq = 1.0 / bin_counts\n    \n    # Map weights back to samples using the bin indices\n    weights = bins.map(inverse_freq)\n    \n    # Normalize weights to mean 1.0\n    return weights / weights.mean()","metadata":{"execution":{"iopub.execute_input":"2025-04-17T15:30:26.223125Z","iopub.status.busy":"2025-04-17T15:30:26.222459Z","iopub.status.idle":"2025-04-17T15:30:26.228193Z","shell.execute_reply":"2025-04-17T15:30:26.227441Z"},"papermill":{"duration":0.064882,"end_time":"2025-04-17T15:30:26.229575","exception":false,"start_time":"2025-04-17T15:30:26.164693","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"","metadata":{"papermill":{"duration":0.056905,"end_time":"2025-04-17T15:30:26.343919","exception":false,"start_time":"2025-04-17T15:30:26.287014","status":"completed"},"tags":[]}},{"cell_type":"code","source":"def convert_to_categories(predictions, boundary_values):\n    \"\"\"\n    Transform continuous prediction values into discrete categories \n    based on specified boundary thresholds.\n    \"\"\"\n    categories = np.zeros(len(predictions), dtype=int)\n    \n    # Apply each threshold sequentially\n    for category, threshold in enumerate(boundary_values):\n        categories[predictions >= threshold] = category + 1\n        \n    return categories\n\n\ndef find_optimal_boundaries(actual_values, model_predictions, initial_boundaries=None):\n    \"\"\"\n    Determine the optimal category boundaries that maximize agreement\n    between predictions and actual values using quadratic weighted kappa.\n    \"\"\"\n    # Set default initial boundaries if none provided\n    if initial_boundaries is None:\n        initial_boundaries = [30, 50, 80]\n    \n    # Define optimization objective function\n    def objective_function(boundaries, actuals, predictions):\n        categorized_predictions = convert_to_categories(predictions, boundaries)\n        kappa_score = cohen_kappa_score(actuals, categorized_predictions, weights='quadratic')\n        # Return negative since we want to maximize kappa\n        return -kappa_score\n    \n    # Perform optimization\n    optimization_result = minimize(\n        objective_function, \n        x0=initial_boundaries,\n        args=(actual_values, model_predictions),\n        method='Powell'\n    )\n    \n    # Verify optimization completed successfully\n    if not optimization_result.success:\n        raise RuntimeError(\"Boundary optimization failed to converge\")\n        \n    return optimization_result.x","metadata":{"execution":{"iopub.execute_input":"2025-04-17T15:30:26.459325Z","iopub.status.busy":"2025-04-17T15:30:26.459007Z","iopub.status.idle":"2025-04-17T15:30:26.465660Z","shell.execute_reply":"2025-04-17T15:30:26.465012Z"},"papermill":{"duration":0.066011,"end_time":"2025-04-17T15:30:26.466985","exception":false,"start_time":"2025-04-17T15:30:26.400974","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def evaluate_k_fold_validate(estimator, dataset, target_col, class_col, splitter, apply_weighting=False, show_progress=False):\n   \"\"\"\n   Evaluates a model using k-fold validation, optimizing decision boundaries to maximize agreement.\n   \n   Parameters:\n   - estimator: Model object with fit and predict methods\n   - dataset: DataFrame containing all data\n   - predictors: List of feature column names\n   - target_col: Column name for continuous target variable\n   - class_col: Column name for class labels\n   - splitter: Cross-validation iterator\n   - apply_weighting: Whether to apply balancing weights during training\n   - show_progress: Whether to display progress information\n   \n   Returns:\n   - mean_agreement: Average kappa agreement score across folds\n   - holdout_predictions: Predictions for all samples from their respective holdout folds\n   - boundary_sets: Optimized decision boundaries from each fold\n   \"\"\"\n   agreement_metrics = []\n   holdout_predictions = np.zeros(dataset.shape[0])\n   boundary_sets = []\n   \n   # Default decision boundaries if needed\n   default_boundaries = sii_thresholds\n   \n   # Iterate through cross-validation folds\n   for fold_num, (training_indices, validation_indices) in enumerate(splitter.split(dataset, class_col)):\n       # Extract training and validation data\n       X_training = dataset.iloc[training_indices]\n       y_training_target = target_col.iloc[training_indices] ## PCIAT Total\n       y_training_class = class_col.iloc[training_indices] ## SII\n       \n       X_validation = dataset.iloc[validation_indices]\n       y_validation_target = target_col.iloc[validation_indices]  ## PCIAT Total\n       y_validation_class = class_col.iloc[validation_indices] ## SIIs\n\n       # # Remove the 'id' column from your training data\n       # X_training = X_training.drop(columns=['id'])\n       # X_validation = X_validation.drop(columns=['id'])\n       \n       # Apply sample weighting if needed for class imbalance issue \n       if apply_weighting:\n           sample_weights = sample_weights_optimized(y_training_target)\n           estimator.fit(X_training, y_training_target, sample_weight=sample_weights)\n       else:\n           estimator.fit(X_training, y_training_target)\n       \n       # Generate predictions\n       training_predictions = estimator.predict(X_training)\n       validation_predictions = estimator.predict(X_validation)\n       \n       # Store predictions in the appropriate positions\n       holdout_predictions[validation_indices] = validation_predictions\n       \n       # Optimize decision boundaries based on training predictions\n       optimized_boundaries = find_optimal_boundaries(\n           y_training_class, \n           training_predictions, \n           initial_boundaries=default_boundaries\n       )\n       boundary_sets.append(optimized_boundaries)\n       \n       # Apply optimized boundaries to validation predictions\n       discretized_predictions = convert_to_categories(validation_predictions, optimized_boundaries)\n       \n       # Calculate kappa metric\n       kappa = cohen_kappa_score(y_validation_class, discretized_predictions, weights='quadratic')\n       agreement_metrics.append(kappa)\n       # \n       if show_progress:\n           print(f\"Fold {fold_num+1}: Quadratic Kappa = {kappa:.4f}\")\n   \n   if show_progress:\n       print(f\"Mean Agreement: {np.mean(agreement_metrics):.4f}\")\n       print(f\"Standard Deviation: {np.std(agreement_metrics):.4f}\")\n   \n   return np.mean(agreement_metrics), holdout_predictions, boundary_sets","metadata":{"execution":{"iopub.execute_input":"2025-04-17T15:30:26.582048Z","iopub.status.busy":"2025-04-17T15:30:26.581446Z","iopub.status.idle":"2025-04-17T15:30:26.590594Z","shell.execute_reply":"2025-04-17T15:30:26.589654Z"},"papermill":{"duration":0.068509,"end_time":"2025-04-17T15:30:26.592041","exception":false,"start_time":"2025-04-17T15:30:26.523532","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def optimize_xgb(trial, dataset, target_col, class_col,splitter, apply_weighting):\n    \"\"\"\n    Optuna objective function for XGBoost parameter optimization.\n    \n    Parameters:\n    trial: Optuna trial object\n    X: Features DataFrame\n    y: Target Series\n    eval_metric: Metric to evaluate on\n    \n    Returns:\n    float: Mean cross-validation score (RMSE)\n    \"\"\"\n    # Parameter search space definition\n    params = {\n        'objective': trial.suggest_categorical('objective', ['reg:squarederror', 'reg:tweedie']),\n        'tree_method': 'approx',\n        'n_estimators': trial.suggest_int('n_estimators', 100, 300),\n        'max_depth': trial.suggest_int('max_depth', 2, 6),\n        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.1),\n        'subsample': trial.suggest_float('subsample', 0.5, 0.9),\n        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 0.9),\n        'reg_alpha': trial.suggest_float('reg_alpha', 1e-5, 0.1),\n        'reg_lambda': trial.suggest_float('reg_lambda', 1e-5, 0.1),\n        'random_state': 42,\n        'verbosity': 0\n    }\n    \n    # Add tweedie-specific parameters if that objective is selected\n    if params['objective'] == 'reg:tweedie':\n        params['tweedie_variance_power'] = trial.suggest_float('tweedie_variance_power', 1.0, 2.0)\n    \n    model = XGBRegressor(**params)\n    score, _, _ = evaluate_k_fold_validate(\n        estimator=model,\n        dataset=dataset,\n        target_col=target_col,\n        class_col=class_col,\n        splitter=splitter,\n        apply_weighting=apply_weighting,\n        show_progress=False\n    )\n    return score\n\n\ndef optimize_lgbm(trial, dataset, target_col, class_col, splitter, apply_weighting):\n    params = {\n        'objective': trial.suggest_categorical('objective', ['regression', 'poisson', 'tweedie']),\n        'n_estimators': trial.suggest_int('n_estimators', 100, 300),\n        'max_depth': trial.suggest_int('max_depth', 2, 6),\n        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.1),\n        'subsample': trial.suggest_float('subsample', 0.5, 0.9),\n        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 0.9),\n        'min_child_samples': trial.suggest_int('min_child_samples', 10, 100),\n        'random_state': 42,\n        'verbosity': -1\n    }\n\n    if params['objective'] == 'tweedie':\n        params['tweedie_variance_power'] = trial.suggest_float('tweedie_variance_power', 1.0, 2.0)\n\n    model = LGBMRegressor(**params)\n    score, _, _ = evaluate_k_fold_validate(\n        estimator=model,\n        dataset=dataset,\n        target_col=target_col,\n        class_col=class_col,\n        splitter=splitter,\n        apply_weighting=apply_weighting,\n        show_progress=False\n    )\n    return score\n\n\ndef optimize_extraTrees(trial, dataset, target_col, class_col, splitter, apply_weighting):\n    params = {\n        'n_estimators': trial.suggest_int('n_estimators', 100, 500),\n        'max_depth': trial.suggest_int('max_depth', 5, 30),\n        'min_samples_leaf': trial.suggest_int('min_samples_leaf', 1, 20),\n        'max_features': trial.suggest_categorical('max_features', ['sqrt', 'log2', None]),\n        'bootstrap': trial.suggest_categorical('bootstrap', [True, False]),\n        'random_state': 42\n    }\n\n    model = ExtraTreesRegressor(**params)\n    score, _, _ = evaluate_k_fold_validate(\n        estimator=model,\n        dataset=dataset,\n        target_col=target_col,\n        class_col=class_col,\n        splitter=splitter,\n        apply_weighting=apply_weighting,\n        show_progress=False\n    )\n    return score\n\ndef optimize_catboost(trial, dataset, target_col, class_col, splitter, apply_weighting):\n    params = {\n        'loss_function': trial.suggest_categorical('loss_function', ['RMSE', 'Poisson']),\n        'iterations': trial.suggest_int('iterations', 100, 300),\n        'depth': trial.suggest_int('depth', 3, 7),\n        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.1),\n        'l2_leaf_reg': trial.suggest_float('l2_leaf_reg', 1e-3, 0.1),\n        'bagging_temperature': trial.suggest_float('bagging_temperature', 0.0, 1.0),\n        'random_strength': trial.suggest_float('random_strength', 1e-3, 10.0),\n        'min_data_in_leaf': trial.suggest_int('min_data_in_leaf', 20, 60),\n        'random_seed': 42,\n        'verbose': 0\n    }\n\n    model = CatBoostRegressor(**params)\n    score, _, _ = evaluate_k_fold_validate(\n        estimator=model,\n        dataset=dataset,\n        target_col=target_col,\n        class_col=class_col,\n        splitter=splitter,\n        apply_weighting=apply_weighting,\n        show_progress=False\n    )\n    return score\n    \n\ndef run_optuna_optimization(dataset, target_col, class_col, model_type, splitter, apply_weighting=True, n_trials=30):\n    \"\"\"\n    Run Optuna optimization for the specified model type.\n    \n    Parameters:\n    X: Features DataFrame\n    y: Target Series\n    model_type: Type of model to optimize ('xgb', 'lgbm', or 'catboost')\n    n_trials: Number of optimization trials\n    \n    Returns:\n    dict: Best parameters found by Optuna\n    \"\"\"\n    print(f\"Starting optimization for {model_type.upper()} with {n_trials} trials\")\n    \n    # Create Optuna study (set direction to minimize for RMSE)\n    study = optuna.create_study(direction='maximize')\n    \n    # Select the appropriate objective function\n    if model_type.lower() == 'xgb':\n        study.optimize(lambda trial: optimize_xgb(trial, dataset, target_col, class_col, splitter, apply_weighting),\n                   n_trials=n_trials)\n    elif model_type.lower() == 'lgbm':\n        study.optimize(lambda trial: optimize_lgbm(trial, dataset, target_col, class_col, splitter, apply_weighting),\n                   n_trials=n_trials)\n    elif model_type.lower() == 'extratrees':\n        study.optimize(lambda trial: optimize_extraTrees(trial, dataset, target_col, class_col, splitter, apply_weighting),\n                   n_trials=n_trials)\n    elif model_type.lower() == 'catboost':\n        study.optimize(lambda trial: optimize_catboost(trial, dataset, target_col, class_col, splitter, apply_weighting),\n                   n_trials=n_trials)\n    else:\n        raise ValueError(\"model_type must be 'xgb', 'lgbm', 'catboost' or 'extratrees'\")\n    \n    print(\"\\nBest Parameters:\")\n    print(study.best_params)\n    print(f\"Best Quadratic Kappa: {study.best_value:.4f}\")\n    return study.best_params","metadata":{"execution":{"iopub.execute_input":"2025-04-17T15:30:26.706807Z","iopub.status.busy":"2025-04-17T15:30:26.706301Z","iopub.status.idle":"2025-04-17T15:30:26.725294Z","shell.execute_reply":"2025-04-17T15:30:26.724660Z"},"papermill":{"duration":0.078243,"end_time":"2025-04-17T15:30:26.726750","exception":false,"start_time":"2025-04-17T15:30:26.648507","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"code","source":"splitter = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)","metadata":{"execution":{"iopub.execute_input":"2025-04-17T15:30:26.842112Z","iopub.status.busy":"2025-04-17T15:30:26.841397Z","iopub.status.idle":"2025-04-17T15:30:26.845494Z","shell.execute_reply":"2025-04-17T15:30:26.844742Z"},"papermill":{"duration":0.062957,"end_time":"2025-04-17T15:30:26.846856","exception":false,"start_time":"2025-04-17T15:30:26.783899","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# For XGBoost\nXGB_OPTIMAL_PARAMS = run_optuna_optimization(X, Y_target, Y_class, model_type='xgb', splitter=splitter, apply_weighting=True, n_trials=30)\n\n# For LightGBM\nLGBM_OPTIMAL_PARAMS = run_optuna_optimization(X, Y_target, Y_class, model_type='lgbm', splitter=splitter, apply_weighting=True, n_trials=30)\n\n# For CatBoost\nCATBOOST_OPTIMAL_PARAMS = run_optuna_optimization(X, Y_target, Y_class, model_type='catboost', splitter=splitter, apply_weighting=True, n_trials=30)\n\n# # For ExtraTree\n# XTRATREE_OPTIMAL_PARAMS = run_optuna_optimization(X, Y_target, Y_class, model_type='extratrees', splitter=splitter, apply_weighting=True, n_trials=30)","metadata":{"execution":{"iopub.execute_input":"2025-04-17T15:30:26.962053Z","iopub.status.busy":"2025-04-17T15:30:26.961729Z","iopub.status.idle":"2025-04-17T15:38:24.262094Z","shell.execute_reply":"2025-04-17T15:38:24.261165Z"},"papermill":{"duration":477.35928,"end_time":"2025-04-17T15:38:24.263494","exception":false,"start_time":"2025-04-17T15:30:26.904214","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model_param_combination = [\n    [LGBMRegressor, LGBM_OPTIMAL_PARAMS],\n    [XGBRegressor, XGB_OPTIMAL_PARAMS],\n    [CatBoostRegressor, CATBOOST_OPTIMAL_PARAMS],\n]\ntrain_list = []\npredictions_list = []\nscore_list = []\nweights = sample_weights_optimized(train['PCIAT-PCIAT_Total'])\ntest = test.drop(columns=['id', 'BIA-BIA_BMI', 'SDS-SDS_Total_T','Physical-Diastolic_BP', 'Physical-Systolic_BP', 'SR_max_norm', 'FFMI_norm', 'Physical-Waist_Circumference'])\n\nfor model,params in model_param_combination:\n    print(f\"Model: {model.__name__}, Parameters: {params}\")\n    model_instance = model(**params)\n    kappa_score, oof, thresholds = evaluate_k_fold_validate(\n        model_instance, X, Y_target, Y_class, splitter, apply_weighting=True)\n    score_list.append(kappa_score)\n\n    model_instance.fit(X, Y_target, sample_weight=weights)\n    thresholds_ens = np.mean(thresholds, axis=0)\n    predictions = model_instance.predict(test)\n    predictions = convert_to_categories(predictions, thresholds_ens)\n    predictions_list.append(predictions)\n    train_pred = model_instance.predict(X)\n    train_pred = convert_to_categories(train_pred, thresholds_ens)\n    train_list.append(train_pred)\n\nENSEMBLE = 'voting'\nif ENSEMBLE == 'voting':\n    # Mode voting (majority rules)\n    test_preds = np.array(predictions_list)\n    voted_test = stats.mode(test_preds, axis=0).mode.flatten().astype(int)\n    final_test = voted_test","metadata":{"execution":{"iopub.execute_input":"2025-04-17T15:38:24.376112Z","iopub.status.busy":"2025-04-17T15:38:24.375577Z","iopub.status.idle":"2025-04-17T15:38:37.447482Z","shell.execute_reply":"2025-04-17T15:38:37.446538Z"},"papermill":{"duration":13.128931,"end_time":"2025-04-17T15:38:37.448786","exception":false,"start_time":"2025-04-17T15:38:24.319855","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"code","source":"final_test","metadata":{"execution":{"iopub.execute_input":"2025-04-17T15:38:37.560739Z","iopub.status.busy":"2025-04-17T15:38:37.560391Z","iopub.status.idle":"2025-04-17T15:38:37.565325Z","shell.execute_reply":"2025-04-17T15:38:37.564609Z"},"papermill":{"duration":0.060098,"end_time":"2025-04-17T15:38:37.566441","exception":false,"start_time":"2025-04-17T15:38:37.506343","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Create submission file\nsubmission = pd.read_csv(\"/kaggle/input/child-mind-institute-problematic-internet-use/sample_submission.csv\")\nsubmission['sii'] = final_test\nsubmission.to_csv(\"submission.csv\", index=False)","metadata":{"execution":{"iopub.execute_input":"2025-04-17T15:38:37.681992Z","iopub.status.busy":"2025-04-17T15:38:37.681708Z","iopub.status.idle":"2025-04-17T15:38:37.701159Z","shell.execute_reply":"2025-04-17T15:38:37.700610Z"},"papermill":{"duration":0.078125,"end_time":"2025-04-17T15:38:37.702294","exception":false,"start_time":"2025-04-17T15:38:37.624169","status":"completed"},"tags":[]},"outputs":[],"execution_count":null}]}